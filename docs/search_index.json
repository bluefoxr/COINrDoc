[["index.html", "Composite Indicator Development and Analysis in R with COINr Chapter 1 Overview 1.1 Background 1.2 Installation 1.3 Whats it for? 1.4 Demo 1.5 Features and status", " Composite Indicator Development and Analysis in R with COINr William Becker 2021-04-22 Chapter 1 Overview 1.1 Background This documentation describes in detail the COINr package, which is an open source R package for developing and analysing composite indicators. In fact, this is slightly more than technical documentation, and also gives some tips on composite indicator development along the way. A composite indicator is an aggregation of indicators which aims to measure a particular concept. Composite indicators are typically used to measure complex and multidimensional concepts which are difficult to define, and cannot be measured directly. Examples include innovation, human development, environmental performance, and so on. Composite indicators are closely related to scoreboards, which are also groups of indicators aiming to capture a concept. However, scoreboards do not aggregate indicator values. Composite indicators also usually use a hierarchical structure which breaks the concept down into elements, sometimes known as sub-pillars, pillars, sub-indexes, dimensions, and so on. COINr is currently still under development, therefore the package itself, and this documentation, are a work in progress but are being continually updated. You can find the latest version of COINr at its GitHub repo, and also the source code for this documentation here. 1.2 Installation Although COINr is under development, a pre-beta version can be installed via Github. First, install the devtools package if you dont already have it, then run: devtools::install_github(&quot;bluefoxr/COINr&quot;) This should directly install the package from Github, without any other steps. You may be asked to update packages. This might not be strictly necessary, so you can also skip this step. At the time of writing (April 2021), the functionalities of the package are perhaps 75% complete. Roughly speaking, the features that are described in detail in this documentation are more or less complete and have survived an initial round of testing and checks. Any functions that are not described in detail here should be treated with more caution and it is likely that bugs would be encountered here and there. The entire package will need to beta tested before a more official release. In case you do spot a bug, or have a suggestion, the best way is to open an issue on the repo. Otherwise, you can also just email me. 1.3 Whats it for? COINr is the first fully-flexible development and analysis environment for composite indicators and scoreboards. The main features are: Flexible and fast development of composite indicators with no limits on aggregation levels, numbers of indicators, highly flexible set of methodological choices. In-depth statistical analysis of indicators and results, including multivariate analysis, statistical reweighting, uncertainty and sensitivity analysis, etc. Interactive data exploration and visualisation via Shiny apps which can be hosted online. HTML widgets for incorporation in interactive markdown documents. In short, COINr aims to allow composite indicators to be developed and prototyped very quickly and in a structured fashion, with the results immediately available and able to be explored interactively. Although it is built in R, it is a high-level package that aims to make command simple and intuitive, with the hard work performed behind the scenes, therefore it is also accessible to less experienced R users. 1.4 Demo COINr is highly customisable, but equally allows composite indicator construction in a few commands. Taking the built-in ASEM dataset, we can assemble a composite indicator in a few steps. library(COINr) ## ## Attaching package: &#39;COINr&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## aggregate # assemble basic composite indicator object from input data ASEM &lt;- assemble(IndData = ASEMIndData, IndMeta = ASEMIndMeta, AggMeta = ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- Here, we have loaded the COINr package, and assembled a so-called COIN, which is a structured object used universally within COINr. It contains a complete record of all data sets, parameters, methodological choices, analysis and results generated during the construction and analysis process. A COIN is initially assembled by supplying a table of indicator data, a table of indicator metadata, and a table of aggregation metadata. More details can be found in the chapter on COINs: the currency of COINr. Currently, the COIN (composite indicator) only contains the input data set and other metadata. To actually get the aggregated results (i.e. and index), we have to denominate, normalise and aggregate it. These operations are performed by dedicated functions in COINr. # denominate data using specifications in ASEMIndMeta ASEM &lt;- denominate(ASEM) # normalise data ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;) # Aggregate normalised data ASEM &lt;- aggregate(ASEM, dset = &quot;Normalised&quot;) Of course, there are many ways to aggregate data, and other steps could be performed before aggregating, such as (multivariate) analysis, data treatment and so forth. Here, we have simply taken three basic steps. Each of the functions generates a new data set  here, respectively Denominated, Normalised and Aggregated data sets which are added to and stored within the COIN. Although in the example here the default specifications have been used, the functions have many options, and can be directed to operate on any of the data sets within the COIN. Details on this are left to later chapters in this book. At this point, let us examine some of the outputs in the COIN. First, we may want to see the index structure: plotframework(ASEM) In the online version of this documentation, this plot appears as an interactive HTML widget which can be embedded into HTML documents. COINr leverages existing R packages, and interactive graphics (like that of Figure 6) are generated using bindings to the plotly package, which is based on Javascript libraries. Static graphics are largely produced by the ggplot2 package. The framework plot is called a sunburst plot and summarises the structure of the index and the relative weight of each indicator in each aggregation level and the index. We can also get indicator statistics in table format: library(reactable) library(magrittr) ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;) ASEM$Analysis$Raw$StatTable %&gt;% reactable::reactable(defaultPageSize = 5, highlight = TRUE, wrap = F) Again, this table is an presented as an interactive HTML table and is easier explored using the online version of this book. In short, getstats() provides key statistics on each indicator, relating to data availability, outliers, summary statistics such as mean, median, skew, kurtosis, and so on. This can be applied to any data set within the COIN. Indicator distributions can also be easily plotted: plotIndDist(ASEM, type = &quot;Violindot&quot;, icodes = &quot;Political&quot;) Finally, results can be visualised and explored in an interactive dashboard. This can be useful as a rapid prototype of the index. The app may even be hosted online, or can be used as the basis for a more customised indicator visualisation platform. Here, a basic screenshot is provided. Figure 1.1: Results dashboard screenshot Clearly, this is not meant to replace a dedicated composite indicator visualisation site, but is a fast first step. 1.5 Features and status COINr has, or will have, many more features. Among those that are already implemented and in various stages of testing: Unlimited indicators, units, aggregation levels Denomination of indicators Indicator visualisation via static plots and dashboards Indicator statistics and missing data analysis Imputation of missing data via various methods Outlier analysis and treatment Normalisation via various methods Aggregation of indicators via various methods Multivariate analysis (PCA) and correlation analysis Interactive re-weighting Export to Excel Import from the COIN Tool And those that will be included soon, among others: Automatic re-weighting Grouping units by e.g. income groups, size groups Support for panel data, i.e. indexes for multiple years What-if experiments, comparing alternative versions of the index Sensitivity analysis, robustness to methodological assumptions, weights, etc. "],["foundations.html", "Chapter 2 Foundations 2.1 How to use COINr 2.2 Suggested workflows 2.3 A map of COINr 2.4 Terminology 2.5 Tips for using COINr", " Chapter 2 Foundations This chapter aims to explain some important principles about how COINr works, as well as giving an overview of the package and suggested workflows. If you want to immediately get to COINr functions without any further ado, skip this chapter for now. However if you want to understand COINr better, this might be worth reading. 2.1 How to use COINr There are two main ways to use COINr. The first is to use the functions as standalone tools for your own specific use. For example, coin_normalise.R will take a data frame as an input, and normalise the columns according to the options specified in its input, outputting another data frame. The second way is to work inside what Ill call the COINrverse (with tongue firmly in cheek). What this means is that all operations are performed on a single object, the COIN object, which stores all data, all analysis, parameters, and methodological choices made along the way. The COINrverse approach enables the full feature set of COINr, because some functions only work on COIN objects - for example, exporting all results to Excel in one command, or running an uncertainty/sensitivity analysis. Indeed, COINr is built mainly for COINrverse use, but functions have been written with the flexibility to also accommodate independent use where possible. More details on how the COINrverse works can be found in XXX. 2.2 Suggested workflows E.g. quick construction Analysis, audit Comparisons 2.3 A map of COINr COINr consists of many functions that do a number of different types of operations. Here is a summary. 2.3.1 Construction The following functions are the main functions for building a composite indicator in COINr. Function Description assemble() Assembles indicator data/metadata into a COIN checkData() Data availability check and unit screening denominate() Denominate (divide) indicators by other indicators impute() Impute missing data using various methods treat() Treat outliers with Winsorisation and transformations normalise() Normalise data using various methods aggregate() Aggregate indicators into hierarchical levels, up to index regen() Regenerates COIN results using specifications stored in .$Method 2.3.2 Visualisation These functions are for visualising data. Function Description iplotBar() Interactive bar chart for any indicator iplotIndDist() Interactive indicator distribution plots for a single indicator iplotIndDist2() Interactive indicator distribution plots for two indicators simultaneously iplotMap() Interactive choropleth map for any indicator (only works for countries) plotCorr() Interactive correlation heatmap between any levels of the index plotframework() Interactive sunburst plot visualising the indicator framework plotIndDist() Static plot of distributions for indicators and groups of indicators 2.3.3 Analysis The following functions analyse indicator data. Function Description cPCA() Principle component analysis on a specified data set and subset of indicators. Returns PCA weights. effectiveWeight() Calculates the effective weights of each element in the indicator hierarchy. getStats() Get table of indicator statistics for any data set weightOpt() Weight optimisation according to a pre-specified vector of importances 2.3.4 Interactive apps These are interactive apps, built using Shiny, which allow fast interactive exploration and adjustments. Function Description indDash() Indicator visualisation (distribution) dashboard for one or two indicators rew8r() Interactively re-weight indicators and check updated results and correlations 2.3.5 Import/export Functions to import and export data and results to and from COINr and R. Function Description COINToolIn() Import indicator data and metadata from COIN Tool coin_2Excel() Write data, analysis and results from a COIN to Excel 2.3.6 Other functions These are other functions that may be of interest, but do not neatly fit in the previous categories. Many of them are called from the other functions listed above, but may still be useful on their own. Function Description BoxCox() Box Cox transformation on a vector of data build_ASEM() Build ASEM (example) composite indicator in one command coin_win() Winsorise one column of data according to skew/kurtosis thresholds copeland() Aggregates a data frame into a single column using the Copeland method. geoMean() Weighted geometric mean of a vector getIn() Useful function for subsetting indicator data. See Helper functions. harMean() Weighted harmonic mean of a vector loggish() Log-type transformation, of various types, for a vector names2Codes() Given a character vector of long names (probably with spaces), generates short codes. outrankMatrix() Outranking matrix based on a data frame of indicator data and corresponding weights roundDF() Round down a data frame (i.e. for presentation) 2.3.7 Data COINr comes with some example data embedded into the package. Function Description ASEMIndData ASEM (example) indicator data as input for assemble() ASEMIndMeta ASEM (example) indicator metadata as input for assemble() ASEMAggMeta ASEM (example) aggregate metadata as input for assemble() WorldDenoms National denomination data (GDP, population, etc) worldwide 2.3.8 Finally There are also a number functions which are mainly for internal use, and are not listed here. 2.4 Terminology Lets clearly define a few terms first to avoid confusion later on. An indicator is a variable which has an observed value for each unit. Indicators might be things like life expectancy, CO2 emissions, number of tertiary graduates, and so on. A unit is one of the entities that you are comparing using indicators. Often, units are countries, but they could also be regions, universities, individuals or even competing policy options (the latter is the realm of multicriteria decision analysis). Together, indicators and units form the main input data frame for COINr (units as rows, and indicators as columns). 2.5 Tips for using COINr 2.5.1 Missing data Its very important to understand the difference between missing data and zeros. A zero means that you know that the value of the indicator is zero, whereas missing data (in R this is denoted as NA) means you dont know what the value is at all. When aggregating indicators, this difference becomes particularly important. If a data point is missing, it is often excluded from the aggregation, which effectively means reassigning it with the mean (or similar) value of the indicators in the same group. If you impute the data, missing values could be assigned with mean or median indicator values, for example. Clearly, these values will be very different from zeros. This point is made here, because data may sometimes come with missing values denoted as zeros, or with zeros denoted as missing values. Ensure that these are checked carefully before proceeding with construction. 2.5.2 Ordering Currently, COINr is somewhat dependent on the order of the units. Although you can input your data in any order you like (as long as you follow the rules in [Data Input]), in some cases the order does matter. For example, if you want to denominate your data by a separate data frame of indicators (not in the COIN), you will have to make sure that the units (rows) correspond to the same order of rows as your indicator data. In future updates this dependence on order should be removed, because it is a possible source of errors. For now though, just keep ordering in mind. Its worth mentioning that the row order never changes in the .$Data folder. However, if you screen units using a minimum data requirement (e.g. using datacheck()), then the number of units can change. This should also be kept in mind. 2.5.3 Syntax Where possible, COINr functions use a common syntax to keep things simple. For example: dset always refers to the name of the data set that is found in .$Data. For example, dset = \"Normalised\" will return .$Data$Normalised. The only exception is dset = \"Denominators\", which returns .$Input$Denominators. The argument dset is used in many COINr functions because they have to know which data set to operate on. More to be added here. "],["coins-the-currency-of-coinr.html", "Chapter 3 COINs: the currency of COINr 3.1 Introduction 3.2 The three inputs 3.3 Putting everything together 3.4 Moving on", " Chapter 3 COINs: the currency of COINr 3.1 Introduction Where possible COINr functions can be used as standalone functions, typically on data frames of indicator data, for operations such as normalisation, imputation and so on. The full functionality of COINr is however harnessed by assembling the indicator data, and the structure of the index, into a COIN. A COIN is a structured list, which neatly stores the various data sets, parameters, analyses and methodological decisions in one place. There are various reasons for doing this: It simplifies operations because functions know where data and parameters are. Therefore, COINr operations typically follow a syntax of the form COINobj &lt;- COINr_function(COINobj, &lt;methodological settings&gt;), updating the COIN object with new results and data sets generated by the function. This avoids having to repeatedly specify indicator names, structure, etc, as separate arguments. Its like putting a coin in a vending machine and getting a packet of crisps. Except you also get your coin back1. It keeps things organised and avoids a workspace of dozens of variables. It keeps a record of methodological decisions - this allows results to be easily regenerated following what if experiments, such as removing or changing indicators etc (see Section on adjustments and reruns TO ADD). The logic of doing this will become clearer as you build your index. If you only want to use the COINr functions on data frames, you can probably skip this chapter. 3.2 The three inputs To build a COIN object, three ingredients (data frames) are needed to begin with. In short, they are the indicator data, the indicator metadata, and the aggregation metadata. Here, each will be explained separately. Inputting the data in the first place is where you will do most of the work, because you have to get your data in a format that COINr understands. But once you have got your data assembled, COINr should do most of the hard work, so hang in there! 3.2.1 Indicator data The indicator data is a data frame which, shockingly, specifies the data for the indicators. However, it can do more than that, and also some rules have to be followed. The easiest way to explain is to start with an example. library(COINr) head(ASEMIndData) ## # A tibble: 6 x 60 ## UnitName UnitCode Group_GDP Group_GDPpc Group_Pop Group_EurAsia Year Den_Area ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Austria AUT L XL M Europe 2018 83871 ## 2 Belgium BEL L L L Europe 2018 30528 ## 3 Bulgaria BGR S S M Europe 2018 110879 ## 4 Croatia HRV S M S Europe 2018 56594 ## 5 Cyprus CYP S L S Europe 2018 9251 ## 6 Czech R~ CZE M L M Europe 2018 78867 ## # ... with 52 more variables: Den_Energy &lt;dbl&gt;, Den_GDP &lt;dbl&gt;, Den_Pop &lt;dbl&gt;, ## # LPI &lt;dbl&gt;, Flights &lt;dbl&gt;, Ship &lt;dbl&gt;, Bord &lt;dbl&gt;, Elec &lt;dbl&gt;, Gas &lt;dbl&gt;, ## # ConSpeed &lt;dbl&gt;, Cov4G &lt;dbl&gt;, Goods &lt;dbl&gt;, Services &lt;dbl&gt;, FDI &lt;dbl&gt;, ## # PRemit &lt;dbl&gt;, ForPort &lt;dbl&gt;, Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, UNVote &lt;dbl&gt;, ## # CostImpEx &lt;dbl&gt;, Tariff &lt;dbl&gt;, TBTs &lt;dbl&gt;, TIRcon &lt;dbl&gt;, RTAs &lt;dbl&gt;, ## # Visa &lt;dbl&gt;, StMob &lt;dbl&gt;, Research &lt;dbl&gt;, Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, ## # CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, MigStock &lt;dbl&gt;, Lang &lt;dbl&gt;, Renew &lt;dbl&gt;, ## # PrimEner &lt;dbl&gt;, CO2 &lt;dbl&gt;, MatCon &lt;dbl&gt;, Forest &lt;dbl&gt;, Poverty &lt;dbl&gt;, ## # Palma &lt;dbl&gt;, TertGrad &lt;dbl&gt;, FreePress &lt;dbl&gt;, TolMin &lt;dbl&gt;, NGOs &lt;dbl&gt;, ## # CPI &lt;dbl&gt;, FemLab &lt;dbl&gt;, WomParl &lt;dbl&gt;, PubDebt &lt;dbl&gt;, PrivDebt &lt;dbl&gt;, ## # GDPGrow &lt;dbl&gt;, RDExp &lt;dbl&gt;, NEET &lt;dbl&gt; COINr comes prepackaged with some example data - here we are looking at indicator data from the ASEM Sustainable Connectivity Portal, which has an indicator data set covering 51 Asian and European countries. The first thing to notice is that each row is an observation (here, a country), and each column is a variable (mostly indicators, but also other things). Look at the structure of the data frame, working from left to right: UnitName [required] gives the name of each unit. Here, units are countries, so these are the names of each country. UnitCode [required] is a unique code assigned to each unit (country). This is very important, and is the main reference inside COINr for units. If your units are countries, I recommend using ISO Alpha-3 codes, because these are recognised by COINr for generating maps. It also makes data processing generally easier. Group_* [optional] Any column name that starts with Group_ is recognised as a group column rather than an indicator. You dont have to have any groups, but some COINr functions support specific operations on groups (e.g. imputation within group, and future updates plan to expand this capacity). You can have as many group columns as you want. Year [optional] gives the reference year of the data. This allows you to have multiple years of data, for example, you can have a value for a given country for 2018, and another value for the same country for 2019, and so on. Like groups, this feature is not yet fully developed in COINr. Den_*[optional] Any column names that begin with Den_* are recognised as denominators, i.e. indicators that are used to scale other indicators. Finally, any column that begins with x_ will be ignored and passed through. This is not shown in the data set above, but is useful for e.g. alternative codes or other variables that you want to retain. Any remaining columns that do not begin with x_ or use the other names in this list are recognised as indicators. You will notice that all column (variable/indicator) names use short codes. This is to keep things concise in tables, rough plots etc. Indicator codes should be short, but informative enough that you know which indicator it refers to (e.g. Ind1 is not so helpful). In some COINr plots, codes are displayed, so you might want to take that into account. In any case, the full names of indicators, and other details, are also specified in the indicator metadata table - see the next section. Some important rules and tips to keep in mind are: The following columns are required. All other columns are optional: UnitCode UnitName At least one indicator column Columns dont have to be in any particular order, columns are identified by names rather than positions. You can have as many indicators and units as you like. Indicator codes and unit codes must have unique names. You cant use the same code twice otherwise bad things will happen. Avoid any accented characters or basically any characters outside of English - this can sometimes cause trouble with encoding. Column names are case-sensitive. Most things in COINr are built to have a degree of flexibility where possible, but column names need to be written exactly as they appear here for COINr to recognise them., 3.2.2 Indicator metadata The second data frame you need to input specifies the metadata of each indicator. This serves two purposes: first, to give details about each indicator, such as its name, its units and so on; and second, to specify the structure of the index. Heres what this looks like, for our example ASEM data set: head(ASEMIndMeta) ## # A tibble: 6 x 10 ## IndName IndCode Direction IndWeight Denominator IndUnit Target Agg1 Agg2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Logist~ LPI 1 1 &lt;NA&gt; Score ~ 4.12 Phys~ Conn ## 2 Intern~ Flights 1 1 Den_Pop Thousa~ 200. Phys~ Conn ## 3 Liner ~ Ship 1 1 &lt;NA&gt; Score 20.1 Phys~ Conn ## 4 Border~ Bord 1 1 Den_Area Number~ 116. Phys~ Conn ## 5 Trade ~ Elec 1 1 Den_Energy TWh 105. Phys~ Conn ## 6 Trade ~ Gas 1 1 Den_Energy Billio~ 90.1 Phys~ Conn ## # ... with 1 more variable: Agg3 &lt;chr&gt; Notice that now, the table is flipped on its side (transposed), and each row is an indicator, and the columns specify other things. This is to keep the data in a tidy format. Lets go through the columns one by one. IndName [required] This is the full name of the indicator, which will be used in display plots. IndCode[required] A reference code for each indicator. These must be the same codes as specified in the indicator metadata. The codes must also be unique. Direction [required] The direction of each indicator - this takes values of either 1 or -1 for each indicator. A value of 1 means that higher values of the indicator correspond to higher values of the index, whereas -1 means the opposite. IndWeight [required] The initial weights assigned to each indicator. Weights are relative and do not need to sum to one, so you can simply put all 1s here if you dont know what else to put (the values can be adjusted later). Denominator [required??] These should be the indicator codes of one of the denominator variables for each indicator to be denominated. E.g. here Den_Pop specifies that the indicator should be denominated by the Den_Pop indicators (population, in this case). For any indicators that do not need denominating, just set NA. Denominators can also be specified later, so if you want you can leave this column out. See Denomination for more information. IndUnit [optional] The units of the indicator. This helps for keeping track of what the numbers actually mean, and can be used in plots. Target [optional] Targets associated with each indicator. Here, artificial targets have been generated which are 95% of the maximum score (accounting for the direction of the indicator). These are only used if the normalisation method is distance-to-target. Agg* [required] Any column name that begins with Agg is recognised as a column specifying the aggregation group, and therefore the structure of the index. Aggregation columns should be in the order of the aggregation, but otherwise can have arbitrary names. Lets look at the aggregation columns in a bit more detail. Each column represents a separate aggregation level, so in the ASEM example here we have three aggregation levels - the pillars, the two sub-indexes, and the overall index. The entry of each column specifies which group each indicator falls in. So, the first column Agg1 specifies the pillar of each indicator. Again, each aggregation group (pillar, sub-index or index) is referenced by a unique code. The next column Agg2, gives the sub-index that the indicator belongs to. There is a bit of redundancy here, because obviously indicators in the same pillar must also belong to the same sub-index. Finally, Agg3 specifies that all indicators belong to the index. You can have as many Agg columns as you like, and the names dont have to be Agg1 etc, but could be e.g. Agg_Pillar, Agg_SubIndex, etc. However, they must begin with Agg, otherwise COINr will not recognise them. And they must appear in the order of the aggregation, i.e. lowest level of aggregation first, then working upwards. 3.2.3 Aggregation metadata The final data input is the aggregation metadata, which is also the simplest. Heres our example for the ASEM data set: head(ASEMAggMeta) ## # A tibble: 6 x 4 ## AgLevel Code Name Weight ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Physical Physical 1 ## 2 2 ConEcFin Economic and Financial (Con) 1 ## 3 2 Political Political 1 ## 4 2 Instit Institutional 1 ## 5 2 P2P People to People 1 ## 6 2 Environ Environmental 1 This data frame simply consists of three columns for each aggregation level: *Code [required] The aggregation group codes. This column must end with Code, and the codes must match the codes in the corresponding column in the indicator metadata aggregation columns. *Name [required] The aggregation group names. This column must end with Name. *Weight [required] The aggregation group weights. This column must end with Weight. Again, these weights can be changed later on. Columns must appear in the order of the aggregation groups. 3.3 Putting everything together Having got all your data in the correct format, you can finally build it into a COIN object. From here, things start to get a bit easier. The function to build the COIN object is called assemble(). This function takes the three data frames mentioned and converts them into a so-called COIN Object, which is a hierarchical list that is structured in a way that is recognised by all COINr functions. Lets run assemble() using the built-in data sets to show how it works. ASEM &lt;- assemble(IndData = ASEMIndData, IndMeta = ASEMIndMeta, AggMeta = ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- The three inputs here are as follows: IndData which is the indicator data IndMeta which is the indicator metadata AggMeta which is the aggregation metadata And this outputs a COIN Object which you can name as you want (here we have called it ASEM). There are two further arguments to assemble() which are not used here, and they are: include - this is a character vector of indicator codes (i.e. codes that are found in IndData$IndCode) which specifies which indicators to include out of the IndData and IndMeta inputs. By default, all indicators are included, but this gives the option to only include certain indicators, if for example, you have several alternative data sources. exclude- this an analogous character vector of indicator codes, but specifying which indicators to exclude, if any. Again, by default, nothing is excluded. This may be easier to specify when creating a subset of indicators. The structure of the list is as follows. First, it is divided into five main sub-lists, which I am going to call folders: Input is the input data, and has the following entries: IndData The indicator data, after any exclusion of indicators IndMeta The indicator metadata, after any exclusion of indicators AggMeta The aggregation metadata, after any exclusion of indicators Original A list containing the original, unaltered inputs to assemble() Data is where the indicator data sets are stored. As you go through the construction, this will populated with more data sets, e.g. imputed data, treated data, aggregated data, and so on. On first assembly you will only see Raw present here. Parameters stores some useful parameters that are used by COINr functions, and may be of general interest, such as the indicator codes, unit codes, weights, and so on. Analysis is where analysis of the indicator data will be stored, including missing data analysis, indicator summary statistics, principal component analysis and so on. Method keeps a record of the methodological decisions made when constructing the composite indicator, for example the normalisation method and parameters, data treatment specifications, and so on. Apart from keeping a record of the construction, this also allows the entire results to be regenerated using the information stored in the COIN Object, this will be explained better in Chapter XXX. The logic of the COIN object is that COINr functions can take all the data and parameters that they need from it, and you only have to specify some particular parameters of the function. Then, outputs, such as new data sets, are returned back to the COIN object, which is added to and expanded as the analysis progresses. Apart from building the COIN Object, assemble() does a few other things: It checks that indicator codes are consistent between indicator data and indicator metadata It checks that required columns, such as indicator codes, are presenr It returns some basic information about the data that was input, such as the number of indicators, the number of units, the number of aggregation levels and the groups in each level. This is done so you can check what you have entered, and that it agrees with your expectations. 3.4 Moving on Now that you have a COIN Object, you can start using all the other functions in COINr to their full potential. Most functions will still work on standalone data frames, so you dont need to work with COIN objects if you prefer not to. Depending on the function, you either get a packet of crisps (e.g. a plot or analysis), or your coin plus interest (an updated COIN with a new data set or analysis) "],["initial-visualisation-and-analysis.html", "Chapter 4 Initial visualisation and analysis 4.1 Structure 4.2 Distributions 4.3 Ranks and Maps 4.4 Statistics and analysis", " Chapter 4 Initial visualisation and analysis One of the first things to do with indicator data is to look at it, in as many ways as possible. This helps to get a feel for how the data is distributed between units/countries, how it may be spatially distibuted, and how indicators relate to one another. COINr includes various tools for visualising and analysing indicator data and the index structure. The types of plots generated by COINr fall into two categories: static plots, and interactive plots. static plots generate images in standard formats such as png, pdf and so on. Interactive plots generate javascript graphics, which have interactive elements such as zooming and panning, and information when you hover the mouse over. These latter type of plots are particularly useful for including in HTML documents, because they are self contained. For example, they can be used in R Markdown documents, then knitted to HTML, or embedded on websites (such as this one - see below), e.g. via the blogdown or bookdown packages. Javascript plots can also be rendered to png and other formats, so can also be used in static documents. The plotting tools here can be useful at any stage of building a composite indicator or scoreboard, from initial visualisation of the data, to checking the effects of data treatment, to visualising final index results. 4.1 Structure Independently from the indicator data, a good way to begin is to check the structure of the index. This can be done visually with the plotframework() function, which generates a sunburst plot of the index structure. plotframework(ASEM) The sunburst plot is useful for a few things. First, it shows the structure that COINr has understood. If you get an error here, it is probably an indication that something has gone wrong in the input of the structure, so go back to the input data and check. If it does successfully display the sunburst plot, you can check whether the structure agrees with your expectations. Second, it shows the effective weight of each indicator (the value is visible by hovering over each segment). This can reveal which indicators are implicitly weighted more than others, by e.g. having more or less indicators in the same aggregation groups. Finally, it can be a good way to communicate your index structure to other people. 4.2 Distributions Individual indicator distributions can be visualised in several different ways. For static plots, the main tool is plotIndDist() which generates histograms, boxplots, violin plots, dot plots and violin-dot plots. This is powered by ggplot2, and if you want to customise plots, you should use that directly. However, COINr plotting functions are intended as quick tools to visualise data, with easy access to the hierarchical data set. You can plot individual indicators: plotIndDist(ASEM, type = &quot;Histogram&quot;, icodes = &quot;LPI&quot;) And you can also plot groups of indicators by calling aggregate names (notice that when multiple indicators are plotted, the indicator codes are used to label each plot, rather than indicator names, to save space): plotIndDist(ASEM, type = &quot;Violindot&quot;, icodes = &quot;Physical&quot;) The plotIndDist() function has several arguments. In the first place, any indicator or aggregation (pillar, dimension, index etc) can be plotted by using the dset argument. If you have only just assembled the COIN Object, you will only have the Raw dataset, but any other dataset can be accessed, e.g. treated data set, aggregated data set, and so on. You can also target different levels using the aglev argument - for more details see the chapter on Helper functions. Stand-alone data frames are also supported by plotIndDist() (this can also be achieved directly by ggplot without too much effort): df &lt;- as.data.frame(matrix(rnorm(90),nrow = 30, ncol = 3)) colnames(df) &lt;- c(&quot;Dogs&quot;, &quot;Cats&quot;, &quot;Rabbits&quot;) plotIndDist(df, type = &quot;Box&quot;) COINr also includes some interactive plots, which are embedded into apps (see later), but can be used for your own purposes, such as embedding in HTML documents or websites. iplotIndDist(ASEM, &quot;Raw&quot;, &quot;Renew&quot;, ptype = &quot;Violin&quot;) Since all the plotting functions output plot objects (plotly objects for iplotIndDist, and ggplot2 plot objects for plotIndDist), you can also modify them if you want to customise the plots. This might be a helpful workflow - to use COINrs default options and then tweak the plot to your liking. In a very simple example, here we just change the title. iplotIndDist(ASEM, &quot;Raw&quot;, &quot;Flights&quot;, ptype = &quot;Histogram&quot;) %&gt;% plotly::layout(title = &quot;Customised plot&quot;) If you are purely interested in exploring the data, rather than presenting it to someone else, the plots here are also embedded into a Shiny app which lets you quickly explore and compare indicator distributions - see Data Treatment for more details on this. 4.3 Ranks and Maps While the previous functions concerned plotting the statistical distributions of each indicator, functions are also available for plotting the indicator values in order or on a map. iplotBar(ASEM, dset = &quot;Raw&quot;, isel = &quot;Embs&quot;, usel = &quot;SGP&quot;) Here, a single indicator is plotted in order as a bar chart. There is an optional argument to highlight one or more units, using the usel argument. From a different perspective, we can plot the same data on a map: iplotMap(ASEM, dset = &quot;Raw&quot;, isel = &quot;Embs&quot;) Note that this only works if IndData$UnitCode correspond to ISO alpha-3 country codes. If you want to do some more sophisticated mapping R, Plotly has many mapping options, but R in general has all kinds of mapping packages, you just have to search for them. COINr uses Plotly maps to keep things simple and to not depend on too many packages. COINr has yet more tools to plot data, but lets leave it at that for the moment. Other tools will be introduced in other chapters. 4.4 Statistics and analysis Aside from plots, COINr gives a fairly detailed statistical analysis of initial indicator data. The function getStats() returns a series of statistics which can be aimed at any of the data sets in the .$Data folder. You can also specify if you want the output to be returned back to the COIN, or to a separate list. # get stats ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;, out2 = &quot;COIN&quot;) ## Number of collinear indicators = 10 ## Number of signficant negative indicator correlations = 324 ## Number of indicators with high denominator correlations = 18 # display in table using Reactable # (note the use of helper function roundDF() to round the values to a sensible number of decimals) ASEM$Analysis$Raw$StatTable %&gt;% roundDF() %&gt;% reactable::reactable(resizable = TRUE, bordered = TRUE, highlight = TRUE, defaultPageSize = 10) The columns of this table give all kind of information from max, min, standard deviation, etc, to the presence of outliers and amount of missing data. Apart from the overall statistics for each indicator, getStats also returns a few other things: .$Outliers, which flags individual outlying points using the relation to the interquartile range .$Correlations, which gives a correlation matrix between all indicators in the data set .$DenomCorrelations, which gives the correlations between indicators and any denominators Each of these aspects will be explained in more detail in later chapters (to add which), so for the moment it is enough to mention that they exist. "],["multivariate-analysis.html", "Chapter 5 Multivariate analysis", " Chapter 5 Multivariate analysis Correlations, and other relationships between indicators, can help to understand the structure of the data and to see whether indicators are redundant or are mistakenly encoded. Correlations PCA Cronbach To finish. "],["missing-data-and-imputation.html", "Chapter 6 Missing data and Imputation 6.1 Concept 6.2 Data checks and screening 6.3 Imputation in COINr", " Chapter 6 Missing data and Imputation Imputation is the process of estimating missing data points. This can be done in any number of ways, and as usual, the best way depends on the problem. Of course, you dont have to impute data. You can also simply delete any indicator or unit that has missing values, although in many cases this can be too restrictive. Reasonable results can still be obtained despite small amounts of missing data, although if too much data is missing, the uncertainty can be too high to give a meaningful analysis. As usual, it is a balance. A good first step is to check how much data is missing, and where (see below). Units with very high amounts of missing data can be screened out. Small amounts of missing data can then be imputed. 6.1 Concept The simplest imputation approach is to use values of other units to estimate the missing point. Typically, this could involve the sample mean or median of the indicator. Heres some data from the ASEM data set regarding the average connection speed of each country. Towards the end there are some missing values, so lets view the last few rows: library(COINr) Ind1 &lt;- data.frame(Country = ASEMIndData$UnitName, ConSpeed = ASEMIndData$ConSpeed) Ind1[40:nrow(Ind1),] ## Country ConSpeed ## 40 Korea 28.6 ## 41 Lao PDR NA ## 42 Malaysia 8.9 ## 43 Mongolia NA ## 44 Myanmar NA ## 45 New Zealand 14.7 ## 46 Pakistan NA ## 47 Philippines 5.5 ## 48 Russian Federation 11.8 ## 49 Singapore 20.3 ## 50 Thailand 16.0 ## 51 Vietnam 9.5 Using our simple imputation method, we just replace the NA values with the sample mean. Ind1$ConSpeed &lt;- replace(Ind1$ConSpeed, is.na(Ind1$ConSpeed), mean(Ind1$ConSpeed, na.rm = T)) Ind1[40:nrow(Ind1),] ## Country ConSpeed ## 40 Korea 28.60000 ## 41 Lao PDR 14.28605 ## 42 Malaysia 8.90000 ## 43 Mongolia 14.28605 ## 44 Myanmar 14.28605 ## 45 New Zealand 14.70000 ## 46 Pakistan 14.28605 ## 47 Philippines 5.50000 ## 48 Russian Federation 11.80000 ## 49 Singapore 20.30000 ## 50 Thailand 16.00000 ## 51 Vietnam 9.50000 This approach can be reasonable if countries are somehow similar in that indicator. However, other perhaps more informative ways are available: Substituting by the mean or median of the country group (e.g. income group or continent) If time series data is available, use the latest known data point Then there is another class of methods which use data from other indicators to estimate the missing point. The core idea here is that if the indicators are related to one another, it is possible to guess the missing point by using known values of other indicators. This can take the form of: Simply substituting the mean or median of the normalised values of the other indicators Subsituting the mean or median of normalised values of the other indicators within the aggregation group Using a more formal approach, based on regression or more generally on statistical modelling Lets explore the options available in COINr 6.2 Data checks and screening A first step is to check in detail how much data missing. The function checkData() does this, and also has the option to screen units based on data availability. # Assemble ASEM data first ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- # Missing data check on the raw data set ASEM &lt;- checkData(ASEM, dset = &quot;Raw&quot;) head(ASEM$Analysis$Raw$MissDatSummary) ## UnitCode N_missing PrcDataAll LowDataAll ## 1 AUT 0 100 FALSE ## 2 BEL 0 100 FALSE ## 3 BGR 0 100 FALSE ## 4 HRV 0 100 FALSE ## 5 CYP 0 100 FALSE ## 6 CZE 0 100 FALSE The MissDataSummary table shows the unit code, number of missing observations, and overall percentage data availability. Finally, the LowDataAll column can be used as a flag for units with data availability lower than a set amount ind_thresh which is one of the input arguments to checkData(). ASEM indicators were already chosen to fulfill minimum data requirements, for which reason the data availability is all above the default of 2/3. We will set the minimum data threshold a bit higher, and in doing so also demonstrate another feature of checkData(), which is to automatically screen units based on data availability. Setting unit_screen = TRUE will generate a new data set .$Data$Screened which only includes units with data availability above the set threshold. This data set can then be passed on to subsequent operations. # Missing data check on the raw data set ASEM &lt;- checkData(ASEM, dset = &quot;Raw&quot;, ind_thresh = 0.85, unit_screen = TRUE) ASEM$Analysis$Raw$RemovedUnits ## [1] &quot;BRN&quot; &quot;LAO&quot; &quot;MMR&quot; This generates a new data set .$Data$Screened with the removed units recorded in .$Analysis$Raw$RemovedUnits, which in this case are Brunei, Laos and Myanmar. As a final option to mention, you can manually include or exclude units. So for example, a unit that doesnt have sufficient data coverage could be manually included anyway, and another unit could be excluded. # Countries to include and exclude df &lt;- data.frame(UnitCode = c(&quot;AUT&quot;,&quot;BRN&quot;), Status = c(&quot;Exclude&quot;, &quot;Include&quot;)) # Missing data check on the raw data set ASEM &lt;- checkData(ASEM, dset = &quot;Raw&quot;, ind_thresh = 0.85, unit_screen = TRUE, Force = df) ASEM$Analysis$Raw$RemovedUnits ## [1] &quot;AUT&quot; &quot;LAO&quot; &quot;MMR&quot; Now AUT has been excluded, even though it had sufficient data coverage, and BRN has been manually included, even though it didnt. The intention is to give some flexibility to make exceptions for hard rules. The other output of interest from checkData() is missing data by group: head(ASEM$Analysis$Raw$MissDatByGroup[30:40,]) ## UnitCode Physical ConEcFin Political Instit P2P Environ Social ## 30 GBR 100.0 100 100 100.00000 100 100 100.00000 ## 31 AUS 100.0 100 100 100.00000 100 100 100.00000 ## 32 BGD 87.5 100 100 83.33333 75 100 88.88889 ## 33 BRN 87.5 80 100 100.00000 75 100 55.55556 ## 34 KHM 87.5 80 100 100.00000 75 100 77.77778 ## 35 CHN 100.0 100 100 100.00000 100 100 100.00000 ## SusEcFin Conn Sust Index ## 30 100 100.00000 100.00000 100.00000 ## 31 100 100.00000 100.00000 100.00000 ## 32 80 86.66667 89.47368 87.75510 ## 33 60 86.66667 68.42105 79.59184 ## 34 100 86.66667 89.47368 87.75510 ## 35 80 100.00000 94.73684 97.95918 This gives the percentage indicator data availability inside each aggregation group. 6.3 Imputation in COINr Now we turn to estimating missing data points in COINr. The function of interest is impute(), which gives a number of options, corresponding to the types of imputation discussed above. Briefly, COINr can impute using: The indicator mean, either across all countries or within a specified group The indicator median, either across all countries or within a specified group The mean or median, within the aggregation group The expectation maximisation algorithm, via the AMELIA package 6.3.1 By column The impute() function follows a similar logic to other COINr functions. We can enter a COIN or a data frame, and get a COIN or a data frame back. Lets impute on a COIN first, using the raw ASEM data. # Assemble ASEM data if not done already (uncomment the following) # ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, # AggMeta = COINr::ASEMAggMeta) # check how many NAs we have sum(is.na(ASEM$Data$Raw)) ## [1] 63 # impute using indicator mean ASEM &lt;- impute(ASEM, dset = &quot;Raw&quot;, imtype = &quot;ind_mean&quot;) ## Missing data points detected = 63 ## Missing data points imputed = 63, using method = ind_mean # check how many NAs after imputation sum(is.na(ASEM$Data$Imputed)) ## [1] 0 If the input is a COIN, by default the output is an updated COIN, with a new data set .$Data$Imputed. We can also set out2 = \"df\" to output the imputed data frame on its own. The above example replaces NA values with the mean of the indicator, and setting imtype = ind_median will instead use the indicator median. In this respect, we are using information from other units, inside the same indicator, to estimate the missing values. Imputing by group may be more informative if you have meaningful grouping variables. For example, the ASEM data set has groupings by GDP, population, GDP per capita and whether the country is European or Asian. We might hypothesise that it is better to replace NA values with the median inside a group, say by GDP group. This would imply that countries with a similar GDP are likely to have similar indicator values. # impute using GDP group median ASEM &lt;- impute(ASEM, dset = &quot;Raw&quot;, imtype = &quot;indgroup_median&quot;, groupvar = &quot;Group_GDP&quot;) ## Missing data points detected = 63 ## Missing data points imputed = 63, using method = indgroup_median 6.3.2 By row So far, the imputation has been using values from the same column (indicator). Another possibility is to use values from other indicators, i.e operate row-wise. COINr offers two basic options in this respect: either to take the mean or median of the other indicators inside the aggregation group, and this can be done by setting imtype = \"agg_mean\" or imtype = \"agg_median\" respectively. Both of these imputation methods will, for a given unit, take the mean or median of the normalised values of the other indicators in the aggregation group in the level above indicator level. The process is as follows, for each NA value: Find indicators in the same aggregation group Normalise these indicators using the min-max method, so they each have a minimum of zero and a maximum of one. Replace the NA with the mean or median of the normalised values, for the unit in question. Reverse the min-max transformation so that the indicators are returned to their original scale. Its important to realise that this is equivalent to aggregating with the mean or median without imputing first. This is because in the aggregation step, if we take the mean of a group of indicators, and there is a NA present, this value is excluded from the mean calculation. Doing this is mathematically equivalent to assigning the mean to that missing value. Therefore, one reason to use this imputation method is to see which values are being implicitly assigned as a result of excluding missing values from the aggregation step. This is sometimes known as shadow imputation. 6.3.3 By column and row Finally, we can use a regression-based expectation maximisation algorithm, imported via the AMELIA package. This estimates missing values using the values of the other indicators and the other units. This effectively involves regressing each indicator on other indicators, and predicting missing values. A catch of this approach is that if the number of units is small compared to the number of indicators, there will not be enough observations to estimate the parameters of the model. This is definitely the case if you have more indicators than observations. To use the EM algorithm in this case, imputation is performed on aggregation groups. So, to use the EM approach in COINr, you have to specify the aggregation level to group indicators by. # impute using EM, grouping indicators inside their pillars (level 2) ASEM &lt;- impute(ASEM, imtype = &quot;EM&quot;, EMaglev = 2) ## Missing data points detected = 63 ## Missing data points imputed = 63, using method = EM This works, because the ASEM data set has 51 units and no more thann eight indicators per pillar. However, if we were to use EMaglev = 3, i.e. try imputing indicators grouped into the two sub-indexes, it does not work. Finding the right aggregation level to impute at might involve a bit of trial and error, but the advantage of imputing by groups of indicators is that aggregation groups are usually composed of indicators that are similar in some respect. This makes it more likely that they are good predictors of the other indicators in their group. The AMELIA package offers many more options for imputation that are not available through the impute() function, such as bootstrapping, time-series imputation and more. As with plotting packages COINr aims to provide an easy and quick interface to standard imputation in AMELIA, but if you want to have full control you should use the AMELIA package directly. Other good imputation options are the MICE package (Multivariate Imputation via Chained Equations) and the missForest package (using random forests). "],["denomination.html", "Chapter 7 Denomination 7.1 Concept 7.2 Denominating in COINr 7.3 When to denominate, and by what?", " Chapter 7 Denomination The first section here gives a bit of introduction to the concept of denomination. If you just want to know how to denominate in COINr, skip to the section on Denominating in COINr. 7.1 Concept Denomination refers to the operation of dividing one indicator by another. But why should we do that? As anyone who has ever looked at a map will know, countries come in all different sizes. The problem is that many indicators, on their own, are strongly linked to the size of the country. That means that if we compare countries using these indicators directly, we will often get a ranking that has roughly the biggest countries at the top, and the smallest countries at the bottom. To take an example, lets examine some indicators in the ASEM data set, and introduce another plotting function in the process. # load COINr package library(COINr) library(magrittr) # for pipe operations # build ASEM index ASEM &lt;- build_ASEM() # plot international trade in goods against GDP iplotIndDist2(ASEM, dsets = c(&quot;Denominators&quot;, &quot;Raw&quot;), icodes = c(&quot;Den_GDP&quot;, &quot;Goods&quot;)) # (note: need to fix labelling and units of denominators) The function iplotIndDist2() is similar to iplotIndDist() but allows plotting two indicators against each other. You can pick any indicator from any data set for each, including denominators. What are these denominators anyway? Denominators are indicators that are used to scale other indicators, in order to remove the size effect. Typically, they are those related to physical or economic size, including GDP, population, land area and so on. Anyway, looking at the plot above, it is clear that that countries with a higher GDP have a higher international trade in international goods (e.g. Germany, China, UK, Japan, France), which is not very surprising. The problem comes when we want to include this indicator as a measure of connectivity: on its own, trade in goods simply summarises having a large economy. What is more interesting, is to measure international trade per unit GDP, and this is done by dividing (i.e. denominating) the international trade of each country by its GDP. Lets do that manually here and see what we get. # divide trade by GDP tradeperGDP &lt;- ASEM$Data$Raw$Goods/ASEM$Input$Denominators$Den_GDP # bar chart: add unit names first iplotBar(data.frame(UnitCode=ASEM$Parameters$UnitCodes, TradePerGDP = tradeperGDP)) Now the picture is completely different - small countries like Slovakia, Czech Republic and Singapore have the highest values. The rankings here are completely different because the meanings of these two measures are completely different. Denomination is in fact a nonlinear transformation, because every value is divided by a different value (each country is divided by its own unique GDP, in this case). That doesnt mean that denominated indicators are suddenly more right than the before their denomination, however. Trade per GDP is a useful measure of how much a countrys economy is dependent on international trade, but in terms of economic power, it might not be meaningful to scale by GDP. In summary, it is important to consider the meaning of the indicator compared to what you want to actually measure. More precisely, indicators can be thought of as either intensive or extensive variables. Intensive variables are not (or only weakly) related to the size of the country, and allow fair comparisons between countries of different sizes. Extensive variables, on the contrary, are strongly related to the size of the country. This distinction is well known in physics, for example. Mass is related to the size of the object and is an extensive variable. If we take a block of steel, and double its size (volume), we also double its mass. Density, which is mass per unit volume, is an intensive quantity: if we double the size of the block, the density remains the same. An example of an extensive variable is population. Bigger countries tend to have bigger populations. An example of an intensive variable is population density. This is no longer dependent on the physical size of the country. The summary here is that an extensive variable becomes an intensive variable when we divide it by a denominator. 7.2 Denominating in COINr Denomination is fairly simple to do in R, its just dividing one vector by another. Nevertheless, COINr has a dedicated function for denominating which makes life easier and helps you to track what you have done. Before we get to that, its worth mentioning that COINr has a few built-in denominators sourced from the World Bank. It looks like this: WorldDenoms ## # A tibble: 249 x 7 ## UnitName UnitCode Den_GDP Den_Pop Den_Area Den_GDPpc Den_IncomeGroup ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan AFG 1.93e10 3.80e7 652860 507. Low income ## 2 Albania ALB 1.53e10 2.85e6 27400 5353. Upper middle in~ ## 3 Algeria DZA 1.71e11 4.31e7 2381740 3974. Lower middle in~ ## 4 American Sam~ ASM 6.36e 8 5.53e4 200 11467. Upper middle in~ ## 5 Andorra AND 3.15e 9 7.71e4 470 40886. High income ## 6 Angola AGO 8.88e10 3.18e7 1246700 2791. Lower middle in~ ## 7 Anguilla AIA NA NA NA NA &lt;NA&gt; ## 8 Antarctica ATA NA NA NA NA &lt;NA&gt; ## 9 Antigua and ~ ATG 1.66e 9 9.71e4 440 17113. High income ## 10 Argentina ARG 4.45e11 4.49e7 2736690 9912. Upper middle in~ ## # ... with 239 more rows and the metadata can be found by calling ?WorldDenoms. Data here is the latest available as of February 2021 and I would recommend using these only for exploration, then updating your data yourself. To denominate your indicators in COINr, the function to call is denominate(). Like other COINr functions, this can be used either independently on a data frame of indicators, or on COINs. Consider that in all cases you need three main ingredients: Some indicator data that should be denominated Some other indicators to use as denominators A mapping to say which denominators (if any) to use for each indicator. 7.2.1 On COINs If you are working with a COIN, the indicator data will be present in the .$Data folder. If you specified any denominators in IndData (i.e. columns beginning with Den_) when calling assemble() you will also find them in .$Input$Denominators. Finally, if you specified a Denominator column in IndMeta when calling assemble() then the mapping of denominators to indicators will also be present. # The raw indicator data which will be denominated ASEM$Data$Raw ## # A tibble: 51 x 56 ## UnitCode UnitName Year Group_GDP Group_GDPpc Group_Pop Group_EurAsia LPI ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AUT Austria 2018 L XL M Europe 4.10 ## 2 BEL Belgium 2018 L L L Europe 4.11 ## 3 BGR Bulgaria 2018 S S M Europe 2.81 ## 4 HRV Croatia 2018 S M S Europe 3.16 ## 5 CYP Cyprus 2018 S L S Europe 3.00 ## 6 CZE Czech R~ 2018 M L M Europe 3.67 ## 7 DNK Denmark 2018 L XL M Europe 3.82 ## 8 EST Estonia 2018 S M S Europe 3.36 ## 9 FIN Finland 2018 M XL M Europe 3.92 ## 10 FRA France 2018 XL L L Europe 3.90 ## # ... with 41 more rows, and 48 more variables: Flights &lt;dbl&gt;, Ship &lt;dbl&gt;, ## # Bord &lt;dbl&gt;, Elec &lt;dbl&gt;, Gas &lt;dbl&gt;, ConSpeed &lt;dbl&gt;, Cov4G &lt;dbl&gt;, ## # Goods &lt;dbl&gt;, Services &lt;dbl&gt;, FDI &lt;dbl&gt;, PRemit &lt;dbl&gt;, ForPort &lt;dbl&gt;, ## # Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, UNVote &lt;dbl&gt;, CostImpEx &lt;dbl&gt;, Tariff &lt;dbl&gt;, ## # TBTs &lt;dbl&gt;, TIRcon &lt;dbl&gt;, RTAs &lt;dbl&gt;, Visa &lt;dbl&gt;, StMob &lt;dbl&gt;, ## # Research &lt;dbl&gt;, Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, ## # MigStock &lt;dbl&gt;, Lang &lt;dbl&gt;, Renew &lt;dbl&gt;, PrimEner &lt;dbl&gt;, CO2 &lt;dbl&gt;, ## # MatCon &lt;dbl&gt;, Forest &lt;dbl&gt;, Poverty &lt;dbl&gt;, Palma &lt;dbl&gt;, TertGrad &lt;dbl&gt;, ## # FreePress &lt;dbl&gt;, TolMin &lt;dbl&gt;, NGOs &lt;dbl&gt;, CPI &lt;dbl&gt;, FemLab &lt;dbl&gt;, ## # WomParl &lt;dbl&gt;, PubDebt &lt;dbl&gt;, PrivDebt &lt;dbl&gt;, GDPGrow &lt;dbl&gt;, RDExp &lt;dbl&gt;, ## # NEET &lt;dbl&gt; # The denominators ASEM$Input$Denominators ## # A tibble: 51 x 11 ## UnitCode UnitName Year Group_GDP Group_GDPpc Group_Pop Group_EurAsia ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AUT Austria 2018 L XL M Europe ## 2 BEL Belgium 2018 L L L Europe ## 3 BGR Bulgaria 2018 S S M Europe ## 4 HRV Croatia 2018 S M S Europe ## 5 CYP Cyprus 2018 S L S Europe ## 6 CZE Czech R~ 2018 M L M Europe ## 7 DNK Denmark 2018 L XL M Europe ## 8 EST Estonia 2018 S M S Europe ## 9 FIN Finland 2018 M XL M Europe ## 10 FRA France 2018 XL L L Europe ## # ... with 41 more rows, and 4 more variables: Den_Area &lt;dbl&gt;, ## # Den_Energy &lt;dbl&gt;, Den_GDP &lt;dbl&gt;, Den_Pop &lt;dbl&gt; # The mapping of denominators to indicators (see Denominator column) ASEM$Input$IndMeta ## # A tibble: 49 x 10 ## IndName IndCode Direction IndWeight Denominator IndUnit Target Agg1 Agg2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Logist~ LPI 1 1 &lt;NA&gt; Score ~ 4.12e0 Phys~ Conn ## 2 Intern~ Flights 1 1 Den_Pop Thousa~ 2.00e2 Phys~ Conn ## 3 Liner ~ Ship 1 1 &lt;NA&gt; Score 2.01e1 Phys~ Conn ## 4 Border~ Bord 1 1 Den_Area Number~ 1.16e2 Phys~ Conn ## 5 Trade ~ Elec 1 1 Den_Energy TWh 1.05e2 Phys~ Conn ## 6 Trade ~ Gas 1 1 Den_Energy Billio~ 9.01e1 Phys~ Conn ## 7 Averag~ ConSpe~ 1 1 &lt;NA&gt; Mbps 2.74e1 Phys~ Conn ## 8 Den_Po~ Cov4G 1 1 &lt;NA&gt; Percent 9.50e1 Phys~ Conn ## 9 Trade ~ Goods 1 1 Den_GDP Trilli~ 1.82e3 ConE~ Conn ## 10 Trade ~ Servic~ 1 1 Den_GDP Millio~ 6.24e2 ConE~ Conn ## # ... with 39 more rows, and 1 more variable: Agg3 &lt;chr&gt; COINrs denominate() function knows where to look for each of these ingredients, so we can simply call: ASEM &lt;- denominate(ASEM) which will return a new data set .Data$Denominated. To return the dataset directly, rather than outputting an updated COIN, you can also set out2 = \"df\" (this is a common argument to many functions which can be useful if you want to examine the result directly). You can also change which indicators are denominated, and by what. # Get denominator specification from metadata denomby_meta &lt;- ASEM$Input$IndMeta$Denominator # Example: we want to change the denominator of flights from population to GDP denomby_meta[ASEM$Input$IndMeta$IndCode == &quot;Flights&quot;] &lt;- &quot;Den_GDP&quot; # Now re-denominate. Return data frame for inspection ASEM &lt;- denominate(ASEM, dset = &quot;Raw&quot;, specby = &quot;user&quot;, denomby = denomby_meta) Here we have changed the denominator of one of the indicators, Flights, to GDP. This is done by creating a character vector denomby_meta (copied from the original denominator specification) which has an entry for each indicator, specifying which denominator to use, if any. We then changed the entry corresponding to Flights. This overwrites any previous denomination. If you want to keep and compare alternative specifications, see the chapter on (chap to add). Lets compare the raw Flights data with the Flights per GDP data: # plot raw flights against denominated iplotIndDist2(ASEM, dsets = c(&quot;Raw&quot;, &quot;Denominated&quot;), icodes = &quot;Flights&quot;) Clearly, the denominated and raw indicators are very different from one another, reflecting the completely different meaning. 7.2.2 On data frames If you are just working with data frames, you need to supply the three ingredients directly to the function. Here we will take some of the ASEM data for illustration (recalling that both indicator and denominator data is specified in ASEMIndMeta). # a small data frame of indicator data IndData &lt;- ASEMIndData[c(&quot;Goods&quot;, &quot;Services&quot;, &quot;FDI&quot;)] IndData ## # A tibble: 51 x 3 ## Goods Services FDI ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 278. 108. 5 ## 2 598. 216. 5.71 ## 3 42.8 13.0 1.35 ## 4 28.4 17.4 0.387 ## 5 8.77 15.2 1.23 ## 6 274. 43.5 3.88 ## 7 147. 114. 9.1 ## 8 28.2 10.2 0.580 ## 9 102. 53.8 6.03 ## 10 849. 471. 30.9 ## # ... with 41 more rows # two selected denominators Denoms &lt;- ASEMIndData[c(&quot;Den_Pop&quot;, &quot;Den_GDP&quot;)] # denominate the data IndDataDenom &lt;- denominate(IndData, denomby = c(&quot;Den_GDP&quot;, NA, &quot;Den_Pop&quot;), denominators = Denoms) IndDataDenom ## # A tibble: 51 x 3 ## Goods Services FDI ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.712 108. 0.000572 ## 2 1.28 216. 0.000500 ## 3 0.804 13.0 0.000191 ## 4 0.554 17.4 0.0000924 ## 5 0.437 15.2 0.00104 ## 6 1.40 43.5 0.000365 ## 7 0.478 114. 0.00159 ## 8 1.21 10.2 0.000443 ## 9 0.426 53.8 0.00109 ## 10 0.344 471. 0.000476 ## # ... with 41 more rows Since the input is recognised as a data frame, you dont need to specify any other arguments, and the output is automatically a data frame. Note how denomby works: here it specifies that that Goods should be denominated by Den_GDP, that Services should not be denominated, and that FDI should be denominated by Den_Pop. 7.3 When to denominate, and by what? Denomination is mathematically very simple, but from a conceptual point of view it needs to be handled with care. As we have shown, denominating an indicator will usually completely change it, and will have a corresponding impact on the results. Two ways of looking at the problem are first, from the conceptual point of view. Consider each indicator and whether it fits with the aim of your index. Some indicators are anyway intensive, such as the percentage of tertiary graduates. Others, such as trade, will be strongly linked to the size of the country. In those cases, consider whether countries with high trade values should have higher scores in your index or not? Or should it be trade as a percentage of GDP? Or trade per capita? Each of these will have different meanings. Sometimes extensive variables will anyway be the right choice. The Lowy Asia Power Index measures the power of each country in an absolute sense: in this case, the size of the country is all-important and e.g. trade or military capabilities per capita would not make much sense. The second (complimentary) way to approach denomination is from a statistical point of view. We can check which indicators are strongly related to the size of a country by correlating them with some typical denominators, such as the ones used here. The getStats() function does just this, checking the correlations between each indicator and each denominator, and flagging any possible high correlations. # get statistics on raw data set ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;, out2 = &quot;COIN&quot;, t_denom = 0.8) ## Number of collinear indicators = 10 ## Number of signficant negative indicator correlations = 324 ## Number of indicators with high denominator correlations = 15 # get the result ctable &lt;- ASEM$Analysis$Raw$DenomCorrelations # remove first column ctable &lt;- ctable[-1] # return only columns that have entries with correlation above 0.8 ctable[,colSums((abs(ctable) &gt; 0.8))&gt;0] ## Flights Goods Services FDI PRemit Tariff RTAs ## 1 0.004086325 0.04516500 0.01792843 0.2192345 0.1132941 0.3736916 -0.8930525 ## 2 0.280719950 0.36006001 0.42590268 0.6536837 0.4639201 0.7945156 -0.7191475 ## 3 0.845140925 0.84178477 0.86759186 0.9627225 0.9161336 0.2961538 -0.2755920 ## 4 0.025920170 0.07065718 0.23821808 0.4772343 0.2635235 0.8762061 -0.4544421 ## Visa StMob Research Pat CultGood MigStock Poverty ## 1 -0.8858828 0.3838843 0.10899708 -0.06388983 -0.001505749 0.4749737 0.0860072 ## 2 -0.7501379 0.4869492 0.33615583 0.29043647 0.312864841 0.7703435 0.6413876 ## 3 -0.2080551 0.8573485 0.89007903 0.83013272 0.849055254 0.9104204 0.2622643 ## 4 -0.4539003 0.2041878 0.08386292 0.06948564 0.052972424 0.4772082 0.9371589 ## FemLab ## 1 -0.1099044 ## 2 -0.5970645 ## 3 -0.2373947 ## 4 -0.8455970 The matrix that is displayed above only includes columns where there is a correlation value (between an indicator and any denominator) of greater than 0.8. The results show some interesting patterns: Many indicators have a high positive correlation with GDP, including flights, trade, foreign direct investment (very high), personal remittances, research, stock of migrants, and so on. Bigger countries, in terms of land area, tend to have less trade agreements (RTAs) and a more restrictive visa policy Larger populations are associated with higher levels of poverty The purpose here is not to draw causal links between these quantities, although they might reveal interesting patterns. Rather, these might suggest which quantities to denominate by, if the choices also work on a conceptual level. "],["data-treatment.html", "Chapter 8 Data Treatment 8.1 Why treat data? 8.2 How to treat data 8.3 Data treatment in COINr 8.4 Interactive visualisation", " Chapter 8 Data Treatment Data treatment is the process of altering indicators to improve their statistical properties, mainly for the purposes of aggregation. Data treatment is a delicate subject, because it essentially involves changing the values of certain observations, or transforming an entire distribution. This entails balancing two opposing considerations: On the one hand, treatment should be used as sparingly as possible, because you are altering one or more known data points. On the other hand, this is only done for the purposes of aggregation, (i.e. creating a composite score), and since composite indicators are normally presented with the index scores and original data accessible underneath, the underlying data would normally be presented in its original form. Therefore, be careful, but also realise that data treatment is not unethical, its simply an assumption in a statistical process. Like any other step or assumption though, any data treatment should be carefully recorded. 8.1 Why treat data? There can be many reasons to treat data, but in composite indicators the main reason is to remove outliers or adjust heavily-skewed distributions. Outliers can exist because of errors in measurement and data processing, and should always be double-checked. But often, they are simply a reflection of reality. Outliers and skewed distributions are common in economics. One has to look no further than the in-built ASEM data set, and the denominating variables of population, GDP, energy consumption and country area: plotIndDist(WorldDenoms) To illustrate why this can be a problem, consider the following artificial example. library(plotly) library(magrittr) # normally-distributed data outlierdata &lt;- rnorm(100) # two outliers outlierdata &lt;- c(outlierdata, 10, 25) # plot plot_ly(x = outlierdata, type = &quot;histogram&quot;) Here we have the large majority of observations with values which are crammed into the bottom fifth of the scale, and two observations that are much higher, i.e. they are outliers. If we were to normalise this distribution using a min-max method scaled to [0, 100], for example, this would produce an indicator where the large majority of units have a very low score. This might not reflect the narrative that you want to convey. It may be, for example, that values above the median are good, and in this case, values in the 1.5-2.5 range are very good. Two units have truly exception values, but that shouldnt take away from the fact that other units have values that are considered to be good. The problem is that by normalising to [0, 100], these units with very good values will have normalised scores of around 20 (out of 100), which when compared to other indicators, is a bad score. And this will be reflected in the aggregated score. In summary, the outlying values are dominating the scale, which reduces the discriminatory power of the indicator. A few things to consider here are that: The outlier problem in composite indicators is mostly linked to the fact that you are aggregating indicators. If you are not aggregating, the outlying values may not be so much of a problem. It might be that you want to keep the distribution as it is, and let the indicator be defined by its outliers. This will depend on the objectives of the index. If you do wish to do something about the outlying values, there are two possibilities. One is to treat the data, and this is described in the rest of this chapter The second is to use a normalisation method that is less sensitive to outliers - this is dealt with in the Normalisation chapter. 8.2 How to treat data If you decide to treat the data before normalising, there are a two main options. 8.2.1 Winsorisation The first is to Winsorise the data. Winsorisation involves reassigning outlying points to the next highest point, or to a percentile value. To do this manually, using the data from the previous section, it would look like this: # get position of maximum value imax &lt;- which(outlierdata==max(outlierdata)) # reassign with next highest value outlierdata[imax] &lt;- max(outlierdata[-imax]) plot_ly(x = outlierdata, type = &quot;histogram&quot;) # and let's do it again imax And now we have arrived back to a normal distribution with no outliers, which is well-spread over the scale. Of course, this has come at the cost of actually moving data points. However keep in mind that this is only done for the purposes of aggregation, and the original indicator data would still be retained. A helpful way of looking at Winsorisation is that it is like capping the scale: it is enough to know that certain units have the highest score, without needing to know that they are ten times higher than the other units. Clearly, outliers could also occur from the lower end of the scale, in which case Winsorisation would use the minimum values. Notice that in general, Winsorisation does not change the shape of the distribution, apart from the outlying values. Therefore it is suited to distributions like the example, which are well-spread except for some few outlying values. 8.2.2 Transformation The second option is to transform the distribution, by applying a transformation that changes all of the data points and thus the overall shape of the distribution. The most obvious transformation in this respect is to take the logarithm. Denoting \\(x\\) as the original indicator and \\(x&#39;\\) as the transformed indicator, this would simply be: \\[ x&#39; = \\ln(x) \\] This is a sensible choice because skewed distributions are often roughly log-normal, and taking the log of a log-normal distrinbution results in a normal distribution. However, this will not work for negative values. In that case, an alternative is: \\[ x&#39; = \\ln(x- \\text{min}(x)+a), \\; \\; \\text{where}\\; \\; a = 0.01(\\text{max}(x)-\\text{min}(x)) \\] The logic being that by subtracting the minimum and adding something, all values will be positive, so they can be safely log-transformed. This formula is similar to that used in the COIN Tool, but with an adjustment. In the COIN Tool, \\(a=1\\), which, depending on the range of the indicator, can give only a gentle and sometimes near-linear transformation. By setting \\(a\\) to be a set percentage of the range, it is ensured that the shape of the transformation is consistent. A general family of transformations are called the Box-Cox transformations. These are given as: \\[ x&#39;(\\lambda) = \\begin{cases} \\frac{x^\\lambda-1}{\\lambda} &amp; \\text{if}\\ \\lambda \\neq 0 \\\\ \\ln(x) &amp; \\text{if}\\ \\lambda = 0 \\end{cases} \\] In other words, a log transformation or a power transformation. The Box Cox transformation is often accompanied by an optimisation which chooses the \\(\\lambda\\) value to best approximate the normal distribution. Comparing to Winsorisation, log transforms and Box Cox transforms perform the transformation on every data point. To see the effect of this, we can take the log of one of the denominator indicators in WorldDenoms, the national GDPs of over 200 countries in 2019. # make df with original indicator and log-transformed version df &lt;- data.frame(GDP = WorldDenoms$Den_GDP, LogGDP = log(WorldDenoms$Den_GDP)) plotIndDist(df, type = &quot;Histogram&quot;) And there we see a beautiful normal distribution as a result. 8.3 Data treatment in COINr COINr data treatment has a number of options and gives the user a fairly high degree of control in terms of which kind of treatment should be applied, either for all indicators at once, or for individual indicators. We will first explore the default data treatment options that are applied to all indicators, then see how to make exceptions and specific treatment requests. 8.3.1 Global treatment The default treatment in COINr is similar to that applied in the COIN Tool, and widely used in composite indicator construction. It follows a basic process of trying to Winsorise up to a specified limit, followed by a transformation if the Winsorisation does not sufficiently correct the distribution. In short, for each indicator it: Checks skew and kurtosis values If absolute skew is greater than a threshold (default 2) AND kurtosis is greater than a threshold (default 3.5): Successively Winsorise up to a specified maximum number of points. If either skew or kurtosis goes below thresholds, stop. If after reaching the maximum number of points, both thresholds are still exceeded, then: Perform a transformation This is the default process, although individual specifications can be made for individual indicators - see the next section. The data treatment function in COINr is called treat(). To perform a default data treatment, we can simply use the COIN in, COIN out approach as usual. # build ASEM data set, up to denomination ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ASEM &lt;- denominate(ASEM) # treat at defaults ASEM &lt;- treat(ASEM, dset = &quot;Denominated&quot;, winmax = 5) # Check which indicators were treated, and how library(dplyr, quietly = T, verbose = F) library(magrittr) ASEM$Analysis$Treated$TreatSummary %&gt;% filter(Treatment != &quot;None&quot;) ## IndCode Low High TreatSpec Treatment ## V2 Flights 0 1 Default, winmax = 5 Winsorised 1 points ## V4 Bord 0 3 Default, winmax = 5 Winsorised 3 points ## V6 Gas 0 3 Default, winmax = 5 Winsorised 3 points ## V10 Services 0 4 Default, winmax = 5 Winsorised 4 points ## V11 FDI 0 2 Default, winmax = 5 Winsorised 2 points ## V13 ForPort 0 3 Default, winmax = 5 Winsorised 3 points ## V17 CostImpEx 0 1 Default, winmax = 5 Winsorised 1 points ## V18 Tariff 0 3 Default, winmax = 5 Winsorised 3 points ## V23 StMob 0 2 Default, winmax = 5 Winsorised 2 points ## V26 CultServ 0 2 Default, winmax = 5 Winsorised 2 points ## V35 Forest 0 2 Default, winmax = 5 Winsorised 2 points ## V36 Poverty 0 3 Default, winmax = 5 Winsorised 3 points ## V41 NGOs 0 3 Default, winmax = 5 Winsorised 3 points ## V43 FemLab 1 0 Default, winmax = 5 Winsorised 1 points The results of treat() are a new data set .Data$Treated with the treated data in it, and a details of the data treatment in $Analysis$Treated, including TreatSummary (shown above), which gives a summary of the data treatment specified and actually applied for each indicator. In this case, all indicators were successfully brought within the specified skew and kurtosis limits by Winsorisation, within the specified Winsorisation limit winmax of five points. To see what happens if winmax is exceeded, we will lower the threshold to winmax = 3. Additionally, we can control what type of log transform should be applied when winmax is exceeded, using the deflog argument. We will set deflog = \"CTlog\" which ensures that negative values will not cause errors. # treat at defaults ASEM &lt;- treat(ASEM, dset = &quot;Denominated&quot;, winmax = 3, deflog = &quot;CTlog&quot;) # Check which indicators were treated, and how ASEM$Analysis$Treated$TreatSummary %&gt;% filter(Treatment != &quot;None&quot;) ## IndCode Low High TreatSpec Treatment ## V2 Flights 0 1 Default, winmax = 3 Winsorised 1 points ## V4 Bord 0 3 Default, winmax = 3 Winsorised 3 points ## V6 Gas 0 3 Default, winmax = 3 Winsorised 3 points ## V10 Services 0 0 Default, winmax = 3 CTLog (exceeded winmax) ## V11 FDI 0 2 Default, winmax = 3 Winsorised 2 points ## V13 ForPort 0 3 Default, winmax = 3 Winsorised 3 points ## V17 CostImpEx 0 1 Default, winmax = 3 Winsorised 1 points ## V18 Tariff 0 3 Default, winmax = 3 Winsorised 3 points ## V23 StMob 0 2 Default, winmax = 3 Winsorised 2 points ## V26 CultServ 0 2 Default, winmax = 3 Winsorised 2 points ## V35 Forest 0 2 Default, winmax = 3 Winsorised 2 points ## V36 Poverty 0 3 Default, winmax = 3 Winsorised 3 points ## V41 NGOs 0 3 Default, winmax = 3 Winsorised 3 points ## V43 FemLab 1 0 Default, winmax = 3 Winsorised 1 points # Compare before and after treatment iplotIndDist2(ASEM, dsets = c(&quot;Denominated&quot;, &quot;Treated&quot;), icodes = &quot;Services&quot;, ptype = &quot;Histogram&quot;) Now we can see from the table that the Services indicator has had a log transformation applied, and from the histograms that the skewness has been treated. The range of the indicator is now different, and this introduces negative values, however this will be anyway fixed in the normalisation step. The important thing here is the shape of the distribution. Other options for deflog are log (a standard log transform), GIIlog, which is a scaled log transformation used by the Global Innovation Index, and boxcox, which uses a Box-Cox transformation with the \\(\\lambda\\) parameter set by the boxlam argument to treat(). Optimised \\(\\lambda\\) values are not currently available in COINr, though this may be considered for future versions, along with custom transformation functions. A further point to note is that Winsorisation can work in two ways in COINr, specified by the winchange argument. If set to FALSE, then Winsorisation works in the following way: Perform an initial check of skew and kurtosis (call them \\(s_0\\) and \\(k_0\\)). Only if both exceed thresholds, continue to 2. Otherwise no treatment is applied. If \\(s_0\\) is positive (usually the case), begin by Winsorising the highest value (assign the value of the second-highest point). If \\(s_0\\) is negative, begin by Winsorising the lowest value (assign the value of the second-lowest point). If either skew or kurtosis thesholds are below thresholds, stop, otherwise Go to 2. if \\(s_0\\) was positive, or 3. if \\(s_0\\) was negative. Notably, the direction of the Winsorisation here is always the same, i.e. if \\(s_0\\) was positive, Winsorisation will always be applied to the highest values. The only drawback here is that the sign of the skew can conceivably change during the Winsorisation process. For that reason, if you set winchange = TRUE (which is anyway default), the function will check after every Winsorised point to see whether to Winsorise high values or low values. It might seem unlikely that the skew could change sign from one iteration to the next, and still remain outside the absolute threshold. But this has been observed to happen in some cases. Finally, you can set the skewness and kurtosis threshold values to your favourite values using the t_skew and t_kurt arguments to treat(). 8.3.2 Individual treatment If you want more control over the treatment of individual indicators, this can be done by specifying the optional individual and indiv_only arguments. individual is a data frame which specifies the treatment to apply to specific indicators. It looks like this: # Example of &quot;individual&quot; data frame spec individual = data.frame( IndCode = c(&quot;Bord&quot;, &quot;Services&quot;, &quot;Flights&quot;, &quot;Tariff&quot;), Treat = c(&quot;win&quot;, &quot;log&quot;, &quot;none&quot;, &quot;boxcox&quot;), Winmax = c(10, NA, NA, NA), Thresh = c(&quot;thresh&quot;, NA, NA, NA), Boxlam = c(NA, NA, NA, 2) ) individual ## IndCode Treat Winmax Thresh Boxlam ## 1 Bord win 10 thresh NA ## 2 Services log NA &lt;NA&gt; NA ## 3 Flights none NA &lt;NA&gt; NA ## 4 Tariff boxcox NA &lt;NA&gt; 2 The IndCode column is the list of indicators to apply a specific treatment. The Treat column dictates which treatment to apply: options are either Winsorise (win), or any of the deflog options described previously. The Winmax column gives the maximum number of points to Winsorise for that indicator. This value is only used if the corresponding row of Treat is set to win. The Thresh column specifies whether to use skew/kurtosis thresholds or not. If thresh, it uses the thresholds specified in the thresh argument of treat(). If instead it is set at NA, then it will ignore the thresholds and force the Winsorisation to Winsorise exactly winmax points. Finally, the Boxlam column allows an individual setting of the boxlam parameter if the corresponding entry of the Treat column is set to boxcox. If Individual is specified, all of these columns need to be present regardless of the individual treatments specified. Where entries are not needed, you can simply put NA. An important point is that if the treatment specified in the Treat column is forced upon the specified indicator, for example, if CTlog is specified, this will be applied, without any Winsorisation first. Conversely, if win is specified, this will be applied, without any subsequent log transform even if it fails to correct the distribution. Finally, the indiv_only argument specifies if TRUE, ONLY the indicators listed in individual are treated, and the rest get no treatment. Otherwise, if FALSE then the remaining indicators are subject to the default treatment process. Lets now put this into practice. ASEM &lt;- treat(ASEM, dset = &quot;Denominated&quot;, individual = individual, indiv_only = FALSE) # Check which indicators were treated, and how ASEM$Analysis$Treated$TreatSummary %&gt;% filter(Treatment != &quot;None&quot;) ## IndCode Low High TreatSpec Treatment ## V2 Flights 0 0 None ForcedNone ## V4 Bord 0 3 Forced Win, winmax = 10 Winsorised 3 points ## V6 Gas 0 3 Default, winmax = 6 Winsorised 3 points ## V10 Services 0 0 Forced log Log ## V11 FDI 0 2 Default, winmax = 6 Winsorised 2 points ## V13 ForPort 0 3 Default, winmax = 6 Winsorised 3 points ## V17 CostImpEx 0 1 Default, winmax = 6 Winsorised 1 points ## V18 Tariff 0 0 Forced Box-Cox Box Cox with lambda = 2 ## V23 StMob 0 2 Default, winmax = 6 Winsorised 2 points ## V26 CultServ 0 2 Default, winmax = 6 Winsorised 2 points ## V35 Forest 0 2 Default, winmax = 6 Winsorised 2 points ## V36 Poverty 0 3 Default, winmax = 6 Winsorised 3 points ## V41 NGOs 0 3 Default, winmax = 6 Winsorised 3 points ## V43 FemLab 1 0 Default, winmax = 6 Winsorised 1 points And this shows how some indicators have had certain treatments applied. Of course here, the individual treatments have been chosen arbitrarily, and the default treatment would have sufficed. In practice, you would apply individual treatment for specific indicators for good reasons, such as excessive skew, or not wanting to treat certain indicators for conceptual reasons. 8.4 Interactive visualisation As we have seen in this chapter, its essential to visualise your indicator distributions and check what treatment has been applied, and what the main differences are. This can be achieved with the plotting functions previously described, but could become cumbersome if you want to check many indicators. COINr has an built-in Shiny app which lets you compare indicator distributions interactively. Shiny is an R package which allows you to build interactive apps, dashboards and gadgets that can run R code. Shiny is a powerful tool for interactively exploring, communicating and presenting results, and can be used to host apps on the web. If you would like to know more about Shiny, I would recommend the book Mastering Shiny. To run the indicator visualisation app, simply run indDash(ASEM). This will open a window which should look like this: Figure 8.1: indDash screenshot The indDash() app is purely illustrative - it does not return anything back to R, but simply displays the distributions of the existing indicators. In the side panel, all data sets that exist in the .$Data folder are accessible, as well as any denominators, if they exist. Any of the indicators from any of these data sets can be plotted against each other. In the context of data treatment, this gives a fast way to check the effects of treating indicators. In the ASEM example, we can set the the data set of indicator 1 to Denominated, and the data set of indicator 2 to Treated. This will compare denominated against treated indicators, which is a sensible comparison since treat() was applied to .Data$Denominated in this case. When a treated data set is selected, a table of treated indicators appears in the side panel - this lists all indicators that had treatment applied to them, and gives the details of the treatments applied. You can click on column headings to sort by values, and this can give an easy way to see which indicators were treated. You can then select an indicator of interest using the dropdown menus. In the screenshot above, we are comparing Gas in the denominated and treated data sets. This gives several comparison plots: histograms of each indicator, violin plots, and a scatter plot of one indicator against the other, as well as overlaid histograms. As with any Plotly plot, any of these can be instantly downloaded using the camera button that appears when you hover over the plot. This might be useful for generating quick reports, for example. Below the violin plots are details of the skew and kurtosis of the indicator, and details of the treatment applied, if any. Finally, when comparing a treated indicator against its un-treated version, it can be helpful to check or un-check the Plot indicators on the same axis range box. Checking this box gives a like-for-like comparison which shows how the data points have moved, particularly useful in the case of Winsorisation. Unchecking it may be more helpful to compare the shapes of the two distributions. While indDash() has specific features for treated data, it is also useful as a general purpose tool for fast indicator visualisation and exploration. It can help, for example, to observe trends and relationships between indicators and denominators, and will work with the aggregated data, i.e. to see how the index may depend on its indicators. "],["normalisation.html", "Chapter 9 Normalisation 9.1 Approaches 9.2 Normalisation in COINr 9.3 Individual normalisation", " Chapter 9 Normalisation Normalisation is the operation of bringing indicators onto comparable scales so that they can be aggregated more fairly. To see why this is necessary, consider aggregating GDP values (billions or trillions of dollars) with percentage tertiary graduates (tens of percent). Average values here would make no sense because one is on a completely different scale to the other. 9.1 Approaches 9.1.1 First: adjust direction Indicators can either be positively or negatively related to the concept that you are trying to measure. For example, in an index of quality of life, median income would probably be a positive indicator. Prevalance of malnourishment would be a negative indicator (higher values should give lower scores in quality of life). Accounting for these differences is considered part of the normalisation step. Indicators loaded into COINr should usually have a Direction column in the IndMeta input to assemble(), which is 1 for positive indicators, and -1 for negative indicators. With this information, normalisation is a two step procedure: Multiply the values of each indicator by their corresponding direction value (either 1 or -1). Apply one of the normalisation methods described below. Its that simple. COINr has this built in, so you dont need to do anything other than specify the directions. 9.1.2 Linear transformations Normalisation is relatively simple but there are still a number of different approaches which have different properties. Perhaps the most straightforward and intuitive option (and therefore probably the most widely used) is called the min-max transformation. This is a simple linear function which rescales the indicator to have a minimum value \\(l\\), a maximum value \\(u\\), and consequently a range \\(u-l\\), and is as follows: \\[ \\tilde{x}_{\\text{min}} = \\frac{ x - x_{\\text{min}} }{ x_{\\text{max}} - x_{\\text{min}} } \\times (u-l) + l\\] where \\(\\tilde{x}\\) is the normalised indicator value. For example, if \\(l=0\\) and \\(u=100\\) this will rescale the indicator to lie exactly onto the interval \\([0, 100]\\). The transformation is linear because it does not change the shape of the distribution, it simply shrinks or expands it, and moves it. A similar transformation is to take z-scores, which instead use the mean and standard deviation as reference points: \\[ \\tilde{x}_{\\text{min}} = \\frac{ x - \\mu_x }{ \\sigma_x } \\times a + b\\] where \\(\\mu_x\\) and \\(\\sigma_x\\) are the mean and standard deviation of \\(x\\). The indicator is first re-scaled to have mean zero and standard deviation of one. Then it is scaled by a factor \\(a\\) and moved by a distance \\(b\\). This is very similar to the min-max transformation in that it can be reduced to multiplying by a factor and adding a constant, which is the definition of a linear transformation. However, the two approaches have different implications. One is that Z scores will generally be less sensitive to outliers, because the standard deviation is less dependent on an outlying value than the minimum or maximum. Following the min-max and z-score, the general linear transformation is defined as: \\[ \\tilde{x} = \\frac{ x - p }{ q } \\times a + b\\] and it is fairly straightforward to see how z-scores and the min-max transformations are special cases of this. 9.1.3 Nonlinear transformations A simple nonlinear transformation is the rank transformation. \\[ \\tilde{x} = \\text{rank}(x)\\] where the ranks should be defined so that the lowest indicator value has a rank of 1, the second lowest a rank of 2, and so on. The rank transformation is attractive because it automatically eliminates any outliers. Therefore there would not usually be any need to treat the data previously. However, it converts detailed indicator scores to simple ranks, which might be too reductionist for some. Its worth pointing out that there are different ways to rank values, because of how ties (units with the same score) are handled. To read about this, just call ?rank in R. Similar approaches to simple ranks include Borda scores, which are simply the ranks described above but minus 1 (so the lowest score is 0 instead of 1), and percentile ranks. 9.1.4 Distances Another approach is to use the distance of each score to some reference value. Possibilities here are the (normalised) distance to the maximum value of the indicator: \\[ \\tilde{x} = 1 - \\frac{\\text{max}(x) - x}{\\text{max}(x) - \\text{min}(x)}\\] the fraction of the maximum value of the indicator: \\[ \\tilde{x} = \\frac{x}{\\text{max}(x)}\\] the distance to a specified unit value in the indicator: \\[ \\tilde{x} = 1 - \\frac{x_u - x}{\\text{max}(x) - \\text{min}(x)}\\] where \\(x_u\\) is the value of unit \\(u\\) in the indicator. This is useful for benchmarking against a reference country, for example. Another possibility is to normalise against indicator targets. This approach is used for example in the EU2020 Index, which used European targets on environment, education and employment issues (among others). \\[ \\tilde{x} = \\text{min} \\left[1, \\ 1 - \\frac{\\text{targ}(x) - x}{\\text{max}(x) - \\text{min}(x)} \\right]\\] where \\(\\text{targ}(x)\\) is the target for the indicator. In this case, any value that exceeds the target is set to 1, i.e. exceeding the target is counted the same as exactly meeting it. There is also an issue of what to use to scale the distance: here the range of the indicator is used, but one could also use \\(\\text{targ}(x) - \\text{min}(x)\\) or perhaps some other range. 9.2 Normalisation in COINr The normalisation function in COINr is imaginatively named normalise(). It has the following main features: A wide range of normalisation methods, including custom func Customisable parameters for normalisation Possibility to specify detailed individual treatment for each indicator The function looks like this: # don&#39;t run this chunk, just for illustration (will throw error if run) normalise &lt;- function(COIN, ntype=&quot;minmax&quot;, npara = NULL, icodes = NULL, dset = &quot;Raw&quot;, directions = NULL, individual = NULL, indiv_only = FALSE, out2 = NULL){ As an input, it takes a COIN or data frame, as usual. It outputs a new dataset to the COIN .$Data$Normalised, or a normalised data frame if out2 = \"df\". Default normalisation (min-max, scaled between 0 and 100) can be achieved by simply calling: library(COINr) # Build ASEM index up to denomination ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ASEM &lt;- denominate(ASEM) # Default normalisation (min max scaled between 0 and 100) on denominated data set ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;) # compare one of the indicators iplotIndDist2(ASEM, dsets = c(&quot;Denominated&quot;, &quot;Normalised&quot;), icodes = &quot;TertGrad&quot;, ptype = &quot;Scatter&quot;) This plot also illustrates the linear nature of the min-max transformation. You can select the normalisation type of the normalise() function using the ntype and accompanying npara arguments, where the latter is an object which specifies any parameters for the type of normalisation selected. ntype = \"minmax\" yields a min-max transformation that scales each indicator onto an interval specified by npara, e.g. if npara = c(0,10) the indicators will scale to [0, 10]. ntype = \"zscore\" scales the indicator to have a mean and standard deviation specified by npara, e.g. if npara = c(0,1) the indicator will have mean zero and standard deviation 1. ntype = \"scaled\" is a general linear transformation defined by npara = c(a,b) which subtracts a and divides by b. ntype = \"rank\" replaces indicator scores with their corresponding ranks, such that the highest scores have the largest rank values. Ties take average rank values. Here npara is not used. ntype = \"borda\" is similar to ntype = \"rank\" but uses Borda scores, which are simply rank values minus one. npara is not used. ntype = \"prank\" gives percentile ranks. npara is not used. ntype = \"fracmax\" scales each indicator by dividing the value by the maximum value of the indicator. npara is not used. ntype = \"dist2ref\" gives the distance to a reference unit, defined by npara. For example, if npara = \"AUT\" then all scores will be normalised as the distance to the score of unit AUT in each indicator (divided by the range of the indicator). This is useful for benchmarking against a set unit, e.g. a reference country. Scores ntype = \"dist2max\" gives the normalised distance to the maximum of each indicator. ntype = \"dist2targ\" gives the normalised distance to indicator targets. Targets are specified, in the IndMeta argument of assemble(), and will be in the COIN at .$Input$IndMeta. Any scores that exceed the target will be capped at a normalised value of one. ntype = \"custom\" allows to pass a custom function to apply to every indicator. For example, npara = function(x) {x/max(x, na.rm = T)} would give the fracmax normalisation method described above. ntype = \"none\" the indicator is not normalised. The normalise() function also allows you to specify the directions of the indicators using the argument directions. If this is not specified, the directions will be taken from the indicator metadata input to assemble(). If they are not present here, all directions will default to positive. 9.3 Individual normalisation Indicators can be normalised using individual specifications in a similar way to the treat() function. This is specified by the following two arguments to normalise(): individual A list of named lists specifiying individual normalisation to apply to specific indicators. Should be structured so that the name of each sublist should be the indicator code. The the list elements are: .$ntype is the type of normalisation to apply (any of the options mentioned above) .$npara is a corresponding object or parameters that are used by ntype indiv_only Logical. As with treat(), if this is set to FALSE (default), then the indicators that are not specified in individual will be normalised according to the ntype and npara arguments specified in the function argument. Otherwise if TRUE, only the indicators in individual will be normalised, and the others will be unaffected. The easiest way to clarify this is with an example. In the following, we will apply min-max, scaled to [0, 1] to all indicators, except for Flights which will be normalised using Borda scores, Renew which will remain un-normalised, and Ship which will be scaled as a distance to the value of Singapore. indiv = list( Flights = list(ntype = &quot;borda&quot;), Renew = list(ntype = &quot;none&quot;), Ship = list(ntype = &quot;dist2ref&quot;, npara = &quot;SGP&quot;) ) # Minmax in [0,1] for all indicators, except custom individual normalisation # for those described above ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;, ntype = &quot;minmax&quot;, npara = c(0,1), individual = indiv, indiv_only = FALSE) We can visualise the new ranges of the data. plotIndDist(ASEM, dset = &quot;Normalised&quot;, icodes = c(&quot;Flights&quot;, &quot;Renew&quot;, &quot;Ship&quot;, &quot;Goods&quot;), type = &quot;Dot&quot;) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. This example is meant to be illustrative of the functionality of normalise(), rather than being a sensible normalisation strategy, because the indicators are now on very different ranges, from the default minmax range of [0, 1] (Goods in this case), to the unchanged scale of Renew and the Borda rank scale of Flights. Notice also the scaling of Ship, which has values above zero because the reference country, Singapore, is does not have the maximum value of the indicator. In practice, if different normalisation strategies are selected, it is a good idea to keep the indicators on similar ranges, otherwise the effects will be very unequal in the aggregation step. "],["aggregation.html", "Chapter 10 Aggregation 10.1 Weighting 10.2 Approaches 10.3 Aggregation in COINr", " Chapter 10 Aggregation Aggregation is the operation of combining multiple indicators into one value. Many composite indicators have a hierarchical structure, so in practice this often involves multiple aggregations, for example aggregating groups of indicators into aggregate values, then aggregating those values into higher-level aggregates, and so on, until the final index value. Aggregating should almost always be done on normalised data, unless the indicators are already on very similar scales. Otherwise the relative influence of indicators will be very uneven. Of course you dont have to aggregate indicators at all, and you might be content with a scoreboard, or perhaps aggregating into several aggregate values rather than a single index. However, consider that aggregation should not substitute the underlying indicator data, but complement it. Overall, aggregating indicators is a form of information compression - you are trying to combine many indicator values into one, and inevitably information will be lost. As long as this is kept in mind, and indicator data is presented and made available along side aggregate values, then aggregate (index) values can complement indicators and be used as a useful tool for summarising the underlying data, and identifying overall trends and patterns. 10.1 Weighting Many aggregation methods involve some kind of weighting, i.e. coefficients that define the relative weight of the indicators/aggregates in the aggregation. In order to aggregate, weights need to first be specified, but to effectively adjust weights it is necessary to aggregate. This chicken and egg conundrum is best solved by aggregating initially with a trial set of weights, perhaps equal weights, then seeing the effects of the weighting, and making any weight adjustments necessary. For this reason, weighting is dealt with in the following chapter on Weighting. 10.2 Approaches 10.2.1 Means The most straightforward and widely-used approach to aggregation is the weighted arithmetic mean. Denoting the indicators as \\(x_i \\in \\{x_1, x_2, ... , x_d \\}\\), a weighted arithmetic mean is calculated as: \\[ y = \\frac{\\sum_{i=1}^d x_iw_i}{\\sum_{i=1}^d w_i} \\] where the \\(w_i\\) are the weights corresponding to each \\(x_i\\). Here, if the weights are chosen to sum to 1, it will simplify to the weighted sum of the indicators. In any case, the weighted mean is scaled by the sum of the weights, so weights operate relative to each other. Clearly, if the index has more than two levels, then there will be multiple aggregations. For example, there may be three groups of indicators which give three separate aggregate scores. These aggregate scores would then be fed back into the weighted arithmetic mean above to calculate the overall index. The arithmetic mean has perfect compensability, which means that a high score in one indicator will perfectly compensate a low score in another. In a simple example with two indicators scaled between 0 and 10 and equal weighting, a unit with scores (0, 10) would be given the same score as a unit with scores (5, 5)  both have a score of 5. An alternative is the weighted geometric mean, which uses the product of the indicators rather than the sum. \\[ y = \\left( \\prod_{i=1}^d x_i^{w_i} \\right)^{1 / \\sum_{i=1}^d w_i} \\] This is simply the product of each indicator to the power of its weight, all raised the the power of the inverse of the sum of the weights. The geometric mean is less compensatory than the arithmetic mean  low values in one indicator only partially substitute high values in others. For this reason, the geometric mean may sometimes be preferred when indicators represent essentials. An example might be quality of life: a longer life expectancy perhaps should not compensate severe restrictions on personal freedoms. A third type of mean, in fact the third of the so-called Pythagorean means is the weighted harmonic mean. This uses the mean of the reciprocals of the indicators: \\[ y = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i/x_i} \\] The harmonic mean is the the least compensatory of the the three means, even less so than the geometric mean. It is often used for taking the mean of rates and ratios. 10.2.2 Other methods The weighted median is also a simple alternative candidate. It is defined by ordering indicator values, then picking the value which has half of the assigned weight above it, and half below it. For ordered indicators \\(x_1, x_2, ..., x_d\\) and corresponding weights \\(w_1, w_2, ..., w_d\\) the weighted median is the indicator value \\(x_m\\) that satisfies: \\[ \\sum_{i=1}^{m-1} w_i \\leq \\frac{1}{2}, \\: \\: \\text{and} \\sum_{i=m+1}^{d} w_i \\leq \\frac{1}{2} \\] The median is known to be robust to outliers, and this may be of interest if the distribution of scores across indicators is skewed. Another somewhat different approach to aggregation is to use the Copeland method. This approach is based pairwise comparisons between units and proceeds as follows. First, an outranking matrix is constructed, which is a square matrix with \\(N\\) columns and \\(N\\) rows, where \\(N\\) is the number of units. The element in the \\(p\\)th row and \\(q\\)th column of the matrix is calculated by summing all the indicator weights where unit \\(p\\) has a higher value in those indicators than unit \\(q\\). Similarly, the cell in the \\(q\\)th row and \\(p\\)th column (which is the cell opposite on the other side of the diagonal), is calculated as the sum of the weights unit where \\(q\\) has a higher value than unit \\(p\\). If the indicator weights sum to one over all indicators, then these two scores will also sum to 1 by definition. The outranking matrix effectively summarises to what extent each unit scores better or worse than all other units, for all unit pairs. The Copeland score for each unit is calculated by taking the sum of the row values in the outranking matrix. This can be seen as an average measure of to what extent that unit performs above other units. Clearly, this can be applied at any level of aggregation and used hierarchically like the other aggregation methods presented here. In some cases, one unit may score higher than the other in all indicators. This is called a dominance pair, and corresponds to any pair scores equal to one (equivalent to any pair scores equal to zero). The percentage of dominance pairs is an indication of robustness. Under dominance, there is no way methodological choices (weighting, normalisation, etc.) can affect the relative standing of the pair in the ranking. One will always be ranked higher than the other. The greater the number of dominance (or robust) pairs in a classification, the less sensitive country ranks will be to methodological assumptions. 10.3 Aggregation in COINr Many will be amazed to learn that the function to aggregate in COINr is called aggregate(). First, lets build the ASEM data set up to the point of normalisation, then aggregate it using the default approaches. library(COINr) # Build ASEM data up to the normalisation step # Assemble data and metadata into a COIN ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- # Denominate as specified in metadata ASEM &lt;- denominate(ASEM) # Normalise using default method: min-max scaled between 0 and 100. ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;) # Now aggregate using default method: arithmetic mean, using weights found in the COIN ASEM &lt;- aggregate(ASEM, dset = &quot;Normalised&quot;) # show last few columns of aggregated data set ASEM$Data$Aggregated[(ncol(ASEM$Data$Aggregated)-5): ncol(ASEM$Data$Aggregated)] ## # A tibble: 51 x 6 ## Environ Social SusEcFin Conn Sust Index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.0 67.0 65.7 50.4 67.9 59.1 ## 2 55.9 86.3 53.5 54.8 65.3 60.0 ## 3 56.4 54.0 62.5 42.4 57.6 50.0 ## 4 76.0 54.8 55.5 45.9 62.1 54.0 ## 5 61.9 59.5 31.1 45.5 50.8 48.2 ## 6 53.7 61.1 72.0 48.9 62.3 55.6 ## 7 72.5 79.8 62.7 51.0 71.7 61.4 ## 8 35.7 68.3 70.1 49.5 58.0 53.8 ## 9 52.9 82.1 63.8 48.2 66.3 57.2 ## 10 66.5 60.8 54.2 47.0 60.5 53.7 ## # ... with 41 more rows By default, the aggregation function performs the following steps: Uses the weights that were attached to IndMeta and AggMeta in assemble() Aggregates hierarchically (with default method of weighted arithmetic mean), following the index structure specified in IndMeta and using the data specified in dset Creates a new data set .$Data$Aggregated, which consists of the data in dset, plus extra columns with scores for each aggregation group, at each aggregation level. Like other COINr functions aggregate() has arguments dset which specifies which data set to aggregate, and out2 which specifies whether to output an updated COIN, or a data frame. Unlike other COINr functions however, aggregate() only works on COINs, because it requires a number of inputs, including data, weights, and the index structure. The types of aggregation supported by aggregate() are specified by agtype from the following options: agtype = arith_mean uses the arithmetic mean, and is the default. agtype = geom_mean uses the geometric mean. This only works if all data values are positive, otherwise it will throw an error. agtype = harm_mean uses the harmonic mean. This only works if all data values are non-zero, otherwise it will throw an error. agtype = median uses the weighted median agtype = copeland uses the Copeland method. This may take a few seconds to process depending on the number of units, because it involves pairwise comparisons across units. agtype = custom allows you to pass a custom aggregation function. For the last option here, if agtype = custom then you need to also specify the custom function via the agfunc argument. As an example: # define an aggregation function which is the weighted minimum weightedMin &lt;- function(x,w) min(x*w, na.rm = TRUE) # pass to aggregate() and aggregate ASEM &lt;- aggregate(ASEM, dset = &quot;Normalised&quot;, agtype = &quot;custom&quot;, agfunc = weightedMin, out2 = &quot;df&quot;) Any custom function should have the form functionName &lt;- function(x,w), i.e. it accepts x and w as vectors of indicator values and weights respectively, and returns a scalar aggregated value. Ensure that NAs are handled (e.g. set na.rm = T) if your data has missing values, otherwise NAs will be passed through to higher levels. A number of sophisticated aggregation approaches and linked weighting methods are available in the compind package. These can nicely complement the features in COINr and may be of interest to those looking for more technical approaches to aggregation, such as those based on benefit of the doubt methods. A further argument, agtype_bylevel, allows specification of different normalisation types at different aggregation levels. For example, agtype_bylevel = c(\"arith_mean\", \"geom_mean\", \"median\") would use the arithmetic mean at the indicator level, the geometric mean at the pillar level, and the median at the sub-index level (for the ASEM data structure). Finally, alternative weights can be used for aggregation by specifying the agweights argument. This can either be: NULL, in which case it will use the weights that were attached to IndMeta and AggMeta in GII_assemble (if they exist), or A character string which corresponds to a named list of weights stored in .$Parameters$Weights. You can either add these manually or through rew8r (see chapter on Weighting). E.g. entering agweights = \"Original\" will use the original weights read in on assembly. This is equivalent to agweights = NULL. A list of weights to use in the aggregation, where each element of the list is a numeric vector of weights for a given level. Now that the data has been aggregated, a natural next step is to explore the results. This is dealt with in the chapter on [Results visualisation]. "],["weighting-1.html", "Chapter 11 Weighting 11.1 Approaches to weighting 11.2 Weighting tools in COINr 11.3 Effective weights 11.4 Final points", " Chapter 11 Weighting Strictly speaking, weighting comes before aggregation. However, in order to understand the effects of weights, we need to aggregate the index first. Weighting in composite indicators is a thorny issue, which attracts considerable attention and is often one of the main focuses of critics. Weighting openly expresses a subjective opinion on the relative importance of each indicator relative to the others, and this opinion can easily be contested. While this criticism definitely has some basis, weights can be viewed as a type of model parameter, and any model (e.g. engineering models, climate models, economic models) is full of uncertain parameters. In large models, these parameters are less evident since they are inside rather complex model code. Weights in composite indicators are easy to criticise since the model of a composite indicator is quite simple, usually using simple averages of indicator values. That said, weights do need to be carefully considered, taking into account at least: The relative conceptual importance of indicators: is indicator A more, less, or equally important to indicator B? The statistical implications of the weights How to communicate the weights to end users The last point is important if the index is for advocacy/awareness. Weights may be fine tuned to account for statistical considerations, but the result may make little sense to the public or the media. Overall, weighting is a large topic which cannot be covered in detail here. Nevertheless, this chapter gives some introduction and points to further references. 11.1 Approaches to weighting Broadly, weighting approaches can be divided into those that are based on expert judgment, and those that are based on statistical considerations. Then there are approaches that combine the two. 11.1.1 Equal weighting The most common approach to weighting is simply to make all weights equal to one another. This may seem like an unjustifiable simplification, but in practice, experts often give near-equal weights when asked, and statistical approaches may give weights that are so unequal that it may be hard to justify them to stakeholders. This is why many composite indicators use equal weights. A further consideration is that the results of a composite indicator are often less sensitive to their weights than you might think. That said, a number of other methods exist, some of which are discussed in the following sections. 11.1.2 Budget allocation Fundamentally, composite indicators involve breaking down a complex multidimensional concept into sub-concepts, and possibly breaking those sub-concepts down into sub-sub-concepts, and so on. Generally, you keep doing this until you arrive at the point where the (sub-)^n-concepts are sufficiently specific that they can be directly measured with a small group of indicators. Regardless of the aggregation level that you are at, however, some of the indicators (or aggregates) in the group are likely to be more or less important to the concept you are trying to measure than others. Take the example of university rankings. One might consider the following indicators to be relevant to the notion of excellence for a university: Number of publications in top-tier journals (research output) Teaching reputation, e.g. through a survey (teaching quality) Research funding from private companies (industry links) Percentage of international students (global renown) Mean earning of graduates (future prospects of students) These may all be relevant for the concept, but the relative importance is definitely debatable. For example, if you are a prospective student, you might prioritise the teaching quality and graduate earnings. If you are researcher, you might priorities publications and research funding. And so on. The two points to make here are: Indicators are often not equally important to the concept, and Different people have different opinions on what should be more or less important. Bringing this back to the issue of weights, it is important to make sure that the relative contributions of the indicators to the index scorers reflect the intended importance. This can be achieved by changing the weights attached to each indicator, and the weights of higher aggregation levels. How then do we understand which indicators should be the most important to the concept? One way is to simply use our own opinion. But this does not reflect a diversity of viewpoints. The budget allocation method is a simple way of eliciting weights from a group of experts. The idea is to get a group of experts on the concept, stakeholders and end-users, ideally from different backgrounds. Then each member of the group is given 100 coins which they can spend on the indicators. Members allocate their budget to each indicator, so that they give more of the budget to important indicators, and less to less important ones. This is a simple way of distributing weight to indicators, in a way that is easy for people to understand and to give their opinions. You can take the results, and take the average weight for each indicator. You can even infer a weight-distribution over each indicator which could feed into an uncertainty analysis. 11.1.3 Principle component analysis A very different approach is to use principle component analysis (PCA). PCA is a statistical approach, which rotates your indicator data into a new set of coordinates with special properties. One way of looking at indicator data is as data points in a multidimensional space. If we only have two indicators, then any unit can be plotted as a point on a two-dimensional plot, where the x-axis is the first indicator, and the y-axis is the second. Ind1 &lt;- 35 Ind2 &lt;- 60 plot(Ind1, Ind2, main = &quot;World&#39;s most boring plot&quot;) Each point on this beautiful base-R plot represents a unit (here we only have one). If we have three indicators, then we will have three axes (i.e. a 3D plot). If we have more than three, then the point lives in a hyperspace which we cant really visualise. Anyway, the main point here is that indicator data can be plotted as points in a space, where each axis (dimension) is an indicator. What PCA does, is that it rotates the data so that the axes are no longer indicators, but instead are linear combinations of indicators. And this is done so that the first axis is the linear combination of indicators that explains the most variance. If this is not making much sense (and if this is the first time you have encountered PCA then it probably wont), I would recommend to look at one of the many visualisations of PCA on the internet, e.g. this one. PCA is useful for composite indicators, because if you use an arithmetic mean, then you are using a linear combination of indicators. And the first principle component gives the weights that maximise the variance of the units. In other words, if you use PCA weights, you will have the best weights for discriminating between units, and for capturing as much information from the underlying indicators. This sounds perfect, but the downside is that PCA weights do not care about the relative importance of indicators. Typically, PCA assigns the highest weights to indicators with the highest correlations with other indicators, and this can result in a very unbalanced combination of indicators. Still, PCA has a number of nice properties, and has the advantage of being a purely statistical approach. 11.1.4 Weight optimisation If you choose to go for the budget allocation approach, to match weights to the opinions of experts, or indeed your own opinion, there is a catch that is not very obvious. Put simply, weights do not directly translate into importance. To understand why, we must first define what importance means. Actually there is more than one way to look at this, but one possible measure is to use the (nonlinear) correlation between each indicator and the overall index. If the correlation is high, the indicator is well-reflected in the index scores, and vice versa. If we accept this definition of importance, then its important to realise that this correlation is affected not only by the weights attached to each indicator, but also by the correlations between indicators. This means that these correlations must be accounted for in choosing weights that agree with the budgets assigned by the group of experts. In fact, it is possible to reverse-engineer the weights either analytically using a linear solution or numerically using a nonlinear solution. While the former method is far quicker than a nonlinear optimisation, it is only applicable in the case of a single level of aggregation, with an arithmetic mean, and using linear correlation as a measure. Therefore in COINr, the second method is used. 11.2 Weighting tools in COINr As seen in the chapter on Aggregation, weights are used to calculate the aggregated data set, and are one of the inputs to the aggregate() function. These can either be directly specified as a list, or as a character string which points to one of the sets of weights stored under .$Parameters$Weights. There is no limit to the number of sets of weights that can be stored here. So how do we create new sets of weights? 11.2.1 Manual weighting The simplest approach is to make manual adjustments. First, make a copy of the existing weights. library(COINr) # quietly build data set ASEM &lt;- build_ASEM() NewWeights &lt;- ASEM$Parameters$Weights$Original NewWeights ## $IndWeight ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [39] 1 1 1 1 1 1 1 1 1 1 1 ## ## [[2]] ## [1] 1 1 1 1 1 1 1 1 ## ## [[3]] ## [1] 1 1 ## ## [[4]] ## [1] 1 The structure of the weights is a list, with one element per aggregation level, and each element is a vector of weights for that level. For example here, NewWeights[[1]] is the indicator weights, NewWeights[[2]] is the pillar weights, and so on. We can just directly modify this. Lets say we want to set the weight of the connectivity sub-index to 0.5. NewWeights[[3]][1] &lt;- 0.5 NewWeights ## $IndWeight ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [39] 1 1 1 1 1 1 1 1 1 1 1 ## ## [[2]] ## [1] 1 1 1 1 1 1 1 1 ## ## [[3]] ## [1] 0.5 1.0 ## ## [[4]] ## [1] 1 Remember that weights are relative, and are re-scaled to sum to 1 during aggregation. To actually use these weights, we can either (a) input them directly in the aggregate() function, or first attach them to the COIN and the call aggregate(), pointing to the new set of weights. The latter option seems more sensible because that way, all our sets of weights are neatly stored. Also, we can access this in the reweighting app as explained in the next section. # put new weights in the COIN ASEM$Parameters$Weights$NewWeights &lt;- NewWeights # Aggregate again to get new results ASEM &lt;- aggregate(ASEM, agweights = &quot;NewWeights&quot;) Now the ASEM COIN contains the updated results using the new weights. 11.2.2 Interactive re-weighting with ReW8R Re-weighting manually, as we have just seen, is quite simple. However, depending on your objectives, weighting can be an iterative process along the lines of: Try a set of weights Examine correlations Check results, rankings Adjust weights Return to 1. Doing this at the command line can be time consuming, so COINr includes a re-weighting app called rew8r() which lets you interactively adjust weights and explore the effects of doing so. To run rew8r() you must have already aggregated your data using aggregate(). This is because every time weights are adjusted, rew8r() re-aggregates to find new results, and it needs to know which aggregation method you are using. So, with your pre-aggregated COIN at hand, simply run: ASEM &lt;- rew8r(ASEM) At this point, a window/tab should open in your browser with the rew8r() app, which looks something like this: Figure 11.1: rew8r() screenshot This may look a little over-complicated to begin with, but lets go through it step by step. 11.2.2.1 Correlations First of all, rew8r() is based around correlations. Why are correlations important? Because they give an idea of how each indicator is represented in the final index, and at other aggregation levels (see earlier discussion in this chapter). The rew8r() app allows you to check the correlations of anything against anything else. The two dropdown menus in the top left allow you to select which aggregation level to correlate against which other. In the screenshot above, the indicators (Level 1) are being correlated against the pillars (Level 2), but this can be changed to any of the levels in the index (the first level should always be below the second level otherwise it can cause errors). You can also select which type of correlation to examine, either Pearson (i.e. standard correlation), Spearman rank correlation, or Kendalls tau (an alternative rank correlation). The correlations themselves are shown in two plots - the first is the scatter plot in the upper right part of the window, which plots the correlation of each indicator in Level 1 on the horizontal axis with its parent in Level 2, against its weight (in Level 1) on the vertical axis. We will come back to this shortly. Figure 11.2: Heatmap screenshot (in rew8r) The second plot is a heatmap of the correlation matrix (between Level 1 and Level 2 in this case) which is shown in the lower right corner. This gives the correlation values between all the indicators/groups of each level as colours according to a discrete colour map. This colour map intends to highlight highly correlated indicators (green), as well as negative or low-correlated indicators (red). These thresholds can be adjusted by adjusting the low and high correlation threshold dropdown menus in the side panel (on the left). The aggregation groups are also shown as overlaid rectangles, and correlation values can be turned on and off using the checkbox below the heatmap. The point of this plot is to show at a glance, where there are very high or low correlations, possibly within aggregation groups. For example, indicators that are negatively correlated with their aggregation group can be problematic. Note that correlation heatmaps can also be generated independently of the app using the plotCorr() function - this is described more in the Multivariate analysis section. Highly-correlated indicators are also listed in a table at the lower middle of the window. The Ind-Ind tab flags any indicators that are above the high correlation threshold, and within the same aggregation group (the aggregation level immediately above the indicator level). Note that this is not dependent on the weights since it regards correlations between indicators only. The Cross-level tab instead gives a table which flags any correlations across the two levels selected in side panel, which are above the threshold. 11.2.2.2 Weighting Let us now turn back to the scatter plot. The scatter plot shows the correlation values on the horizontal axis, but also shows the weight of each indicator or aggregate in the first level selected in the dropdown menu. The purpose here is to show at a glance the weighting, and to make adjustments. To adjust a weight, either click on one of the points in the plot (hovering will show the names of the indicators or aggregates), or select the indicator/aggregate in the dropdown menu in the side panel under the Weighting subsection. Then adjust the weight using the slider. Doing this will automatically update all of the plots and tables, so you can see interactively how the correlations change as a result. Figure 11.3: Correlation scatter plot screenshots in rew8r(). Figure 11.4: Correlation scatter plot screenshots in rew8r(). Importantly, the scatter plot only shows correlations between each indicator/aggregate in the first level with its parent in the second selected level. When everything is plotted on one plot, this may not be so clear, so the Separate groups checkbox in the side panel changes the plot to multiple sub-plots, one for each group in the second selected level. This helps to show how correlations are distributed within each group. The results of the index, in terms of ranks and scores of units, can be found by switching to the Results tab in the main window. Other than adjusting individual weights, any set of weights that is stored in .$Parameters$Weights can be selected, and this will automatically show the correlations. The button Equal weights sets weights to be equal at the current level. Finally, if you have adjusted the weights and you wish to save the new set of weights back to the COIN, you can do this in the Weights output subsection of the side panel. The current set of weights is saved by entering a name and clicking the Save button. This saves the set of weights to a list, and when the app is closed (using the Close app button), it will return them to the updated COIN under .$Parameters$Weights, with the specified name. You can save multiple sets of weights in the same session, by making adjustments, and assigning a name, and clicking Save. When you close the app, all the weight sets will be attached to the COIN. If you dont click Close app, any new weight sets created in the session will be lost. 11.2.2.3 Remarks The rew8r() app is something of a work in progress - it is difficult to strike a balance between conveying the relevant information and conveying too much information. In future versions, it may be updated to become more streamlined. 11.2.3 Automatic re-weighting As discussed earlier, weighting can also be performed statistically, by optimising or targeting some statistical criteria of interest. COINr includes a few possibilities in this respect. 11.2.3.1 With PCA The cPCA() function is used as a quick way to perform PCA on COINs. This does two things: first, to output PCA results for any aggregation level; and second, to provide PCA weights corresponding to the loadings of the first principle component. As discussed above, this is the linear combination that results in the most explained variance. The other outputs of cPCA() are left to the Multivariate analysis section. Here, we will focus on weighting. To obtain PCA weights, simply run something like: ASEM &lt;- cPCA(ASEM, dset = &quot;Aggregated&quot;, aglev = 2) This calls the aggregated data set, and performs PCA on aggregation level 2, i.e. the pillars in the case of ASEM. The result is an updated COIN, with a new set of weights called .Parameters$Weights$PCA_AggregatedL2. This is automatically named according to the data set used and the aggregation level. An important point here is that this weighting represents optimal properties if the components (pillars here) are aggregated directly into a single value. In this example, it is not strictly correct because above the pillar level we have another aggregation (sub-index) before reaching the final index. This means that these weights wouldnt be strictly correct, because they dont account for the intermediate level. However, depending on the aggregation structure and weighting (i.e. if it is fairly even) it may give a fairly good approximation. Note: PCA weights have not been fully tested at the time of writing, treat with skepticism. 11.2.3.2 Weight optimisation An alternative approach is to optimise weight iteratively. This is in a way less elegant (and slower) because it uses numerical optimisation, but allows much more flexibility. The weightOpt() function gives several options in this respect. In short, it allows weights to be optimised to agree with either (a) some pre-specified importances (perhaps obtained by the budget allocation method, or perhaps simply equal importance), or (b) to maximise the information conveyed by the composite indicator. Starting with the first option, this is how it can be applied to our example data: ASEM &lt;- weightOpt(ASEM, itarg = &quot;equal&quot;, aglev = 3, out2 = &quot;COIN&quot;) ## iterating... squared difference = 0.000814548399872482 ## iterating... squared difference = 0.00116234608542398 ## iterating... squared difference = 0.00052279688155717 ## iterating... squared difference = 0.0002714288504979 ## iterating... squared difference = 4.19003122660245e-05 ## iterating... squared difference = 1.66726715407502e-06 ## iterating... squared difference = 0.000166401854209433 ## iterating... squared difference = 0.000357199714112591 ## iterating... squared difference = 4.91262049731255e-05 ## iterating... squared difference = 0.000189074238231706 ## iterating... squared difference = 2.15217457991656e-06 ## iterating... squared difference = 4.02429806062667e-05 ## iterating... squared difference = 1.04404840785834e-05 ## iterating... squared difference = 1.00833908145386e-05 ## iterating... squared difference = 2.40420217478915e-06 ## Optimisation successful! This targets the aggregation level 3 (sub-index), and the itarg argument here is set to equal, which means that the objective is to make the correlations between each sub-index and the index equal to one another. More details of the results of this optimisation are found in the .$Analysis$Weights folder of the object. In particular, we can see the final correlations and the optimised weights: ASEM$Analysis$Weights$OptimsedLev3$CorrResults ## Desired Obtained OptWeight ## 1 0.5 0.8971336 0.400 ## 2 0.5 0.8925119 0.625 In this case, the weights are slightly uneven but seem reasonable. The correlations are also almost exactly equal. The full set of weights is found as a new weight set under .$Parameters$Weights. Other possibilities with the weightOpt() function include: Specifying an unequal vector of target importances using the itarg argument. E.g. taking the sub-pillar level, specifying itarg = c(0.5, 1) would optimise the weights so that the first sub-pillar has half the correlation of the second sub-pillar with the index. Optimising the weights at other aggregation levels using the aglev argument Optimising over other correlation types, e.g. rank correlations, using the cortype argument Aiming to maximise overall correlations by setting optype = infomax Its important to point out that only one level can be optimised at a time, and the optimisation is conditional on the weights at other levels. For example, optmising at the sub-index level produces optimal correlations for the sub-indexes, but not for any other level. If we then optimise at lower levels, this will change the correlations of the sub-indexes! So probably the most sensible strategy is to optimise for the last aggregation level (before the index). That said, an advantage of weightOpt() is that since it is numerical, it can be used for any type of aggregation method, any correlation type, and with any index structure. It calls the aggregation method that you last used to aggregate the index, so it rebuilds the index according to your specifications. Other correlation types may be of interest when linear correlation (i.e. Pearson correlation) is not a sufficient measure. This is the case for example, when distributions are highly skewed. Rank correlations are robust to outliers and can handle some nonlinearity (but not non-monotonicity). Fully nonlinear correlation methods also exist but most of the time, relationships between indicators are fairly linear. Nonlinear correlation may be included in a future version of COINr. Setting optype = infomax changes the optimisation criterion, to instead maximise the sum of the correlations, which is equivalent to maximising the information transferred to the index, and is similar to PCA. Like PCA, this can however result in fairly unequal weights, and not infrequently in negative weights. The weightOpt() function uses a numerical optimisation algorithm which does not always succeed in finding a good solution. Optimisation is a large field in its own right, and is more difficult the more variables you optimise over. If weightOpt() fails to find a good solution, try: Decreasing the toler argument - this decreases the acceptable error between the target importance and the importance with the final set of weights. Increasing the maxiter argument - this is the number of iterations allowed to find a solution within the specified tolerance. By default it is set at 500. Even then, the algorithm may not always converge, depending on the problem. 11.3 Effective weights By now, you may know or have realised that the weight of an indicator is not only affected by its own weight, but also the weights of any aggregate groups to which it belongs. If the aggregation method is the arithmetic mean, the effective weight, i.e. the final weight of each indicator, including its parent weights, can be be easily obtained by COINrs effectiveWeight() function: # get effective weight info EffWeights &lt;- effectiveWeight(ASEM) EffWeights$EffectiveWeightsList ## $IndWeight ## [1] 0.01250000 0.01250000 0.01250000 0.01250000 0.01250000 0.01250000 ## [7] 0.01250000 0.01250000 0.02000000 0.02000000 0.02000000 0.02000000 ## [13] 0.02000000 0.03333333 0.03333333 0.03333333 0.01666667 0.01666667 ## [19] 0.01666667 0.01666667 0.01666667 0.01666667 0.01250000 0.01250000 ## [25] 0.01250000 0.01250000 0.01250000 0.01250000 0.01250000 0.01250000 ## [31] 0.03333333 0.03333333 0.03333333 0.03333333 0.03333333 0.01851852 ## [37] 0.01851852 0.01851852 0.01851852 0.01851852 0.01851852 0.01851852 ## [43] 0.01851852 0.01851852 0.03333333 0.03333333 0.03333333 0.03333333 ## [49] 0.03333333 ## ## [[2]] ## [1] 0.1000000 0.1000000 0.1000000 0.1000000 0.1000000 0.1666667 0.1666667 ## [8] 0.1666667 ## ## [[3]] ## [1] 0.5 0.5 ## ## [[4]] ## [1] 1 A nice way to visualise this is to call plotframework() (see Initial visualisation and analysis). effectiveWeight() also gives other info on the parents of each indicator/aggregate which may be useful for some purposes (e.g. some types of plots external to COINr). 11.4 Final points Weighting and correlations is a complex topic, which is important to explore and address. On the other hand, weighting and correlations are just one part of a composite indicator, and balancing and optimising weights probably not be pursued at the expense of building a confusing composite indicator that makes no sense to most people (depending on the context and application). "],["visualising-results.html", "Chapter 12 Visualising results", " Chapter 12 Visualising results COINr offers a number of options for visualising results "],["adjustments-and-comparisons.html", "Chapter 13 Adjustments and comparisons 13.1 Regeneration 13.2 Adjustments 13.3 Comparisons", " Chapter 13 Adjustments and comparisons Its fairly common to make adjustments to the index, perhaps in terms of alternative data sets, indicators, methodological decisions, and so on. COINr allows you to (a) make fast adjustments, and (b) to compare alternative versions of the index relatively easily. 13.1 Regeneration One of the key advantages of working within the COINrverse is that (nearly) all the methodology that is applied when building a composite indicator (COIN) is stored automatically in a folder of the COIN called .$Method. To see what this looks like: # load COINr if not loaded library(COINr) # build example COIN ASEM &lt;- build_ASEM() ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean # look in ASEM$Method folder in R Studio... Figure 13.1: Method folder in ASEM example The easiest way to view it is by looking in the viewer of R Studio as in the screenshot above. Essentially, the .$Method folder has one entry for each COINr function that was used to build the COIN, and inside each of these folders are the arguments to the function that were input. Notice that the names inside .$Method$Denomination correspond exactly to arguments to denominate(), for example. Every time a COINr construction function is run, the inputs to the function are automatically recorded inside the COIN. Construction functions are any of the following seven: Function Description assemble() Assembles indicator data/metadata into a COIN checkData() Data availability check and unit screening denominate() Denominate (divide) indicators by other indicators impute() Impute missing data using various methods treat() Treat outliers with Winsorisation and transformations normalise() Normalise data using various methods aggregate() Aggregate indicators into hierarchical levels, up to index These are the core functions that are used to build a composite indicator, from assembling from the original data, up to aggregation. One reason to do this is simply to have a record of what you did to arrive at the results. However, this is not the main reason (and in fact, it would be anyway good practice to make your own record by creating a script or markdown doc which records the steps). The real advantage is that results can be automatically regenerated with a handy function called regen(). To regenerate the results, simply run e.g.: ASEM2 &lt;- regen(ASEM) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean The regen() function reruns all functions that are recorded in the .$Method folder in the order they appear in the folder. In this example, it runs, in order, assemble(), denominate(), impute(), treat(), normalise()and aggregate(). This replicates exactly the results. But what is the point of that? Well, it means that we can make changes to the index, generally by altering parameters in the .$Method folder, and then rerun everything very quickly with a single command. This will be demonstrated in the following section. After that, we will also see how to compare between alternative versions of the index. Before that, there is one extra feature of regen() which is worth mentioning. The regen() function only runs the seven construction functions as listed above. These functions are very flexible and should encompass most needs for constructing a composite indicator. But what happens if you want to build in an extra operation, or operations, that are outside of these seven functions? It is possible to also include custom chunks of code inside the COIN. Custom chunks should be written manually using the quote() function, to a special folder .$Method$Custom. These chunks can be any type of code, but the important thing is to also know when to run the code, i.e. at what point in the construction process. More specifically, custom code chunks are written as a named list. The name of the item in the list specifies when to perform the operation. For example, AfterTreatment means to perform this immediately after the treatment step. Other options are e.g. AfterNormalisation or AfterScreening  in general it should be After followed by a name of one of the existing folders in the .$Method folder. The corresponding value in the list should be an operation which must be enclosed in the quote() function. Clearly, the list can feature multiple operations at different points in construction. This is slightly complicated, but can be clarified a bit with an example. Lets imagine that after Winsorisation, we would like to reset one of the Winsorised points to its original value. This is currently not possible inside the treat() function so has to be done manually. But we would like this operation to be kept when we regenerate the index with variations in methodology (e.g. trying different aggregation functions, weights, etc.). We create a list with one operation: resetting the point after the treatment step. # Create list. NOTE the use of the quote() function! custlist = list(AfterTreatment = quote(ASEM$Data$Treated$Bord[ASEM$Data$Treated$UnitCode==&quot;BEL&quot;] &lt;- ASEM2$Data$Imputed$Bord[ASEM$Data$Imputed$UnitCode==&quot;BEL&quot;])) # Add the list to the $Method$Custom folder ASEM2$Method$Custom &lt;- custlist Specifically, we have replaced the Winsorised value of Belgium, for the Bord indicator, with its imputed value (i.e. the value it had before it was treated). Now, when we regenerate the COIN using regen(), it will insert this extra line of code immediately after the treatment step. Using custom code may seem a bit confusing but it adds an extra layer of flexibility. It is however intended for small snippets of custom code, rather than large blocks of custom operations. If you are doing a lot of operations outside COINr, it may be better to do this in your own dedicated script or function, rather than trying to encode absolutely everything inside the COIN. 13.2 Adjustments Let us now explore how the COIN can be adjusted and regenerated. This will (hopefully) clarify why regenerating is a useful thing. The general steps for adjustments are: Copy the object Adjust the index methodology or data by editing the .$Method folder and/or the underlying data Regenerate the results Compare alternatives Copying the object is straightforward, and regeneration has been dealt with in the previous section. Comparison is also addressed in the following section. Here we will focus on adjustments and changing methodology. 13.2.1 Adding/removing indicators First of all, lets consider an alternative index where we decide to add or remove one or more indicators. There are different ways that we could consider doing this. A first possibility would be to manually create a new data frame of indicator data and indicator metadata, i.e. the IndData and IndMeta inputs for assemble(). This is fine, but we would have to start the index again from scratch and rebuild it. A better idea is that when we first supply the set of indicators to assemble(), we include all indicators that we might possibly want to include in the index, including e.g. alternative indicators with alternative data sources. Then we build different versions of the index using subsets of these indicators. This allows us to use regen() and therefore to make fast copies of the index. To illustrate, consider again the ASEM example. We can rebuild the ASEM index using a subset of the indicators by using the include or exclude arguments of the assemble() function. Because these are stored in .$Method, we can easily regenerate the new results. # Make a copy ASEM_NoLPIShip &lt;- ASEM # Edit method: exclude two indicators ASEM_NoLPIShip$Method$Assemble$exclude &lt;- c(&quot;LPI&quot;, &quot;Ship&quot;) # Regenerate results (suppress any messages) ASEM_NoLPIShip &lt;- regen(ASEM_NoLPIShip, quietly = TRUE) Note that, in the ASEM example, by default, ASEM$Method$Assemble doesnt exist because the include or exclude arguments of assemble() are empty, and these are the only arguments to assemble() that are recorded in .$Method$Assemble. In summary, we have removed two indicators, then regenerated the results using exactly the same methodology as used before. Importantly, include and exclude operate relative to the original data input to assemble(), i.e. the data found in .$Input$Original. This means that if we now were to make a copy of the version excluding the two indicators above, and exclude another different indicator: # Make a copy ASEM_NoBord &lt;- ASEM_NoLPIShip # Edit method: exclude two indicators ASEM_NoBord$Method$Assemble$exclude &lt;- &quot;Bord&quot; # Regenerate results ASEM_NoBord &lt;- regen(ASEM_NoBord, quietly = TRUE) ASEM_NoBord$Parameters$IndCodes ## [1] &quot;Flights&quot; &quot;Elec&quot; &quot;Gas&quot; &quot;ConSpeed&quot; &quot;Cov4G&quot; &quot;Goods&quot; ## [7] &quot;Services&quot; &quot;FDI&quot; &quot;PRemit&quot; &quot;ForPort&quot; &quot;Embs&quot; &quot;IGOs&quot; ## [13] &quot;UNVote&quot; &quot;CostImpEx&quot; &quot;Tariff&quot; &quot;TBTs&quot; &quot;TIRcon&quot; &quot;RTAs&quot; ## [19] &quot;Visa&quot; &quot;StMob&quot; &quot;Research&quot; &quot;Pat&quot; &quot;CultServ&quot; &quot;CultGood&quot; ## [25] &quot;Tourist&quot; &quot;MigStock&quot; &quot;Lang&quot; &quot;Renew&quot; &quot;PrimEner&quot; &quot;CO2&quot; ## [31] &quot;MatCon&quot; &quot;Forest&quot; &quot;Poverty&quot; &quot;Palma&quot; &quot;TertGrad&quot; &quot;FreePress&quot; ## [37] &quot;TolMin&quot; &quot;NGOs&quot; &quot;CPI&quot; &quot;FemLab&quot; &quot;WomParl&quot; &quot;PubDebt&quot; ## [43] &quot;PrivDebt&quot; &quot;GDPGrow&quot; &quot;RDExp&quot; &quot;NEET&quot; we see that LPI and Ship are once again present. In fact, COINr has an even quicker way to add and remove indicators, which is a short cut function called indChange(). In one command you can add or remove indicators and regenerate the results. Unlike the method above, indChange() also adds and removes relative to the existing index, which may be more convenient in some circumstances. To demonstrate this, we can use the same example as above: # Make a copy ASEM_NoBord2 &lt;- indChange(ASEM_NoLPIShip, drop = &quot;Bord&quot;, regen = TRUE) ## COIN has been regenerated using new specs. ASEM_NoBord2$Parameters$IndCodes ## [1] &quot;Flights&quot; &quot;Elec&quot; &quot;Gas&quot; &quot;ConSpeed&quot; &quot;Cov4G&quot; &quot;Goods&quot; ## [7] &quot;Services&quot; &quot;FDI&quot; &quot;PRemit&quot; &quot;ForPort&quot; &quot;Embs&quot; &quot;IGOs&quot; ## [13] &quot;UNVote&quot; &quot;CostImpEx&quot; &quot;Tariff&quot; &quot;TBTs&quot; &quot;TIRcon&quot; &quot;RTAs&quot; ## [19] &quot;Visa&quot; &quot;StMob&quot; &quot;Research&quot; &quot;Pat&quot; &quot;CultServ&quot; &quot;CultGood&quot; ## [25] &quot;Tourist&quot; &quot;MigStock&quot; &quot;Lang&quot; &quot;Renew&quot; &quot;PrimEner&quot; &quot;CO2&quot; ## [31] &quot;MatCon&quot; &quot;Forest&quot; &quot;Poverty&quot; &quot;Palma&quot; &quot;TertGrad&quot; &quot;FreePress&quot; ## [37] &quot;TolMin&quot; &quot;NGOs&quot; &quot;CPI&quot; &quot;FemLab&quot; &quot;WomParl&quot; &quot;PubDebt&quot; ## [43] &quot;PrivDebt&quot; &quot;GDPGrow&quot; &quot;RDExp&quot; &quot;NEET&quot; And here we see that now, Bord has been excluded as well as LPI and Ship. 13.2.2 Other adjustments We can make any methodological adjustments we want by editing any parameters in .$Method and then running regen(). For example, we can change the imputation method: # Make a copy ASEMAltImpute &lt;- ASEM # Edit .$Method ASEMAltImpute$Method$Imputation$imtype &lt;- &quot;indgroup_median&quot; # Regenerate ASEMAltImpute &lt;- regen(ASEMAltImpute, quietly = TRUE) We could also change the normalisation method, e.g. to use Borda scores: # Make a copy ASEMAltNorm &lt;- ASEM # Edit .$Method ASEMAltNorm$Method$Normalisation$ntype &lt;- &quot;borda&quot; # Regenerate ASEMAltNorm &lt;- regen(ASEMAltNorm, quietly = TRUE) and of course this extends to any parameters of any of the construction functions. We can even alter the underlying data directly if we want, e.g. by altering values in .$Input$Original$Data. In short, anything inside the COIN can be edited and then the results regenerated. This allows a fast way to make different alternative indexes and explore the effects of different methodology very quickly. 13.3 Comparisons A logical follow up to making alternative indexes is to try to understand the differences between these indexes. COINr "],["sensitivity-analysis.html", "Chapter 14 Sensitivity analysis 14.1 About sensitivity analysis 14.2 Sensitivity analysis in COINr", " Chapter 14 Sensitivity analysis Composite indicators, like any model, have many associated uncertainties. Sensitivity analysis can help to quantify the uncertainty in the scores and rankings of the composite indicator, and to identify which assumptions are driving this uncertainty, and which are less important. 14.1 About sensitivity analysis ss 14.2 Sensitivity analysis in COINr ss "],["helper-functions.html", "Chapter 15 Helper functions 15.1 R Interfaces 15.2 Selecting data sets and indicators 15.3 Rounding data frames", " Chapter 15 Helper functions 15.1 R Interfaces 15.1.1 Data import COINr has a couple of useful functions to help import and export data and metadata. You might have heard of the COIN Tool which is an Excel-based tool for building and analysing composite indicators, similar in fact to COINr2. With the coinToolIn() function you can import data directly from the COIN Tool to cross check or extend your analysis in COINr. To demonstrate, we can take the example version of the COIN Tool, which you can download here. Then its as simple as running: library(COINr) # This is the file path and name where the COIN Tool is downloaded to # You could also just put it in your project directory. fname &lt;- &quot;C:/Users/becke/Downloads/COIN_Tool_v1_LITE_exampledata.xlsm&quot; dflist &lt;- COINToolIn(fname) ## Imported 15 indicators and 28 units. The output of this function is a list with the three data frame inputs to assemble(). dflist ## $IndData ## # A tibble: 28 x 17 ## UnitName UnitCode ind.01 ind.02 ind.03 ind.04 ind.05 ind.06 ind.07 ind.08 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Austria AT 13.3 80.4 492. 14.9 68.8 34 3.7 87.6 ## 2 Belgium BE 15.1 71.8 503. 7 59.6 24 5.2 81.2 ## 3 Bulgaria BG 12.3 78.1 440. 2.2 51.3 15 10.6 72 ## 4 Cyprus CY 14 76 438. 6.9 16.7 23 3.6 73.4 ## 5 Czech R~ CZ 13.5 87.6 491. 8.8 73.2 27 3.9 86.7 ## 6 Germany DE 9.7 80.2 508. 8.5 46.3 30 5.6 90.1 ## 7 Denmark DK 9.7 73 504. 27.7 40.6 39 3.8 83.9 ## 8 Estonia EE 8.6 83.3 524. 15.7 35.7 37 4.1 77.1 ## 9 Greece EL 11.8 70 458. 4 31.5 30 3.9 49.2 ## 10 Spain ES 14.9 57.4 491. 9.4 34.8 33 11.4 68 ## # ... with 18 more rows, and 7 more variables: ind.09 &lt;dbl&gt;, ind.10 &lt;dbl&gt;, ## # ind.11 &lt;dbl&gt;, ind.12 &lt;dbl&gt;, ind.13 &lt;dbl&gt;, ind.14 &lt;dbl&gt;, ind.15 &lt;dbl&gt; ## ## $IndMeta ## # A tibble: 15 x 9 ## IndCode IndName GPupper GPlower Direction IndWeight Agg1 Agg2 Agg3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ind.01 Pre-primary pu~ NA NA -1 0.4 sp.01 p.01 Index ## 2 ind.02 Share of popul~ NA NA 1 0.3 sp.01 p.01 Index ## 3 ind.03 Reading, maths~ NA NA 1 0.3 sp.01 p.01 Index ## 4 ind.04 Recent training NA NA 1 0.3 sp.02 p.01 Index ## 5 ind.05 VET students NA NA 1 0.35 sp.02 p.01 Index ## 6 ind.06 High computer ~ NA NA 1 0.35 sp.02 p.01 Index ## 7 ind.07 Early leavers ~ NA NA -1 0.7 sp.03 p.02 Index ## 8 ind.08 Recent graduat~ NA NA 1 0.3 sp.03 p.02 Index ## 9 ind.09 Activity rate ~ NA NA 1 0.5 sp.04 p.02 Index ## 10 ind.10 Activity rate ~ NA NA 1 0.5 sp.04 p.02 Index ## 11 ind.11 Long-term unem~ NA NA -1 0.4 sp.05 p.03 Index ## 12 ind.12 Underemployed ~ NA NA -1 0.6 sp.05 p.03 Index ## 13 ind.13 Higher educati~ NA NA -1 0.4 sp.06 p.03 Index ## 14 ind.14 ISCED 5-8 prop~ NA NA -1 0.1 sp.06 p.03 Index ## 15 ind.15 Qualification ~ NA NA -1 0.5 sp.06 p.03 Index ## ## $AggMeta ## # A tibble: 10 x 4 ## AgLevel Code Name Weight ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 Index European Skills Index 1 ## 2 3 p.01 Skills Development 0.3 ## 3 3 p.02 Skills Activation 0.3 ## 4 3 p.03 Skills Matching 0.4 ## 5 2 sp.01 Compulsory education 0.5 ## 6 2 sp.02 Training and tertiary education 0.5 ## 7 2 sp.03 Transition to work 0.5 ## 8 2 sp.04 Activity rates 0.5 ## 9 2 sp.05 Unemployment 0.4 ## 10 2 sp.06 Skills mismatch 0.6 Because the COIN Tool uses numeric codes for indicators such as ind.01, you might want slightly more informative codes. The best way to do this is to name the codes yourself, but a quick solution is to set makecodes = TRUE in COINToolIn(). This generates short codes based on the indicator names. It will not yield perfect results, but for a quick analysis it might be sufficient. At least, you could use this and then modify the results by hand. dflist &lt;- COINToolIn(fname, makecodes = TRUE) ## Imported 15 indicators and 28 units. dflist$IndMeta$IndCode ## [1] &quot;Pre-Pupi&quot; &quot;SharPopu&quot; &quot;ReadMath&quot; &quot;ReceTrai&quot; &quot;Stud&quot; ## [6] &quot;HighComp&quot; &quot;EarlLeav&quot; &quot;ReceGrad&quot; &quot;ActiRate&quot; &quot;ActiRate_1&quot; ## [11] &quot;LongUnem&quot; &quot;UndePart&quot; &quot;HighEduc&quot; &quot;Isce5-Pr&quot; &quot;QualMism&quot; While the codes could certainly be improved, its a lot better than uninformative numbers. Finally, we can assemble the output into a COIN and begin the construction. ESI &lt;- assemble(dflist[[1]],dflist[[2]],dflist[[3]],) ## ----------------- ## No denominators detected. ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 15 ## Number of units = 28 ## No Year column detected in input data. Assuming you only have one year of data. ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 6 aggregate groups: sp.01, sp.02, sp.03, sp.04, sp.05, sp.06 ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 3 aggregate groups: p.01, p.02, p.03 ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- 15.1.2 Export to Excel Trigger warning for R purists! Sometimes its easier to look at your data in Excel. There, I said it. R is great for doing all kinds of complicated tasks, but if you just want to look at big tables of numbers and play around with them, maybe make a few quick graphs, then Excel is a great tool. Actually Excel is kind of underrated by many people who are used to programming in R or Python, Matlab or even Stata. It has a lot of clever tools that not many people know about. But more importantly, Excel is a lingua franca between all kinds of professions - you can pass an Excel spreadsheet to almost anyone and they will be able to take a look at it and use the data. Try doing that with an R or Python script. It just boils down to using the right tool for the right job. Anyway, with that aside, lets look at COINrs coin_2excel() function. You put in your COIN, and it will write a spreadsheet. # Build ASEM index ASEM &lt;- build_ASEM() # Get some statistics ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;) # write to Excel coin_2excel(ASEM, fname = &quot;ASEMresults.xlsx&quot;) The spreadsheet will contain a number of tabs, including: The indicator data, metadata and aggregation metadata that was input to COINr All data sets in the .$Data folder, e.g. raw, treated, normalised, aggregated, etc. (almost) All data frames found in the .$Analysis folder, i.e. statistics tables, outlier flags, correlation tables. 15.2 Selecting data sets and indicators The getIn() function is widely used by many COINr functions. It is used for selecting specific data sets, and returning subsets of indicators. While some of this can be achieved fairly easily with base R, or dplyr::select(), subsetting in a hierarchical context can be more awkward. Thats where getIn() steps in to help. Although it was made to be used internally, it might also help in other contexts. Note that this can work on COINs or data frames, but is most useful with COINs. Lets take some examples. First, we can get a whole data set. getIn() will retrieve any of the data sets in the .$Data folder, as well as the denominators. # Build data set first, if not already done ASEM &lt;- build_ASEM() ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Raw&quot;) datalist$ind_data_only ## # A tibble: 51 x 49 ## LPI Flights Ship Bord Elec Gas ConSpeed Cov4G Goods Services ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.10 29.0 0 35 35.4 0.273 14.1 98 278. 108. ## 2 4.11 31.9 20.6 48 26.5 36.1 16.3 99.9 598. 216. ## 3 2.81 9.24 7.92 18 11.3 0.312 15.5 56.7 42.8 13.0 ## 4 3.16 9.25 12.4 41 19.5 0.422 8.6 98 28.4 17.4 ## 5 3.00 8.75 11.7 0 0.439 0.029 6.9 60 8.77 15.2 ## 6 3.67 15.3 0 35 46.8 2.7 16.9 98.7 274. 43.5 ## 7 3.82 32.8 14.4 3 20.1 0.418 20.1 100. 147. 114. ## 8 3.36 3.13 9.17 6 9.50 0.0667 11.6 100 28.2 10.2 ## 9 3.92 18.9 10.0 18 24.4 0.38 20.5 99.9 102. 53.8 ## 10 3.90 97.6 19.5 75 78.5 29.9 10.8 80 849. 471. ## # ... with 41 more rows, and 39 more variables: FDI &lt;dbl&gt;, PRemit &lt;dbl&gt;, ## # ForPort &lt;dbl&gt;, Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, UNVote &lt;dbl&gt;, CostImpEx &lt;dbl&gt;, ## # Tariff &lt;dbl&gt;, TBTs &lt;dbl&gt;, TIRcon &lt;dbl&gt;, RTAs &lt;dbl&gt;, Visa &lt;dbl&gt;, ## # StMob &lt;dbl&gt;, Research &lt;dbl&gt;, Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, CultGood &lt;dbl&gt;, ## # Tourist &lt;dbl&gt;, MigStock &lt;dbl&gt;, Lang &lt;dbl&gt;, Renew &lt;dbl&gt;, PrimEner &lt;dbl&gt;, ## # CO2 &lt;dbl&gt;, MatCon &lt;dbl&gt;, Forest &lt;dbl&gt;, Poverty &lt;dbl&gt;, Palma &lt;dbl&gt;, ## # TertGrad &lt;dbl&gt;, FreePress &lt;dbl&gt;, TolMin &lt;dbl&gt;, NGOs &lt;dbl&gt;, CPI &lt;dbl&gt;, ## # FemLab &lt;dbl&gt;, WomParl &lt;dbl&gt;, PubDebt &lt;dbl&gt;, PrivDebt &lt;dbl&gt;, GDPGrow &lt;dbl&gt;, ## # RDExp &lt;dbl&gt;, NEET &lt;dbl&gt; The output, here datalist is a list containing the full data set .$ind_data, the data set .$ind_data_only only including numerical (indicator) columns, as well as unit codes, indicator codes and names, and the object type. More usefully, we can get specific indicators: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Raw&quot;, icodes = c(&quot;Flights&quot;, &quot;LPI&quot;)) datalist$ind_data_only ## # A tibble: 51 x 2 ## Flights LPI ## &lt;dbl&gt; &lt;dbl&gt; ## 1 29.0 4.10 ## 2 31.9 4.11 ## 3 9.24 2.81 ## 4 9.25 3.16 ## 5 8.75 3.00 ## 6 15.3 3.67 ## 7 32.8 3.82 ## 8 3.13 3.36 ## 9 18.9 3.92 ## 10 97.6 3.90 ## # ... with 41 more rows More usefully still, we can get groups of indicators based on their groupings. For example, we can ask for indicators that belong to the Political group: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Raw&quot;, icodes = &quot;Political&quot;, aglev = 1) datalist$ind_data_only ## # A tibble: 51 x 3 ## Embs IGOs UNVote ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 88 227 42.6 ## 2 84 248 43.0 ## 3 67 209 43.0 ## 4 62 197 42.7 ## 5 43 172 42.3 ## 6 84 201 42.2 ## 7 77 259 42.8 ## 8 46 194 42.9 ## 9 74 269 42.7 ## 10 100 329 40.4 ## # ... with 41 more rows To do this, you have to specify the aglev argument, which specifies which level to retrieve the indicators from. Before the data set is aggregated, you can anyway only select the indicators, but for the aggregated data set, the situation is more complex. To illustrate, we can call the Connectivity sub-index, first asking for all indicators: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Aggregated&quot;, icodes = &quot;Conn&quot;, aglev = 1) datalist$ind_data_only ## # A tibble: 51 x 30 ## LPI Flights Ship Bord Elec Gas ConSpeed Cov4G Goods Services FDI ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 94.1 27.9 0 42.0 29.0 1.02 37.2 98 40.7 33.0 6.24 ## 2 94.6 23.4 97.1 100 14.0 100 46.8 99.9 80.1 58.7 5.79 ## 3 34.3 10.9 37.4 16.3 25.0 3.48 43.3 56.7 47.1 28.4 15.8 ## 4 50.7 18.5 58.8 72.9 61.6 6.84 13.4 98 29.7 41.6 2.26 ## 5 43.2 62.4 55.2 0 6.80 2.20 6.06 60 21.6 100 43.1 ## 6 74.4 12.0 0 44.7 40.5 12.1 49.4 98.7 88.9 25.5 11.6 ## 7 81.0 48.0 68.0 7.01 33.4 3.49 63.2 100. 24.4 46.0 19.0 ## 8 60.0 20.0 43.3 13.4 74.0 2.57 26.4 100 75.4 55.4 15.4 ## 9 85.9 28.7 47.4 5.36 22.2 1.66 64.9 99.9 20.8 25.9 15.7 ## 10 84.9 12.6 92.3 11.7 11.7 23.3 22.9 80 15.1 21.1 6.04 ## # ... with 41 more rows, and 19 more variables: PRemit &lt;dbl&gt;, ForPort &lt;dbl&gt;, ## # Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, UNVote &lt;dbl&gt;, CostImpEx &lt;dbl&gt;, Tariff &lt;dbl&gt;, ## # TBTs &lt;dbl&gt;, TIRcon &lt;dbl&gt;, RTAs &lt;dbl&gt;, Visa &lt;dbl&gt;, StMob &lt;dbl&gt;, ## # Research &lt;dbl&gt;, Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, ## # MigStock &lt;dbl&gt;, Lang &lt;dbl&gt; Or we can call all the pillars belonging to Connectivity, i.e. the level below: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Aggregated&quot;, icodes = &quot;Conn&quot;, aglev = 2) datalist$ind_data_only ## # A tibble: 51 x 5 ## Physical ConEcFin Political Instit P2P ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 41.1 26.2 78.2 77.5 54.1 ## 2 72.0 48.2 80.8 75.6 43.3 ## 3 28.4 24.6 67.5 75.9 27.1 ## 4 47.6 24.6 62.6 76.8 39.1 ## 5 29.5 46.4 48.7 74.6 59.8 ## 6 41.5 33.8 71.0 74.4 39.6 ## 7 50.5 30.4 78.1 75.4 51.0 ## 8 42.5 38.9 55.4 77.3 53.5 ## 9 44.5 24.1 77.9 75.1 39.1 ## 10 42.4 20.6 87.6 75.4 28.4 ## # ... with 41 more rows Finally, if we want Conn itself, we can just call it directly with no aglev specified. # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Aggregated&quot;, icodes = &quot;Conn&quot;) datalist$ind_data_only ## # A tibble: 51 x 1 ## Conn ## &lt;dbl&gt; ## 1 55.4 ## 2 64.0 ## 3 44.7 ## 4 50.1 ## 5 51.8 ## 6 52.1 ## 7 57.1 ## 8 53.5 ## 9 52.1 ## 10 50.9 ## # ... with 41 more rows We can also use getIn() with data frames, and it will behave in more or less the same way, except a data frame has no information about the structure of the index. Here, getIn() returns what it can, and arguments like dset and aglev are ignored. # use the ASEM indicator data frame directly datalist &lt;- getIn(ASEMIndData, icodes = c(&quot;LPI&quot;, &quot;Goods&quot;)) datalist$ind_data_only ## # A tibble: 51 x 2 ## LPI Goods ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.10 278. ## 2 4.11 598. ## 3 2.81 42.8 ## 4 3.16 28.4 ## 5 3.00 8.77 ## 6 3.67 274. ## 7 3.82 147. ## 8 3.36 28.2 ## 9 3.92 102. ## 10 3.90 849. ## # ... with 41 more rows 15.3 Rounding data frames The roundDF function is a small helper function for rounding data frames that contain a mix of numeric and non-numeric columns. This is very handy for presenting tables generated by COINr in documents. # use the ASEM indicator data frame directly library(magrittr) ASEMIndData %&gt;% roundDF(decimals = 3) %&gt;% reactable::reactable(defaultPageSize = 5, highlight = TRUE, wrap = F) By default, numbers are rounded to two decimal places. Full disclosure, I was also involved in the development of the COIN Tool "],["appendix-r-resources.html", "Chapter 16 Appendix: R Resources 16.1 Introduction to R 16.2 Advanced resources", " Chapter 16 Appendix: R Resources One of the great things about R is the sheer number of freely-available resources that are out there. Not just the software and packages, but also online books and materials to learn everything you need to know about pretty much anything. Here are a list of resources for users who (a) are interested in R and want to get started, and (b) are proficient R users but want to learn more. I am focusing here on resources that have really helped me in the work that I have done. Of course there is far more out there and you only have to look. 16.1 Introduction to R If youre just starting out, these are good places to start. R for Data Science is a modern classic that starts from the beginning and leads you into the world of R, from a data science perspective. It uses the tidyverse approach which is developed by R Guru Hadley Wickham. Even advanced users can probably learn something here. https://r4ds.had.co.nz/index.html Swirl is an R package which lets you learn R at the command line. Also very good for beginners. https://swirlstats.com/ 16.2 Advanced resources 16.2.1 Programming If you really want to sharpen your programming skills in R, Hadley Wickham has another book: this one digs around in the roots of R and teaches you all kinds of tricks and quirks. https://adv-r.hadley.nz/ Want to build your own R package? Hadley come to the rescue, again. https://r-pkgs.org/ 16.2.2 Visualisation Plotly is a big R package which generates interactive graphics using Javascript (there are many others, by the way). This book tells you all you need to know about that. https://plotly-r.com/ Shiny is another R package which lets you build interactive web apps based on R code. Its tricky to get your head around at first, but this book really helps. https://mastering-shiny.org/index.html 16.2.3 Other If youre not using GitHub, ask yourself, why not? GitHub is the best way to collaborate and share code, and you can also host documentation and websites. This book is hosted on GitHub, to take a random example. The Happy Git with R Book gives an easy introduction to hooking up R Studio to work seamlessly with Git and Github. https://happygitwithr.com/ You wouldnt think that R would be a good tool for writing books, but actually it turns out that its a pretty good tool for writing books. The Bookdown package lets you build nifty online books with a simple and neat layout. You can include equations and importantly, R code, outputs and HTML widgets, etc. https://bookdown.org/yihui/bookdown/ Finally, you wouldnt think that R would be a good tool for building website, but actually it turns out its a pretty good tool for building websites. OK, yes, if you want to build something really complicated and/or highly customised, then its not the way forward. But for building fairly simple sites and blogs, personal pages etc (especially if you want to stick in some R code), then the Blogdown package gives a great way to do this. And guess what, you can link it to Github and it automatically updates your website when you push any changes from R Studio. And its all free. An example of this is my very humble website which you can find at http://www.bluefoxdata.eu. Whats that you say, if only there were a book to teach me how to do all this? Well youre in luck - here it is. https://bookdown.org/yihui/blogdown/ "],["acknowledgements.html", "Chapter 17 Acknowledgements", " Chapter 17 Acknowledgements The construction of this package was funded by the European Commissions Joint Research Centre, in particular by the Competence Centre for Composite Indicators and Scoreboards. It was also developed with and inspired by colleagues from the same group. COINr, like all R packages, has been built off of the back of many other packages. Some that COINr uses in particular are: Plotly, for beautiful javascript plots ggplot2, for beautiful static plots reactable, for nice interactive tables shiny, for apps Various elements of the tidyverse, particularly dplyr "]]
