[["index.html", "Composite Indicator Development and Analysis in R with COINr Chapter 1 Overview 1.1 Background 1.2 Installation 1.3 Whats it for? 1.4 Demo 1.5 Features and status", " Composite Indicator Development and Analysis in R with COINr William Becker 2021-06-10 Chapter 1 Overview 1.1 Background This documentation describes in detail the COINr package, which is an open source R package for developing and analysing composite indicators, developed by the European Commissions Joint Research Centre. In fact, this is slightly more than technical documentation, and also gives some tips on composite indicator development along the way. A composite indicator is an aggregation of indicators which aims to measure a particular concept. Composite indicators are typically used to measure complex and multidimensional concepts which are difficult to define, and cannot be measured directly. Examples include innovation, human development, environmental performance, and so on. Composite indicators are closely related to scoreboards, which are also groups of indicators aiming to capture a concept. However, scoreboards do not aggregate indicator values. Composite indicators also usually use a hierarchical structure which breaks the concept down into elements, sometimes known as sub-pillars, pillars, sub-indexes, dimensions, and so on. COINr is currently still under development, therefore the package itself, and this documentation, are a work in progress but are being continually updated. You can find the latest version of COINr at its GitHub repo, and also the source code for this documentation here. 1.2 Installation Although COINr is under development, a pre-beta version can be installed via Github. First, install the devtools package if you dont already have it, then run: devtools::install_github(&quot;bluefoxr/COINr&quot;) This should directly install the package from Github, without any other steps. You may be asked to update packages. This might not be strictly necessary, so you can also skip this step. At the time of writing (April 2021), the functionalities of the package are perhaps 75% complete. Roughly speaking, the features that are described in detail in this documentation are more or less complete and have survived an initial round of testing and checks. Any functions that are not described in detail here should be treated with more caution and it is likely that bugs would be encountered here and there. The entire package will need to beta tested before a more official release. In case you do spot a bug, or have a suggestion, the best way is to open an issue on the repo. Otherwise, you can also just email me. 1.3 Whats it for? COINr is the first fully-flexible development and analysis environment for composite indicators and scoreboards. The main features are: Flexible and fast development of composite indicators with no limits on aggregation levels, numbers of indicators, highly flexible set of methodological choices. In-depth statistical analysis of indicators and results, including multivariate analysis, statistical reweighting, uncertainty and sensitivity analysis, etc. Interactive data exploration and visualisation via Shiny apps which can be hosted online. HTML widgets for incorporation in interactive markdown documents. In short, COINr aims to allow composite indicators to be developed and prototyped very quickly and in a structured fashion, with the results immediately available and able to be explored interactively. Although it is built in R, it is a high-level package that aims to make command simple and intuitive, with the hard work performed behind the scenes, therefore it is also accessible to less experienced R users. 1.4 Demo COINr is highly customisable, but equally allows composite indicator construction in a few commands. Taking the built-in ASEM dataset, we can assemble a composite indicator in a few steps. library(COINr) ## ## Attaching package: &#39;COINr&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## aggregate # assemble basic composite indicator object from input data ASEM &lt;- assemble(IndData = ASEMIndData, IndMeta = ASEMIndMeta, AggMeta = ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- Here, we have loaded the COINr package, and assembled a so-called COIN, which is a structured object used universally within COINr. It contains a complete record of all data sets, parameters, methodological choices, analysis and results generated during the construction and analysis process. A COIN is initially assembled by supplying a table of indicator data, a table of indicator metadata, and a table of aggregation metadata. More details can be found in the chapter on COINs: the currency of COINr. Currently, the COIN (composite indicator) only contains the input data set and other metadata. To actually get the aggregated results (i.e. and index), we have to denominate, normalise and aggregate it. These operations are performed by dedicated functions in COINr. # denominate data using specifications in ASEMIndMeta ASEM &lt;- denominate(ASEM) # normalise data ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;) # Aggregate normalised data ASEM &lt;- aggregate(ASEM, dset = &quot;Normalised&quot;) Of course, there are many ways to aggregate data, and other steps could be performed before aggregating, such as (multivariate) analysis, data treatment and so forth. Here, we have simply taken three basic steps. Each of the functions generates a new data set  here, respectively Denominated, Normalised and Aggregated data sets which are added to and stored within the COIN. Although in the example here the default specifications have been used, the functions have many options, and can be directed to operate on any of the data sets within the COIN. Details on this are left to later chapters in this book. At this point, let us examine some of the outputs in the COIN. First, we may want to see the index structure: plotframework(ASEM) In the online version of this documentation, this plot appears as an interactive HTML widget which can be embedded into HTML documents. COINr leverages existing R packages, and interactive graphics (like that of Figure 6) are generated using bindings to the plotly package, which is based on Javascript libraries. Static graphics are largely produced by the ggplot2 package. The framework plot is called a sunburst plot and summarises the structure of the index and the relative weight of each indicator in each aggregation level and the index. We can also get indicator statistics in table format: library(reactable) library(magrittr) ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;) ASEM$Analysis$Raw$StatTable %&gt;% reactable::reactable(defaultPageSize = 5, highlight = TRUE, wrap = F) Again, this table is an presented as an interactive HTML table and is easier explored using the online version of this book. In short, getstats() provides key statistics on each indicator, relating to data availability, outliers, summary statistics such as mean, median, skew, kurtosis, and so on. This can be applied to any data set within the COIN. Indicator distributions can also be easily plotted: plotIndDist(ASEM, type = &quot;Violindot&quot;, icodes = &quot;Political&quot;) Finally, results can be visualised and explored in an interactive dashboard. This can be useful as a rapid prototype of the index. The app may even be hosted online, or can be used as the basis for a more customised indicator visualisation platform. Here, a basic screenshot is provided. Figure 1.1: Results dashboard screenshot Clearly, this is not meant to replace a dedicated composite indicator visualisation site, but is a fast first step. 1.5 Features and status COINr has, or will have, many more features. Among those that are already implemented and in various stages of testing: Unlimited indicators, units, aggregation levels Denomination of indicators Indicator visualisation via static plots and dashboards Indicator statistics and missing data analysis Imputation of missing data via various methods Outlier analysis and treatment Normalisation via various methods Aggregation of indicators via various methods Multivariate analysis (PCA) and correlation analysis Interactive re-weighting Export to Excel Import from the COIN Tool And those that will be included soon, among others: Automatic re-weighting Grouping units by e.g. income groups, size groups Support for panel data, i.e. indexes for multiple years What-if experiments, comparing alternative versions of the index Sensitivity analysis, robustness to methodological assumptions, weights, etc. "],["foundations.html", "Chapter 2 Foundations 2.1 How to use COINr 2.2 Suggested workflows 2.3 A map of COINr 2.4 Terminology 2.5 Levels 2.6 Tips for using COINr", " Chapter 2 Foundations This chapter aims to explain some important principles about how COINr works, as well as giving an overview of the package and suggested workflows. If you want to immediately get to COINr functions without any further ado, skip this chapter for now. However if you want to understand COINr better, this might be worth reading. 2.1 How to use COINr There are two main ways to use COINr. The first is to use the functions as standalone tools for your own specific use. For example, coin_normalise.R will take a data frame as an input, and normalise the columns according to the options specified in its input, outputting another data frame. The second way is to work inside what Ill call the COINrverse (with tongue firmly in cheek). What this means is that all operations are performed on a single object, the COIN object, which stores all data, all analysis, parameters, and methodological choices made along the way. The COINrverse approach enables the full feature set of COINr, because some functions only work on COIN objects - for example, exporting all results to Excel in one command, or running an uncertainty/sensitivity analysis. Indeed, COINr is built mainly for COINrverse use, but functions have been written with the flexibility to also accommodate independent use where possible. More details on how the COINrverse works can be found in XXX. 2.2 Suggested workflows E.g. quick construction Analysis, audit Comparisons 2.3 A map of COINr COINr consists of many functions that do a number of different types of operations. Here is a summary. 2.3.1 Construction The following functions are the main functions for building a composite indicator in COINr. Function Description assemble() Assembles indicator data/metadata into a COIN checkData() Data availability check and unit screening denominate() Denominate (divide) indicators by other indicators impute() Impute missing data using various methods treat() Treat outliers with Winsorisation and transformations normalise() Normalise data using various methods aggregate() Aggregate indicators into hierarchical levels, up to index regen() Regenerates COIN results using specifications stored in .$Method 2.3.2 Visualisation These functions are for visualising data. Function Description iplotBar() Interactive bar chart for any indicator iplotIndDist() Interactive indicator distribution plots for a single indicator iplotIndDist2() Interactive indicator distribution plots for two indicators simultaneously iplotMap() Interactive choropleth map for any indicator (only works for countries) plotCorr() Interactive correlation heatmap between any levels of the index plotframework() Interactive sunburst plot visualising the indicator framework plotIndDist() Static plot of distributions for indicators and groups of indicators 2.3.3 Analysis The following functions analyse indicator data. Function Description cPCA() Principle component analysis on a specified data set and subset of indicators. Returns PCA weights. effectiveWeight() Calculates the effective weights of each element in the indicator hierarchy. getStats() Get table of indicator statistics for any data set weightOpt() Weight optimisation according to a pre-specified vector of importances 2.3.4 Interactive apps These are interactive apps, built using Shiny, which allow fast interactive exploration and adjustments. Function Description indDash() Indicator visualisation (distribution) dashboard for one or two indicators rew8r() Interactively re-weight indicators and check updated results and correlations 2.3.5 Import/export Functions to import and export data and results to and from COINr and R. Function Description COINToolIn() Import indicator data and metadata from COIN Tool coin_2Excel() Write data, analysis and results from a COIN to Excel 2.3.6 Other functions These are other functions that may be of interest, but do not neatly fit in the previous categories. Many of them are called from the other functions listed above, but may still be useful on their own. Function Description BoxCox() Box Cox transformation on a vector of data build_ASEM() Build ASEM (example) composite indicator in one command coin_win() Winsorise one column of data according to skew/kurtosis thresholds copeland() Aggregates a data frame into a single column using the Copeland method. geoMean() Weighted geometric mean of a vector getIn() Useful function for subsetting indicator data. See Helper functions. harMean() Weighted harmonic mean of a vector loggish() Log-type transformation, of various types, for a vector names2Codes() Given a character vector of long names (probably with spaces), generates short codes. outrankMatrix() Outranking matrix based on a data frame of indicator data and corresponding weights roundDF() Round down a data frame (i.e. for presentation) 2.3.7 Data COINr comes with some example data embedded into the package. Function Description ASEMIndData ASEM (example) indicator data as input for assemble() ASEMIndMeta ASEM (example) indicator metadata as input for assemble() ASEMAggMeta ASEM (example) aggregate metadata as input for assemble() WorldDenoms National denomination data (GDP, population, etc) worldwide 2.3.8 Finally There are also a number functions which are mainly for internal use, and are not listed here. 2.4 Terminology Lets clearly define a few terms first to avoid confusion later on. An indicator is a variable which has an observed value for each unit. Indicators might be things like life expectancy, CO2 emissions, number of tertiary graduates, and so on. A unit is one of the entities that you are comparing using indicators. Often, units are countries, but they could also be regions, universities, individuals or even competing policy options (the latter is the realm of multicriteria decision analysis). Together, indicators and units form the main input data frame for COINr (units as rows, and indicators as columns). 2.5 Levels COINr frequently refers to aggregation levels or equivalently just levels. Composite indicators are built in a hierarchical structure, such that indicators are aggregated to e.g. sub-pillars, sub-pillars are then aggregated to pillars, and so on up the final index. COINr can handle any number of these aggregation levels. Some functions also take the aggregation level as one of their arguments. The convention used by COINr is that the first level (Level 1) is the indicator level, i.e. the columns of data input in IndData. Level 2 is aggregated scores resulting from aggregating indicators, Level 3 is the aggregated scores resulting from aggregating the scores in Level 2, and so on. In the ASEM example, therefore, the levels are defined as follows: Level 1 = Indicators Level 2 = Pillars Level 3 = Sub-indexes Level 4 = Index This means that the ASEM Index has four levels. 2.6 Tips for using COINr 2.6.1 Missing data Its very important to understand the difference between missing data and zeros. A zero means that you know that the value of the indicator is zero, whereas missing data (in R this is denoted as NA) means you dont know what the value is at all. When aggregating indicators, this difference becomes particularly important. If a data point is missing, it is often excluded from the aggregation, which effectively means reassigning it with the mean (or similar) value of the indicators in the same group. If you impute the data, missing values could be assigned with mean or median indicator values, for example. Clearly, these values will be very different from zeros. This point is made here, because data may sometimes come with missing values denoted as zeros, or with zeros denoted as missing values. Ensure that these are checked carefully before proceeding with construction. 2.6.2 Ordering Currently, COINr is somewhat dependent on the order of the units. Although you can input your data in any order you like (as long as you follow the rules in [Data Input]), in some cases the order does matter. For example, if you want to denominate your data by a separate data frame of indicators (not in the COIN), you will have to make sure that the units (rows) correspond to the same order of rows as your indicator data. In future updates this dependence on order should be removed, because it is a possible source of errors. For now though, just keep ordering in mind. Its worth mentioning that the row order never changes in the .$Data folder. However, if you screen units using a minimum data requirement (e.g. using datacheck()), then the number of units can change. This should also be kept in mind. 2.6.3 Syntax Where possible, COINr functions use a common syntax to keep things simple. For example: dset always refers to the name of the data set that is found in .$Data. For example, dset = \"Normalised\" will return .$Data$Normalised. The only exception is dset = \"Denominators\", which returns .$Input$Denominators. The argument dset is used in many COINr functions because they have to know which data set to operate on. More to be added here. 2.6.4 Plotting COINr includes a number of plotting functions which return various types of plots. These are all either based on the ggplot2 or plotly packages. Think of COINr plots as starting points rather than finished plots. While I have tried to make the plots useful and even visually appealing, much more could be done to improve them and to fit particular uses and contexts. Luckily, because of the way that plotly and ggplot2 work, plots generated by COINr can easily be modified by assigning them to an object and then modifying the object. Generally this would look like this: # generate a plot, but assign to variable &quot;plt&quot; plt &lt;- COINr_plot_function(COIN, plot_options) # alter plt plt &lt;- plt + alter_plot_function(options) # view the plot plt The alter_plot_function could be any of the very many layout and geometry functions from ggplot2, for example. This means you can change colours, axis labels, point styles, titles and many other things. In summary, with COINr you may choose to either: Generate plots with COINr functions and be content with them as they are Generate plots with COINr functions and tweak them to your tastes using the approach shown above Make your own plots "],["coins-the-currency-of-coinr.html", "Chapter 3 COINs: the currency of COINr 3.1 Introduction 3.2 The three inputs 3.3 Putting everything together 3.4 Moving on", " Chapter 3 COINs: the currency of COINr 3.1 Introduction Where possible COINr functions can be used as standalone functions, typically on data frames of indicator data, for operations such as normalisation, imputation and so on. The full functionality of COINr is however harnessed by assembling the indicator data, and the structure of the index, into a COIN. A COIN is a structured list, which neatly stores the various data sets, parameters, analyses and methodological decisions in one place. There are various reasons for doing this: It simplifies operations because functions know where data and parameters are. Therefore, COINr operations typically follow a syntax of the form COINobj &lt;- COINr_function(COINobj, &lt;methodological settings&gt;), updating the COIN object with new results and data sets generated by the function. This avoids having to repeatedly specify indicator names, structure, etc, as separate arguments. Its like putting a coin in a vending machine and getting a packet of crisps. Except you also get your coin back1. It keeps things organised and avoids a workspace of dozens of variables. It keeps a record of methodological decisions - this allows results to be easily regenerated following what if experiments, such as removing or changing indicators etc (see Section on adjustments and reruns TO ADD). The logic of doing this will become clearer as you build your index. If you only want to use the COINr functions on data frames, you can probably skip this chapter. 3.2 The three inputs To build a COIN object, three ingredients (data frames) are needed to begin with. In short, they are the indicator data, the indicator metadata, and the aggregation metadata. Here, each will be explained separately. Inputting the data in the first place is where you will do most of the work, because you have to get your data in a format that COINr understands. But once you have got your data assembled, COINr should do most of the hard work, so hang in there! 3.2.1 Indicator data The indicator data is a data frame which, shockingly, specifies the data for the indicators. However, it can do more than that, and also some rules have to be followed. The easiest way to explain is to start with an example. library(COINr) head(ASEMIndData) ## # A tibble: 6 x 60 ## UnitName UnitCode Group_GDP Group_GDPpc Group_Pop Group_EurAsia Year Den_Area ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Austria AUT L XL M Europe 2018 83871 ## 2 Belgium BEL L L L Europe 2018 30528 ## 3 Bulgaria BGR S S M Europe 2018 110879 ## 4 Croatia HRV S M S Europe 2018 56594 ## 5 Cyprus CYP S L S Europe 2018 9251 ## 6 Czech R~ CZE M L M Europe 2018 78867 ## # ... with 52 more variables: Den_Energy &lt;dbl&gt;, Den_GDP &lt;dbl&gt;, Den_Pop &lt;dbl&gt;, ## # LPI &lt;dbl&gt;, Flights &lt;dbl&gt;, Ship &lt;dbl&gt;, Bord &lt;dbl&gt;, Elec &lt;dbl&gt;, Gas &lt;dbl&gt;, ## # ConSpeed &lt;dbl&gt;, Cov4G &lt;dbl&gt;, Goods &lt;dbl&gt;, Services &lt;dbl&gt;, FDI &lt;dbl&gt;, ## # PRemit &lt;dbl&gt;, ForPort &lt;dbl&gt;, Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, UNVote &lt;dbl&gt;, ## # CostImpEx &lt;dbl&gt;, Tariff &lt;dbl&gt;, TBTs &lt;dbl&gt;, TIRcon &lt;dbl&gt;, RTAs &lt;dbl&gt;, ## # Visa &lt;dbl&gt;, StMob &lt;dbl&gt;, Research &lt;dbl&gt;, Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, ## # CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, MigStock &lt;dbl&gt;, Lang &lt;dbl&gt;, Renew &lt;dbl&gt;, ## # PrimEner &lt;dbl&gt;, CO2 &lt;dbl&gt;, MatCon &lt;dbl&gt;, Forest &lt;dbl&gt;, Poverty &lt;dbl&gt;, ## # Palma &lt;dbl&gt;, TertGrad &lt;dbl&gt;, FreePress &lt;dbl&gt;, TolMin &lt;dbl&gt;, NGOs &lt;dbl&gt;, ## # CPI &lt;dbl&gt;, FemLab &lt;dbl&gt;, WomParl &lt;dbl&gt;, PubDebt &lt;dbl&gt;, PrivDebt &lt;dbl&gt;, ## # GDPGrow &lt;dbl&gt;, RDExp &lt;dbl&gt;, NEET &lt;dbl&gt; COINr comes prepackaged with some example data - here we are looking at indicator data from the ASEM Sustainable Connectivity Portal, which has an indicator data set covering 51 Asian and European countries. The first thing to notice is that each row is an observation (here, a country), and each column is a variable (mostly indicators, but also other things). Look at the structure of the data frame, working from left to right: UnitName [required] gives the name of each unit. Here, units are countries, so these are the names of each country. UnitCode [required] is a unique code assigned to each unit (country). This is very important, and is the main reference inside COINr for units. If your units are countries, I recommend using ISO Alpha-3 codes, because these are recognised by COINr for generating maps. It also makes data processing generally easier. Group_* [optional] Any column name that starts with Group_ is recognised as a group column rather than an indicator. You dont have to have any groups, but some COINr functions support specific operations on groups (e.g. imputation within group, and future updates plan to expand this capacity). You can have as many group columns as you want. Year [optional] gives the reference year of the data. This allows you to have multiple years of data, for example, you can have a value for a given country for 2018, and another value for the same country for 2019, and so on. Like groups, this feature is not yet fully developed in COINr. Den_*[optional] Any column names that begin with Den_* are recognised as denominators, i.e. indicators that are used to scale other indicators. Finally, any column that begins with x_ will be ignored and passed through. This is not shown in the data set above, but is useful for e.g. alternative codes or other variables that you want to retain. Any remaining columns that do not begin with x_ or use the other names in this list are recognised as indicators. You will notice that all column (variable/indicator) names use short codes. This is to keep things concise in tables, rough plots etc. Indicator codes should be short, but informative enough that you know which indicator it refers to (e.g. Ind1 is not so helpful). In some COINr plots, codes are displayed, so you might want to take that into account. In any case, the full names of indicators, and other details, are also specified in the indicator metadata table - see the next section. Some important rules and tips to keep in mind are: The following columns are required. All other columns are optional: UnitCode UnitName At least one indicator column Columns dont have to be in any particular order, columns are identified by names rather than positions. You can have as many indicators and units as you like. Indicator codes and unit codes must have unique names. You cant use the same code twice otherwise bad things will happen. Avoid any accented characters or basically any characters outside of English - this can sometimes cause trouble with encoding. Column names are case-sensitive. Most things in COINr are built to have a degree of flexibility where possible, but column names need to be written exactly as they appear here for COINr to recognise them., 3.2.2 Indicator metadata The second data frame you need to input specifies the metadata of each indicator. This serves two purposes: first, to give details about each indicator, such as its name, its units and so on; and second, to specify the structure of the index. Heres what this looks like, for our example ASEM data set: head(ASEMIndMeta) ## # A tibble: 6 x 10 ## IndName IndCode Direction IndWeight Denominator IndUnit Target Agg1 Agg2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Logist~ LPI 1 1 &lt;NA&gt; Score ~ 4.12 Phys~ Conn ## 2 Intern~ Flights 1 1 Den_Pop Thousa~ 200. Phys~ Conn ## 3 Liner ~ Ship 1 1 &lt;NA&gt; Score 20.1 Phys~ Conn ## 4 Border~ Bord 1 1 Den_Area Number~ 116. Phys~ Conn ## 5 Trade ~ Elec 1 1 Den_Energy TWh 105. Phys~ Conn ## 6 Trade ~ Gas 1 1 Den_Energy Billio~ 90.1 Phys~ Conn ## # ... with 1 more variable: Agg3 &lt;chr&gt; Notice that now, the table is flipped on its side (transposed), and each row is an indicator, and the columns specify other things. This is to keep the data in a tidy format. Lets go through the columns one by one. IndName [required] This is the full name of the indicator, which will be used in display plots. IndCode[required] A reference code for each indicator. These must be the same codes as specified in the indicator metadata. The codes must also be unique. Direction [required] The direction of each indicator - this takes values of either 1 or -1 for each indicator. A value of 1 means that higher values of the indicator correspond to higher values of the index, whereas -1 means the opposite. IndWeight [required] The initial weights assigned to each indicator. Weights are relative and do not need to sum to one, so you can simply put all 1s here if you dont know what else to put (the values can be adjusted later). Denominator [required??] These should be the indicator codes of one of the denominator variables for each indicator to be denominated. E.g. here Den_Pop specifies that the indicator should be denominated by the Den_Pop indicators (population, in this case). For any indicators that do not need denominating, just set NA. Denominators can also be specified later, so if you want you can leave this column out. See Denomination for more information. IndUnit [optional] The units of the indicator. This helps for keeping track of what the numbers actually mean, and can be used in plots. Target [optional] Targets associated with each indicator. Here, artificial targets have been generated which are 95% of the maximum score (accounting for the direction of the indicator). These are only used if the normalisation method is distance-to-target. Agg* [required] Any column name that begins with Agg is recognised as a column specifying the aggregation group, and therefore the structure of the index. Aggregation columns should be in the order of the aggregation, but otherwise can have arbitrary names. Lets look at the aggregation columns in a bit more detail. Each column represents a separate aggregation level, so in the ASEM example here we have three aggregation levels - the pillars, the two sub-indexes, and the overall index. The entry of each column specifies which group each indicator falls in. So, the first column Agg1 specifies the pillar of each indicator. Again, each aggregation group (pillar, sub-index or index) is referenced by a unique code. The next column Agg2, gives the sub-index that the indicator belongs to. There is a bit of redundancy here, because obviously indicators in the same pillar must also belong to the same sub-index. Finally, Agg3 specifies that all indicators belong to the index. You can have as many Agg columns as you like, and the names dont have to be Agg1 etc, but could be e.g. Agg_Pillar, Agg_SubIndex, etc. However, they must begin with Agg, otherwise COINr will not recognise them. And they must appear in the order of the aggregation, i.e. lowest level of aggregation first, then working upwards. 3.2.3 Aggregation metadata The final data input is the aggregation metadata, which is also the simplest. Heres our example for the ASEM data set: head(ASEMAggMeta) ## # A tibble: 6 x 4 ## AgLevel Code Name Weight ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Physical Physical 1 ## 2 2 ConEcFin Economic and Financial (Con) 1 ## 3 2 Political Political 1 ## 4 2 Instit Institutional 1 ## 5 2 P2P People to People 1 ## 6 2 Environ Environmental 1 This data frame simply consists of three columns for each aggregation level: *Code [required] The aggregation group codes. This column must end with Code, and the codes must match the codes in the corresponding column in the indicator metadata aggregation columns. *Name [required] The aggregation group names. This column must end with Name. *Weight [required] The aggregation group weights. This column must end with Weight. Again, these weights can be changed later on. Columns must appear in the order of the aggregation groups. 3.3 Putting everything together Having got all your data in the correct format, you can finally build it into a COIN object. From here, things start to get a bit easier. The function to build the COIN object is called assemble(). This function takes the three data frames mentioned and converts them into a so-called COIN Object, which is a hierarchical list that is structured in a way that is recognised by all COINr functions. Lets run assemble() using the built-in data sets to show how it works. ASEM &lt;- assemble(IndData = ASEMIndData, IndMeta = ASEMIndMeta, AggMeta = ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- The three inputs here are as follows: IndData which is the indicator data IndMeta which is the indicator metadata AggMeta which is the aggregation metadata And this outputs a COIN Object which you can name as you want (here we have called it ASEM). There are two further arguments to assemble() which are not used here, and they are: include - this is a character vector of indicator codes (i.e. codes that are found in IndData$IndCode) which specifies which indicators to include out of the IndData and IndMeta inputs. By default, all indicators are included, but this gives the option to only include certain indicators, if for example, you have several alternative data sources. exclude- this an analogous character vector of indicator codes, but specifying which indicators to exclude, if any. Again, by default, nothing is excluded. This may be easier to specify when creating a subset of indicators. The structure of the list is as follows. First, it is divided into five main sub-lists, which I am going to call folders: Input is the input data, and has the following entries: IndData The indicator data, after any exclusion of indicators IndMeta The indicator metadata, after any exclusion of indicators AggMeta The aggregation metadata, after any exclusion of indicators Original A list containing the original, unaltered inputs to assemble() Data is where the indicator data sets are stored. As you go through the construction, this will populated with more data sets, e.g. imputed data, treated data, aggregated data, and so on. On first assembly you will only see Raw present here. Parameters stores some useful parameters that are used by COINr functions, and may be of general interest, such as the indicator codes, unit codes, weights, and so on. Analysis is where analysis of the indicator data will be stored, including missing data analysis, indicator summary statistics, principal component analysis and so on. Method keeps a record of the methodological decisions made when constructing the composite indicator, for example the normalisation method and parameters, data treatment specifications, and so on. Apart from keeping a record of the construction, this also allows the entire results to be regenerated using the information stored in the COIN Object, this will be explained better in Chapter XXX. The logic of the COIN object is that COINr functions can take all the data and parameters that they need from it, and you only have to specify some particular parameters of the function. Then, outputs, such as new data sets, are returned back to the COIN object, which is added to and expanded as the analysis progresses. Apart from building the COIN Object, assemble() does a few other things: It checks that indicator codes are consistent between indicator data and indicator metadata It checks that required columns, such as indicator codes, are presenr It returns some basic information about the data that was input, such as the number of indicators, the number of units, the number of aggregation levels and the groups in each level. This is done so you can check what you have entered, and that it agrees with your expectations. 3.4 Moving on Now that you have a COIN Object, you can start using all the other functions in COINr to their full potential. Most functions will still work on standalone data frames, so you dont need to work with COIN objects if you prefer not to. Depending on the function, you either get a packet of crisps (e.g. a plot or analysis), or your coin plus interest (an updated COIN with a new data set or analysis) "],["initial-visualisation-and-analysis.html", "Chapter 4 Initial visualisation and analysis 4.1 Structure 4.2 Distributions 4.3 Ranks and Maps 4.4 Statistics and analysis", " Chapter 4 Initial visualisation and analysis One of the first things to do with indicator data is to look at it, in as many ways as possible. This helps to get a feel for how the data is distributed between units/countries, how it may be spatially distibuted, and how indicators relate to one another. COINr includes various tools for visualising and analysing indicator data and the index structure. The types of plots generated by COINr fall into two categories: static plots, and interactive plots. static plots generate images in standard formats such as png, pdf and so on. Interactive plots generate javascript graphics, which have interactive elements such as zooming and panning, and information when you hover the mouse over. These latter type of plots are particularly useful for including in HTML documents, because they are self contained. For example, they can be used in R Markdown documents, then knitted to HTML, or embedded on websites (such as this one - see below), e.g. via the blogdown or bookdown packages. Javascript plots can also be rendered to png and other formats, so can also be used in static documents. The plotting tools here can be useful at any stage of building a composite indicator or scoreboard, from initial visualisation of the data, to checking the effects of data treatment, to visualising final index results. 4.1 Structure Independently from the indicator data, a good way to begin is to check the structure of the index. This can be done visually with the plotframework() function, which generates a sunburst plot of the index structure. plotframework(ASEM) The sunburst plot is useful for a few things. First, it shows the structure that COINr has understood. If you get an error here, it is probably an indication that something has gone wrong in the input of the structure, so go back to the input data and check. If it does successfully display the sunburst plot, you can check whether the structure agrees with your expectations. Second, it shows the effective weight of each indicator (the value is visible by hovering over each segment). This can reveal which indicators are implicitly weighted more than others, by e.g. having more or less indicators in the same aggregation groups. Finally, it can be a good way to communicate your index structure to other people. 4.2 Distributions Individual indicator distributions can be visualised in several different ways. For static plots, the main tool is plotIndDist() which generates histograms, boxplots, violin plots, dot plots and violin-dot plots. This is powered by ggplot2, and if you want to customise plots, you should use that directly. However, COINr plotting functions are intended as quick tools to visualise data, with easy access to the hierarchical data set. You can plot individual indicators: plotIndDist(ASEM, type = &quot;Histogram&quot;, icodes = &quot;LPI&quot;) And you can also plot groups of indicators by calling aggregate names (notice that when multiple indicators are plotted, the indicator codes are used to label each plot, rather than indicator names, to save space): plotIndDist(ASEM, type = &quot;Violindot&quot;, icodes = &quot;Physical&quot;) The plotIndDist() function has several arguments. In the first place, any indicator or aggregation (pillar, dimension, index etc) can be plotted by using the dset argument. If you have only just assembled the COIN Object, you will only have the Raw dataset, but any other dataset can be accessed, e.g. treated data set, aggregated data set, and so on. You can also target different levels using the aglev argument - for more details see the chapter on Helper functions. Stand-alone data frames are also supported by plotIndDist() (this can also be achieved directly by ggplot without too much effort): df &lt;- as.data.frame(matrix(rnorm(90),nrow = 30, ncol = 3)) colnames(df) &lt;- c(&quot;Dogs&quot;, &quot;Cats&quot;, &quot;Rabbits&quot;) plotIndDist(df, type = &quot;Box&quot;) COINr also includes some interactive plots, which are embedded into apps (see later), but can be used for your own purposes, such as embedding in HTML documents or websites. iplotIndDist(ASEM, &quot;Raw&quot;, &quot;Renew&quot;, ptype = &quot;Violin&quot;) Since all the plotting functions output plot objects (plotly objects for iplotIndDist, and ggplot2 plot objects for plotIndDist), you can also modify them if you want to customise the plots. This might be a helpful workflow - to use COINrs default options and then tweak the plot to your liking. In a very simple example, here we just change the title. iplotIndDist(ASEM, &quot;Raw&quot;, &quot;Flights&quot;, ptype = &quot;Histogram&quot;) %&gt;% plotly::layout(title = &quot;Customised plot&quot;) If you are purely interested in exploring the data, rather than presenting it to someone else, the plots here are also embedded into a Shiny app which lets you quickly explore and compare indicator distributions - see Data Treatment for more details on this. 4.3 Ranks and Maps While the previous functions concerned plotting the statistical distributions of each indicator, functions are also available for plotting the indicator values in order or on a map. iplotBar(ASEM, dset = &quot;Raw&quot;, isel = &quot;Embs&quot;, usel = &quot;SGP&quot;) Here, a single indicator is plotted in order as a bar chart. There is an optional argument to highlight one or more units, using the usel argument. From a different perspective, we can plot the same data on a map: iplotMap(ASEM, dset = &quot;Raw&quot;, isel = &quot;Embs&quot;) Note that this only works if IndData$UnitCode correspond to ISO alpha-3 country codes. If you want to do some more sophisticated mapping R, Plotly has many mapping options, but R in general has all kinds of mapping packages, you just have to search for them. COINr uses Plotly maps to keep things simple and to not depend on too many packages. COINr has yet more tools to plot data, but lets leave it at that for the moment. Other tools will be introduced in other chapters. 4.4 Statistics and analysis Aside from plots, COINr gives a fairly detailed statistical analysis of initial indicator data. The function getStats() returns a series of statistics which can be aimed at any of the data sets in the .$Data folder. You can also specify if you want the output to be returned back to the COIN, or to a separate list. # get stats ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;, out2 = &quot;COIN&quot;) ## Number of collinear indicators = 3 ## Number of signficant negative indicator correlations = 322 ## Number of indicators with high denominator correlations = 7 # display in table using Reactable # (note the use of helper function roundDF() to round the values to a sensible number of decimals) ASEM$Analysis$Raw$StatTable %&gt;% roundDF() %&gt;% reactable::reactable(resizable = TRUE, bordered = TRUE, highlight = TRUE, defaultPageSize = 10) The columns of this table give all kind of information from max, min, standard deviation, etc, to the presence of outliers and amount of missing data. Apart from the overall statistics for each indicator, getStats also returns a few other things: .$Outliers, which flags individual outlying points using the relation to the interquartile range .$Correlations, which gives a correlation matrix between all indicators in the data set .$DenomCorrelations, which gives the correlations between indicators and any denominators Each of these aspects will be explained in more detail in later chapters (to add which), so for the moment it is enough to mention that they exist. "],["multivariate-analysis.html", "Chapter 5 Multivariate analysis 5.1 Correlations 5.2 PCA", " Chapter 5 Multivariate analysis Correlations, and other relationships between indicators, can help to understand the structure of the data and to see whether indicators are redundant or are mistakenly encoded. 5.1 Correlations Correlations are a measure of statistical dependence between one variable and another. Often, what is meant by correlation, is the Pearson correlation coefficient, which is a measure of linear dependence between two variables. Its worth spending a moment to consider what this the implication of the linear part actually means. Consider the following example: # numbers in [-1, 1] x1 &lt;- seq(-1, 1, 0.1) # x2 is a function of x1 x2 &lt;- x1^2 # plot library(ggplot2) qplot(x1, x2) Now, clearly there is a very strong relationship between x1 and x2. In fact, x2 is completely dependent on x2 (there are no other variables or noise that control its values). So what happens if we check the correlation? cor(x1, x2, method = &quot;pearson&quot;) ## [1] 1.216307e-16 We end up with a correlation of zero. This is because although there is a very strong nonlinear relationship between \\(x_1\\) and \\(x_2\\), the linear relationship is zero. This is made even clearer when we fit a linear regression: qplot(x1, x2) + geom_smooth(method=lm, # Add linear regression line se=FALSE) # Don&#39;t add shaded confidence region ## `geom_smooth()` using formula &#39;y ~ x&#39; Obviously, this is a very contrived example, and it is very unlikely you will see a relationship like this in real indicator data. However the point is that sometimes, linear measures of dependence dont tell the whole story. That said, relationships between indicators can often turn out to be fairly linear. Exceptions arise when indicators are highly skewed, for example. In these cases, a good alternative is to turn to rank correlation measures. Two well-known such measures are the Spearman rank correlation and the Kendall rank correlation. Rank correlations can handle nonlinearity as long as the relationship is monotonic. You could also argue that since the focus of composite indicators is often on ranks, rank correlation also makes sense on a conceptual level. COINr offers a few ways to check correlations that are more convenient than stats::cor(). The first is to call the getStats() function that was already mentioned in Initial visualisation and analysis. This can be pointed at any data set and indicator subset and will give correlation between all indicators present. For example, taking the raw data of our example data set: library(COINr) library(magrittr) ASEM &lt;- build_ASEM() statlist &lt;- getStats(ASEM, dset = &quot;Raw&quot;, out2 = &quot;list&quot;) # see a bit of correlation matrix statlist$Correlations[1:5, 1:5] ## IndCode Goods Services FDI PRemit ## 1 Goods NA 0.8886729 0.8236043 0.8580885 ## 2 Services 0.8886729 NA 0.8567035 0.8322828 ## 3 FDI 0.8236043 0.8567035 NA 0.8009064 ## 4 PRemit 0.8580885 0.8322828 0.8009064 NA ## 5 ForPort 0.4750653 0.6955028 0.4366541 0.4837038 # see a bit of correlations with denominators statlist$DenomCorrelations[, 1:5] ## Denominator Goods Services FDI PRemit ## 1 Den_Area 0.2473558 0.2071061 0.3668625 0.2429470 ## 2 Den_Energy 0.6276219 0.5916882 0.7405170 0.5312195 ## 3 Den_GDP 0.8026770 0.7708846 0.8482453 0.7111943 ## 4 Den_Pop 0.4158811 0.4577006 0.6529376 0.4379999 The type of correlation can be changed by the cortype argument, which can be set to pearson or kendall. The getStats() function also summarises some of this information in its output in the .$StatTable dataframe: statlist$StatTable[ c(&quot;Indicator&quot;, &quot;Collinearity&quot;, &quot;Neg.Correls&quot;, &quot;Denom.correlation&quot;)] %&gt;% head(10) ## # A tibble: 10 x 4 ## Indicator Collinearity Neg.Correls Denom.correlation ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Goods Collinear 1 High ## 2 Services OK 1 High ## 3 FDI OK 1 High ## 4 PRemit OK 0 High ## 5 ForPort OK 6 OK ## 6 CostImpEx OK 16 OK ## 7 Tariff OK 15 OK ## 8 TBTs OK 7 OK ## 9 TIRcon OK 5 OK ## 10 RTAs OK 9 OK This flags any indicators that have collinearity with any other indicators, or are significantly negatively correlated, or have correlations with denominators. These flags are activated based on thresholds that are inputs to the getStats() function (t_colin and t_denom). Correlations can also be plotted using the plotCorr() function. This is a flexible plotting function, powered by ggplot2, which is adapted to the composite indicator context. It has a number of options because there are many ways to plot correlations in a composite indicator, including within an aggregation level, between levels, selecting certain groups, focusing on parents, and so on. To see this, here are a few examples. First, lets plot correlations between raw indicators. plotCorr(ASEM, dset = &quot;Raw&quot;, aglevs = 1, showvals = F) Here, we have plotted raw indicators against indicators. A few things to note: By default, if a level is plotted against itself, it only plots correlations within the groups of the next aggregation level above. Use the grouplev argument to control the grouping or disable. Insignificant correlations (at 5% level) are excluded by default. Use the pval argument to change the significance threshold or to disable. We can repeat the same plot with some variations. First, grouping by sub-index: plotCorr(ASEM, dset = &quot;Raw&quot;, aglevs = 1, showvals = F, grouplev = 3) Second, we can plot the whole correlation matrix, and also include insignificant correlations: plotCorr(ASEM, dset = &quot;Raw&quot;, aglevs = 1, showvals = F, grouplev = 0, pval = 0) It may often be more interesting to plot correlations between aggregation levels. Since there is a bit more space, we will also enable the values of the correlations using showvals = TRUE. plotCorr(ASEM, dset = &quot;Aggregated&quot;, aglevs = c(1,2), showvals = T) Note that again, by default the correlations are grouped with the parent level above. This is disabled by setting withparent = \"none\". To see correlations with multiple parents at once, we can set withparent = \"family\" plotCorr(ASEM, dset = &quot;Aggregated&quot;, aglevs = c(1,2), showvals = T, withparent = &quot;family&quot;) It is also possible to switch to a discrete colour map to highlight negative, weak and collinear values. plotCorr(ASEM, dset = &quot;Aggregated&quot;, aglevs = c(2,3), showvals = T, withparent = &quot;family&quot;, flagcolours = TRUE) The thresholds for these colours can be controlled by the flagthresh argument. This type of plot helps to see at a glance where there might be a problematic indicator. In the example here, we see that TBTs are negatively correlated with all levels of the index. Finally, it might be useful to simply have this information as a table rather than a plot. Setting out2 to dflong or dfwide outputs a data frame in long or wide format, instead of a figure. This can be helpful for presenting the information in a report or doing custom formatting. plotCorr(ASEM, dset = &quot;Aggregated&quot;, aglevs = c(2,3), showvals = T, withparent = &quot;family&quot;, flagcolours = TRUE, out2 = &quot;dfwide&quot;) ## # A tibble: 8 x 3 ## Indicator Agg2 Agg3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ConEcFin 0.53 0.42 ## 2 Instit 0.86 0.81 ## 3 P2P 0.86 0.78 ## 4 Physical 0.87 0.83 ## 5 Political 0.64 0.68 ## 6 Environ 0.48 NA ## 7 Social 0.74 0.9 ## 8 SusEcFin NA NA Arguably its not so intuitive for a function called plotCorr() to output tables, but thats how it is for now. If you like your correlation heatmaps interactive, COINr has another function called iplotCorr(). It functions in a similar way plotCorr(), but is designed in particular as a component of the rew8r() app and can be used in HTML documents. For a basic correlation map: iplotCorr(ASEM, aglevs = c(2, 3)) Unlike plotCorr(), indicators are grouped by rectangles around the groups. This might be better for small plots but can be a bit confusing when there are many elements plotted at the same time. The iplotCorr() function can also show a continuous colour scale, and values can be removed. iplotCorr(ASEM, aglevs = c(1,2), showvals = F, flagcolours = F) Like plotCorr(), threshold values can be controlled for the discrete colour map. The function is powered by plotly, which means it can be further altered by using Plotly commands. 5.2 PCA "],["missing-data-and-imputation.html", "Chapter 6 Missing data and Imputation 6.1 Concept 6.2 Data checks and screening 6.3 Imputation in COINr", " Chapter 6 Missing data and Imputation Imputation is the process of estimating missing data points. This can be done in any number of ways, and as usual, the best way depends on the problem. Of course, you dont have to impute data. You can also simply delete any indicator or unit that has missing values, although in many cases this can be too restrictive. Reasonable results can still be obtained despite small amounts of missing data, although if too much data is missing, the uncertainty can be too high to give a meaningful analysis. As usual, it is a balance. A good first step is to check how much data is missing, and where (see below). Units with very high amounts of missing data can be screened out. Small amounts of missing data can then be imputed. 6.1 Concept The simplest imputation approach is to use values of other units to estimate the missing point. Typically, this could involve the sample mean or median of the indicator. Heres some data from the ASEM data set regarding the average connection speed of each country. Towards the end there are some missing values, so lets view the last few rows: library(COINr) Ind1 &lt;- data.frame(Country = ASEMIndData$UnitName, ConSpeed = ASEMIndData$ConSpeed) Ind1[40:nrow(Ind1),] ## Country ConSpeed ## 40 Korea 28.6 ## 41 Lao PDR NA ## 42 Malaysia 8.9 ## 43 Mongolia NA ## 44 Myanmar NA ## 45 New Zealand 14.7 ## 46 Pakistan NA ## 47 Philippines 5.5 ## 48 Russian Federation 11.8 ## 49 Singapore 20.3 ## 50 Thailand 16.0 ## 51 Vietnam 9.5 Using our simple imputation method, we just replace the NA values with the sample mean. Ind1$ConSpeed &lt;- replace(Ind1$ConSpeed, is.na(Ind1$ConSpeed), mean(Ind1$ConSpeed, na.rm = T)) Ind1[40:nrow(Ind1),] ## Country ConSpeed ## 40 Korea 28.60000 ## 41 Lao PDR 14.28605 ## 42 Malaysia 8.90000 ## 43 Mongolia 14.28605 ## 44 Myanmar 14.28605 ## 45 New Zealand 14.70000 ## 46 Pakistan 14.28605 ## 47 Philippines 5.50000 ## 48 Russian Federation 11.80000 ## 49 Singapore 20.30000 ## 50 Thailand 16.00000 ## 51 Vietnam 9.50000 This approach can be reasonable if countries are somehow similar in that indicator. However, other perhaps more informative ways are available: Substituting by the mean or median of the country group (e.g. income group or continent) If time series data is available, use the latest known data point Then there is another class of methods which use data from other indicators to estimate the missing point. The core idea here is that if the indicators are related to one another, it is possible to guess the missing point by using known values of other indicators. This can take the form of: Simply substituting the mean or median of the normalised values of the other indicators Subsituting the mean or median of normalised values of the other indicators within the aggregation group Using a more formal approach, based on regression or more generally on statistical modelling Lets explore the options available in COINr 6.2 Data checks and screening A first step is to check in detail how much data missing. The function checkData() does this, and also has the option to screen units based on data availability. # Assemble ASEM data first ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- # Missing data check on the raw data set ASEM &lt;- checkData(ASEM, dset = &quot;Raw&quot;) head(ASEM$Analysis$Raw$MissDatSummary) ## UnitCode N_missing N_zero N_miss_or_zero PrcDataAll PrcNonZero LowDataAll ## 1 AUT 0 2 2 100 95.91837 FALSE ## 2 BEL 0 2 2 100 95.91837 FALSE ## 3 BGR 0 0 0 100 100.00000 FALSE ## 4 HRV 0 1 1 100 97.95918 FALSE ## 5 CYP 0 3 3 100 93.87755 FALSE ## 6 CZE 0 3 3 100 93.87755 FALSE ## ZeroFlag LowDatOrZeroFlag Included ## 1 FALSE FALSE TRUE ## 2 FALSE FALSE TRUE ## 3 FALSE FALSE TRUE ## 4 FALSE FALSE TRUE ## 5 FALSE FALSE TRUE ## 6 FALSE FALSE TRUE Note: to include updated checkData() function with screening for zeros. The MissDataSummary table shows the unit code, number of missing observations, and overall percentage data availability. Finally, the LowDataAll column can be used as a flag for units with data availability lower than a set amount ind_thresh which is one of the input arguments to checkData(). ASEM indicators were already chosen to fulfill minimum data requirements, for which reason the data availability is all above the default of 2/3. We will set the minimum data threshold a bit higher, and in doing so also demonstrate another feature of checkData(), which is to automatically screen units based on data availability. Setting unit_screen = TRUE will generate a new data set .$Data$Screened which only includes units with data availability above the set threshold. This data set can then be passed on to subsequent operations. # Missing data check on the raw data set ASEM &lt;- checkData(ASEM, dset = &quot;Raw&quot;, ind_thresh = 0.85, unit_screen = TRUE) ASEM$Analysis$Raw$RemovedUnits ## character(0) This generates a new data set .$Data$Screened with the removed units recorded in .$Analysis$Raw$RemovedUnits, which in this case are Brunei, Laos and Myanmar. As a final option to mention, you can manually include or exclude units. So for example, a unit that doesnt have sufficient data coverage could be manually included anyway, and another unit could be excluded. # Countries to include and exclude df &lt;- data.frame(UnitCode = c(&quot;AUT&quot;,&quot;BRN&quot;), Status = c(&quot;Exclude&quot;, &quot;Include&quot;)) # Missing data check on the raw data set ASEM &lt;- checkData(ASEM, dset = &quot;Raw&quot;, ind_thresh = 0.85, unit_screen = TRUE, Force = df) ASEM$Analysis$Raw$RemovedUnits ## [1] &quot;AUT&quot; Now AUT has been excluded, even though it had sufficient data coverage, and BRN has been manually included, even though it didnt. The intention is to give some flexibility to make exceptions for hard rules. The other output of interest from checkData() is missing data by group: head(ASEM$Analysis$Raw$MissDatByGroup[30:40,]) ## UnitCode ConEcFin Instit P2P Physical Political Environ Social ## 30 GBR 100 100.00000 100 100.0 100 100 100.00000 ## 31 AUS 100 100.00000 100 100.0 100 100 100.00000 ## 32 BGD 100 83.33333 75 87.5 100 100 88.88889 ## 33 BRN 80 100.00000 75 87.5 100 100 55.55556 ## 34 KHM 80 100.00000 75 87.5 100 100 77.77778 ## 35 CHN 100 100.00000 100 100.0 100 100 100.00000 ## SusEcFin Conn Sust Index ## 30 100 100.00000 100.00000 100.00000 ## 31 100 100.00000 100.00000 100.00000 ## 32 80 86.66667 89.47368 87.75510 ## 33 60 86.66667 68.42105 79.59184 ## 34 100 86.66667 89.47368 87.75510 ## 35 80 100.00000 94.73684 97.95918 This gives the percentage indicator data availability inside each aggregation group. 6.3 Imputation in COINr Now we turn to estimating missing data points in COINr. The function of interest is impute(), which gives a number of options, corresponding to the types of imputation discussed above. Briefly, COINr can impute using: The indicator mean, either across all countries or within a specified group The indicator median, either across all countries or within a specified group The mean or median, within the aggregation group The expectation maximisation algorithm, via the AMELIA package 6.3.1 By column The impute() function follows a similar logic to other COINr functions. We can enter a COIN or a data frame, and get a COIN or a data frame back. Lets impute on a COIN first, using the raw ASEM data. # Assemble ASEM data if not done already (uncomment the following) # ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, # AggMeta = COINr::ASEMAggMeta) # check how many NAs we have sum(is.na(ASEM$Data$Raw)) ## [1] 63 # impute using indicator mean ASEM &lt;- impute(ASEM, dset = &quot;Raw&quot;, imtype = &quot;ind_mean&quot;) ## Missing data points detected = 63 ## Missing data points imputed = 63, using method = ind_mean # check how many NAs after imputation sum(is.na(ASEM$Data$Imputed)) ## [1] 0 If the input is a COIN, by default the output is an updated COIN, with a new data set .$Data$Imputed. We can also set out2 = \"df\" to output the imputed data frame on its own. The above example replaces NA values with the mean of the indicator, and setting imtype = ind_median will instead use the indicator median. In this respect, we are using information from other units, inside the same indicator, to estimate the missing values. Imputing by group may be more informative if you have meaningful grouping variables. For example, the ASEM data set has groupings by GDP, population, GDP per capita and whether the country is European or Asian. We might hypothesise that it is better to replace NA values with the median inside a group, say by GDP group. This would imply that countries with a similar GDP are likely to have similar indicator values. # impute using GDP group median ASEM &lt;- impute(ASEM, dset = &quot;Raw&quot;, imtype = &quot;indgroup_median&quot;, groupvar = &quot;Group_GDP&quot;) ## Missing data points detected = 63 ## Missing data points imputed = 63, using method = indgroup_median 6.3.2 By row So far, the imputation has been using values from the same column (indicator). Another possibility is to use values from other indicators, i.e operate row-wise. COINr offers two basic options in this respect: either to take the mean or median of the other indicators inside the aggregation group, and this can be done by setting imtype = \"agg_mean\" or imtype = \"agg_median\" respectively. Both of these imputation methods will, for a given unit, take the mean or median of the normalised values of the other indicators in the aggregation group in the level above indicator level. The process is as follows, for each NA value: Find indicators in the same aggregation group Normalise these indicators using the min-max method, so they each have a minimum of zero and a maximum of one. Replace the NA with the mean or median of the normalised values, for the unit in question. Reverse the min-max transformation so that the indicators are returned to their original scale. Its important to realise that this is equivalent to aggregating with the mean or median without imputing first. This is because in the aggregation step, if we take the mean of a group of indicators, and there is a NA present, this value is excluded from the mean calculation. Doing this is mathematically equivalent to assigning the mean to that missing value. Therefore, one reason to use this imputation method is to see which values are being implicitly assigned as a result of excluding missing values from the aggregation step. This is sometimes known as shadow imputation. 6.3.3 By column and row Finally, we can use a regression-based expectation maximisation algorithm, imported via the AMELIA package. This estimates missing values using the values of the other indicators and the other units. This effectively involves regressing each indicator on other indicators, and predicting missing values. A catch of this approach is that if the number of units is small compared to the number of indicators, there will not be enough observations to estimate the parameters of the model. This is definitely the case if you have more indicators than observations. To use the EM algorithm in this case, imputation is performed on aggregation groups. So, to use the EM approach in COINr, you have to specify the aggregation level to group indicators by. # impute using EM, grouping indicators inside their pillars (level 2) ASEM &lt;- impute(ASEM, imtype = &quot;EM&quot;, EMaglev = 2) ## Missing data points detected = 63 ## Missing data points imputed = 63, using method = EM This works, because the ASEM data set has 51 units and no more thann eight indicators per pillar. However, if we were to use EMaglev = 3, i.e. try imputing indicators grouped into the two sub-indexes, it does not work. Finding the right aggregation level to impute at might involve a bit of trial and error, but the advantage of imputing by groups of indicators is that aggregation groups are usually composed of indicators that are similar in some respect. This makes it more likely that they are good predictors of the other indicators in their group. The AMELIA package offers many more options for imputation that are not available through the impute() function, such as bootstrapping, time-series imputation and more. As with plotting packages COINr aims to provide an easy and quick interface to standard imputation in AMELIA, but if you want to have full control you should use the AMELIA package directly. Other good imputation options are the MICE package (Multivariate Imputation via Chained Equations) and the missForest package (using random forests). "],["denomination.html", "Chapter 7 Denomination 7.1 Concept 7.2 Denominating in COINr 7.3 When to denominate, and by what?", " Chapter 7 Denomination The first section here gives a bit of introduction to the concept of denomination. If you just want to know how to denominate in COINr, skip to the section on Denominating in COINr. 7.1 Concept Denomination refers to the operation of dividing one indicator by another. But why should we do that? As anyone who has ever looked at a map will know, countries come in all different sizes. The problem is that many indicators, on their own, are strongly linked to the size of the country. That means that if we compare countries using these indicators directly, we will often get a ranking that has roughly the biggest countries at the top, and the smallest countries at the bottom. To take an example, lets examine some indicators in the ASEM data set, and introduce another plotting function in the process. # load COINr package library(COINr) library(magrittr) # for pipe operations # build ASEM index ASEM &lt;- build_ASEM() # plot international trade in goods against GDP iplotIndDist2(ASEM, dsets = c(&quot;Denominators&quot;, &quot;Raw&quot;), icodes = c(&quot;Den_GDP&quot;, &quot;Goods&quot;)) # (note: need to fix labelling and units of denominators) The function iplotIndDist2() is similar to iplotIndDist() but allows plotting two indicators against each other. You can pick any indicator from any data set for each, including denominators. What are these denominators anyway? Denominators are indicators that are used to scale other indicators, in order to remove the size effect. Typically, they are those related to physical or economic size, including GDP, population, land area and so on. Anyway, looking at the plot above, it is clear that that countries with a higher GDP have a higher international trade in international goods (e.g. Germany, China, UK, Japan, France), which is not very surprising. The problem comes when we want to include this indicator as a measure of connectivity: on its own, trade in goods simply summarises having a large economy. What is more interesting, is to measure international trade per unit GDP, and this is done by dividing (i.e. denominating) the international trade of each country by its GDP. Lets do that manually here and see what we get. # divide trade by GDP tradeperGDP &lt;- ASEM$Data$Raw$Goods/ASEM$Input$Denominators$Den_GDP # bar chart: add unit names first iplotBar(data.frame(UnitCode=ASEM$Parameters$UnitCodes, TradePerGDP = tradeperGDP)) Now the picture is completely different - small countries like Slovakia, Czech Republic and Singapore have the highest values. The rankings here are completely different because the meanings of these two measures are completely different. Denomination is in fact a nonlinear transformation, because every value is divided by a different value (each country is divided by its own unique GDP, in this case). That doesnt mean that denominated indicators are suddenly more right than the before their denomination, however. Trade per GDP is a useful measure of how much a countrys economy is dependent on international trade, but in terms of economic power, it might not be meaningful to scale by GDP. In summary, it is important to consider the meaning of the indicator compared to what you want to actually measure. More precisely, indicators can be thought of as either intensive or extensive variables. Intensive variables are not (or only weakly) related to the size of the country, and allow fair comparisons between countries of different sizes. Extensive variables, on the contrary, are strongly related to the size of the country. This distinction is well known in physics, for example. Mass is related to the size of the object and is an extensive variable. If we take a block of steel, and double its size (volume), we also double its mass. Density, which is mass per unit volume, is an intensive quantity: if we double the size of the block, the density remains the same. An example of an extensive variable is population. Bigger countries tend to have bigger populations. An example of an intensive variable is population density. This is no longer dependent on the physical size of the country. The summary here is that an extensive variable becomes an intensive variable when we divide it by a denominator. 7.2 Denominating in COINr Denomination is fairly simple to do in R, its just dividing one vector by another. Nevertheless, COINr has a dedicated function for denominating which makes life easier and helps you to track what you have done. Before we get to that, its worth mentioning that COINr has a few built-in denominators sourced from the World Bank. It looks like this: WorldDenoms ## # A tibble: 249 x 7 ## UnitName UnitCode Den_GDP Den_Pop Den_Area Den_GDPpc Den_IncomeGroup ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan AFG 1.93e10 3.80e7 652860 507. Low income ## 2 Albania ALB 1.53e10 2.85e6 27400 5353. Upper middle in~ ## 3 Algeria DZA 1.71e11 4.31e7 2381740 3974. Lower middle in~ ## 4 American Sam~ ASM 6.36e 8 5.53e4 200 11467. Upper middle in~ ## 5 Andorra AND 3.15e 9 7.71e4 470 40886. High income ## 6 Angola AGO 8.88e10 3.18e7 1246700 2791. Lower middle in~ ## 7 Anguilla AIA NA NA NA NA &lt;NA&gt; ## 8 Antarctica ATA NA NA NA NA &lt;NA&gt; ## 9 Antigua and ~ ATG 1.66e 9 9.71e4 440 17113. High income ## 10 Argentina ARG 4.45e11 4.49e7 2736690 9912. Upper middle in~ ## # ... with 239 more rows and the metadata can be found by calling ?WorldDenoms. Data here is the latest available as of February 2021 and I would recommend using these only for exploration, then updating your data yourself. To denominate your indicators in COINr, the function to call is denominate(). Like other COINr functions, this can be used either independently on a data frame of indicators, or on COINs. Consider that in all cases you need three main ingredients: Some indicator data that should be denominated Some other indicators to use as denominators A mapping to say which denominators (if any) to use for each indicator. 7.2.1 On COINs If you are working with a COIN, the indicator data will be present in the .$Data folder. If you specified any denominators in IndData (i.e. columns beginning with Den_) when calling assemble() you will also find them in .$Input$Denominators. Finally, if you specified a Denominator column in IndMeta when calling assemble() then the mapping of denominators to indicators will also be present. # The raw indicator data which will be denominated ASEM$Data$Raw ## # A tibble: 51 x 56 ## UnitCode UnitName Year Group_GDP Group_GDPpc Group_Pop Group_EurAsia Goods ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AUT Austria 2018 L XL M Europe 278. ## 2 BEL Belgium 2018 L L L Europe 598. ## 3 BGR Bulgaria 2018 S S M Europe 42.8 ## 4 HRV Croatia 2018 S M S Europe 28.4 ## 5 CYP Cyprus 2018 S L S Europe 8.77 ## 6 CZE Czech R~ 2018 M L M Europe 274. ## 7 DNK Denmark 2018 L XL M Europe 147. ## 8 EST Estonia 2018 S M S Europe 28.2 ## 9 FIN Finland 2018 M XL M Europe 102. ## 10 FRA France 2018 XL L L Europe 849. ## # ... with 41 more rows, and 48 more variables: Services &lt;dbl&gt;, FDI &lt;dbl&gt;, ## # PRemit &lt;dbl&gt;, ForPort &lt;dbl&gt;, CostImpEx &lt;dbl&gt;, Tariff &lt;dbl&gt;, TBTs &lt;dbl&gt;, ## # TIRcon &lt;dbl&gt;, RTAs &lt;dbl&gt;, Visa &lt;dbl&gt;, StMob &lt;dbl&gt;, Research &lt;dbl&gt;, ## # Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, MigStock &lt;dbl&gt;, ## # Lang &lt;dbl&gt;, LPI &lt;dbl&gt;, Flights &lt;dbl&gt;, Ship &lt;dbl&gt;, Bord &lt;dbl&gt;, Elec &lt;dbl&gt;, ## # Gas &lt;dbl&gt;, ConSpeed &lt;dbl&gt;, Cov4G &lt;dbl&gt;, Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, ## # UNVote &lt;dbl&gt;, Renew &lt;dbl&gt;, PrimEner &lt;dbl&gt;, CO2 &lt;dbl&gt;, MatCon &lt;dbl&gt;, ## # Forest &lt;dbl&gt;, Poverty &lt;dbl&gt;, Palma &lt;dbl&gt;, TertGrad &lt;dbl&gt;, FreePress &lt;dbl&gt;, ## # TolMin &lt;dbl&gt;, NGOs &lt;dbl&gt;, CPI &lt;dbl&gt;, FemLab &lt;dbl&gt;, WomParl &lt;dbl&gt;, ## # PubDebt &lt;dbl&gt;, PrivDebt &lt;dbl&gt;, GDPGrow &lt;dbl&gt;, RDExp &lt;dbl&gt;, NEET &lt;dbl&gt; # The denominators ASEM$Input$Denominators ## # A tibble: 51 x 11 ## UnitCode UnitName Year Group_GDP Group_GDPpc Group_Pop Group_EurAsia ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AUT Austria 2018 L XL M Europe ## 2 BEL Belgium 2018 L L L Europe ## 3 BGR Bulgaria 2018 S S M Europe ## 4 HRV Croatia 2018 S M S Europe ## 5 CYP Cyprus 2018 S L S Europe ## 6 CZE Czech R~ 2018 M L M Europe ## 7 DNK Denmark 2018 L XL M Europe ## 8 EST Estonia 2018 S M S Europe ## 9 FIN Finland 2018 M XL M Europe ## 10 FRA France 2018 XL L L Europe ## # ... with 41 more rows, and 4 more variables: Den_Area &lt;dbl&gt;, ## # Den_Energy &lt;dbl&gt;, Den_GDP &lt;dbl&gt;, Den_Pop &lt;dbl&gt; # The mapping of denominators to indicators (see Denominator column) ASEM$Input$IndMeta ## # A tibble: 49 x 10 ## IndName IndCode Direction IndWeight Denominator IndUnit Target Agg1 Agg2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Trade ~ Goods 1 1 Den_GDP Trilli~ 1.82e+3 ConE~ Conn ## 2 Trade ~ Servic~ 1 1 Den_GDP Millio~ 6.24e+2 ConE~ Conn ## 3 Foreig~ FDI 1 1 Den_GDP Billio~ 7.18e+1 ConE~ Conn ## 4 Person~ PRemit 1 1 Den_GDP Millio~ 2.87e+1 ConE~ Conn ## 5 Foreig~ ForPort 1 1 Den_GDP Millio~ 1.01e+4 ConE~ Conn ## 6 Cost t~ CostIm~ -1 1 &lt;NA&gt; Curren~ 4.96e+1 Inst~ Conn ## 7 Mean t~ Tariff -1 1 &lt;NA&gt; Percent 5.26e-1 Inst~ Conn ## 8 Techni~ TBTs -1 1 &lt;NA&gt; Number~ 8.86e+1 Inst~ Conn ## 9 Signat~ TIRcon 1 1 &lt;NA&gt; (1 (ye~ 9.50e-1 Inst~ Conn ## 10 Region~ RTAs 1 1 &lt;NA&gt; Number~ 4.38e+1 Inst~ Conn ## # ... with 39 more rows, and 1 more variable: Agg3 &lt;chr&gt; COINrs denominate() function knows where to look for each of these ingredients, so we can simply call: ASEM &lt;- denominate(ASEM) which will return a new data set .Data$Denominated. To return the dataset directly, rather than outputting an updated COIN, you can also set out2 = \"df\" (this is a common argument to many functions which can be useful if you want to examine the result directly). You can also change which indicators are denominated, and by what. # Get denominator specification from metadata denomby_meta &lt;- ASEM$Input$IndMeta$Denominator # Example: we want to change the denominator of flights from population to GDP denomby_meta[ASEM$Input$IndMeta$IndCode == &quot;Flights&quot;] &lt;- &quot;Den_GDP&quot; # Now re-denominate. Return data frame for inspection ASEM &lt;- denominate(ASEM, dset = &quot;Raw&quot;, specby = &quot;user&quot;, denomby = denomby_meta) Here we have changed the denominator of one of the indicators, Flights, to GDP. This is done by creating a character vector denomby_meta (copied from the original denominator specification) which has an entry for each indicator, specifying which denominator to use, if any. We then changed the entry corresponding to Flights. This overwrites any previous denomination. If you want to keep and compare alternative specifications, see the chapter on (chap to add). Lets compare the raw Flights data with the Flights per GDP data: # plot raw flights against denominated iplotIndDist2(ASEM, dsets = c(&quot;Raw&quot;, &quot;Denominated&quot;), icodes = &quot;Flights&quot;) Clearly, the denominated and raw indicators are very different from one another, reflecting the completely different meaning. 7.2.2 On data frames If you are just working with data frames, you need to supply the three ingredients directly to the function. Here we will take some of the ASEM data for illustration (recalling that both indicator and denominator data is specified in ASEMIndMeta). # a small data frame of indicator data IndData &lt;- ASEMIndData[c(&quot;Goods&quot;, &quot;Services&quot;, &quot;FDI&quot;)] IndData ## # A tibble: 51 x 3 ## Goods Services FDI ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 278. 108. 5 ## 2 598. 216. 5.71 ## 3 42.8 13.0 1.35 ## 4 28.4 17.4 0.387 ## 5 8.77 15.2 1.23 ## 6 274. 43.5 3.88 ## 7 147. 114. 9.1 ## 8 28.2 10.2 0.580 ## 9 102. 53.8 6.03 ## 10 849. 471. 30.9 ## # ... with 41 more rows # two selected denominators Denoms &lt;- ASEMIndData[c(&quot;Den_Pop&quot;, &quot;Den_GDP&quot;)] # denominate the data IndDataDenom &lt;- denominate(IndData, denomby = c(&quot;Den_GDP&quot;, NA, &quot;Den_Pop&quot;), denominators = Denoms) IndDataDenom ## # A tibble: 51 x 3 ## Goods Services FDI ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.712 108. 0.000572 ## 2 1.28 216. 0.000500 ## 3 0.804 13.0 0.000191 ## 4 0.554 17.4 0.0000924 ## 5 0.437 15.2 0.00104 ## 6 1.40 43.5 0.000365 ## 7 0.478 114. 0.00159 ## 8 1.21 10.2 0.000443 ## 9 0.426 53.8 0.00109 ## 10 0.344 471. 0.000476 ## # ... with 41 more rows Since the input is recognised as a data frame, you dont need to specify any other arguments, and the output is automatically a data frame. Note how denomby works: here it specifies that that Goods should be denominated by Den_GDP, that Services should not be denominated, and that FDI should be denominated by Den_Pop. 7.3 When to denominate, and by what? Denomination is mathematically very simple, but from a conceptual point of view it needs to be handled with care. As we have shown, denominating an indicator will usually completely change it, and will have a corresponding impact on the results. Two ways of looking at the problem are first, from the conceptual point of view. Consider each indicator and whether it fits with the aim of your index. Some indicators are anyway intensive, such as the percentage of tertiary graduates. Others, such as trade, will be strongly linked to the size of the country. In those cases, consider whether countries with high trade values should have higher scores in your index or not? Or should it be trade as a percentage of GDP? Or trade per capita? Each of these will have different meanings. Sometimes extensive variables will anyway be the right choice. The Lowy Asia Power Index measures the power of each country in an absolute sense: in this case, the size of the country is all-important and e.g. trade or military capabilities per capita would not make much sense. The second (complimentary) way to approach denomination is from a statistical point of view. We can check which indicators are strongly related to the size of a country by correlating them with some typical denominators, such as the ones used here. The getStats() function does just this, checking the correlations between each indicator and each denominator, and flagging any possible high correlations. # get statistics on raw data set ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;, out2 = &quot;COIN&quot;, t_denom = 0.8) ## Number of collinear indicators = 3 ## Number of signficant negative indicator correlations = 322 ## Number of indicators with high denominator correlations = 4 # get the result ctable &lt;- ASEM$Analysis$Raw$DenomCorrelations # remove first column ctable &lt;- ctable[-1] # return only columns that have entries with correlation above 0.8 ctable[,colSums((abs(ctable) &gt; 0.8))&gt;0] ## Goods FDI StMob CultGood ## 1 0.2473558 0.3668625 0.5228032 0.2696675 ## 2 0.6276219 0.7405170 0.7345030 0.7120445 ## 3 0.8026770 0.8482453 0.8379575 0.8510374 ## 4 0.4158811 0.6529376 0.5523545 0.5028121 The matrix that is displayed above only includes columns where there is a correlation value (between an indicator and any denominator) of greater than 0.8. The results show some interesting patterns: Many indicators have a high positive correlation with GDP, including flights, trade, foreign direct investment (very high), personal remittances, research, stock of migrants, and so on. Bigger countries, in terms of land area, tend to have less trade agreements (RTAs) and a more restrictive visa policy Larger populations are associated with higher levels of poverty The purpose here is not to draw causal links between these quantities, although they might reveal interesting patterns. Rather, these might suggest which quantities to denominate by, if the choices also work on a conceptual level. "],["data-treatment.html", "Chapter 8 Data Treatment 8.1 Why treat data? 8.2 How to treat data 8.3 Data treatment in COINr 8.4 Interactive visualisation", " Chapter 8 Data Treatment Data treatment is the process of altering indicators to improve their statistical properties, mainly for the purposes of aggregation. Data treatment is a delicate subject, because it essentially involves changing the values of certain observations, or transforming an entire distribution. This entails balancing two opposing considerations: On the one hand, treatment should be used as sparingly as possible, because you are altering one or more known data points. On the other hand, this is only done for the purposes of aggregation, (i.e. creating a composite score), and since composite indicators are normally presented with the index scores and original data accessible underneath, the underlying data would normally be presented in its original form. Therefore, be careful, but also realise that data treatment is not unethical, its simply an assumption in a statistical process. Like any other step or assumption though, any data treatment should be carefully recorded. 8.1 Why treat data? There can be many reasons to treat data, but in composite indicators the main reason is to remove outliers or adjust heavily-skewed distributions. Outliers can exist because of errors in measurement and data processing, and should always be double-checked. But often, they are simply a reflection of reality. Outliers and skewed distributions are common in economics. One has to look no further than the in-built ASEM data set, and the denominating variables of population, GDP, energy consumption and country area: plotIndDist(WorldDenoms) To illustrate why this can be a problem, consider the following artificial example. library(plotly) library(magrittr) # normally-distributed data outlierdata &lt;- rnorm(100) # two outliers outlierdata &lt;- c(outlierdata, 10, 25) # plot plot_ly(x = outlierdata, type = &quot;histogram&quot;) Here we have the large majority of observations with values which are crammed into the bottom fifth of the scale, and two observations that are much higher, i.e. they are outliers. If we were to normalise this distribution using a min-max method scaled to [0, 100], for example, this would produce an indicator where the large majority of units have a very low score. This might not reflect the narrative that you want to convey. It may be, for example, that values above the median are good, and in this case, values in the 1.5-2.5 range are very good. Two units have truly exception values, but that shouldnt take away from the fact that other units have values that are considered to be good. The problem is that by normalising to [0, 100], these units with very good values will have normalised scores of around 20 (out of 100), which when compared to other indicators, is a bad score. And this will be reflected in the aggregated score. In summary, the outlying values are dominating the scale, which reduces the discriminatory power of the indicator. A few things to consider here are that: The outlier problem in composite indicators is mostly linked to the fact that you are aggregating indicators. If you are not aggregating, the outlying values may not be so much of a problem. It might be that you want to keep the distribution as it is, and let the indicator be defined by its outliers. This will depend on the objectives of the index. If you do wish to do something about the outlying values, there are two possibilities. One is to treat the data, and this is described in the rest of this chapter The second is to use a normalisation method that is less sensitive to outliers - this is dealt with in the Normalisation chapter. 8.2 How to treat data If you decide to treat the data before normalising, there are a two main options. 8.2.1 Winsorisation The first is to Winsorise the data. Winsorisation involves reassigning outlying points to the next highest point, or to a percentile value. To do this manually, using the data from the previous section, it would look like this: # get position of maximum value imax &lt;- which(outlierdata==max(outlierdata)) # reassign with next highest value outlierdata[imax] &lt;- max(outlierdata[-imax]) plot_ly(x = outlierdata, type = &quot;histogram&quot;) # and let's do it again imax And now we have arrived back to a normal distribution with no outliers, which is well-spread over the scale. Of course, this has come at the cost of actually moving data points. However keep in mind that this is only done for the purposes of aggregation, and the original indicator data would still be retained. A helpful way of looking at Winsorisation is that it is like capping the scale: it is enough to know that certain units have the highest score, without needing to know that they are ten times higher than the other units. Clearly, outliers could also occur from the lower end of the scale, in which case Winsorisation would use the minimum values. Notice that in general, Winsorisation does not change the shape of the distribution, apart from the outlying values. Therefore it is suited to distributions like the example, which are well-spread except for some few outlying values. 8.2.2 Transformation The second option is to transform the distribution, by applying a transformation that changes all of the data points and thus the overall shape of the distribution. The most obvious transformation in this respect is to take the logarithm. Denoting \\(x\\) as the original indicator and \\(x&#39;\\) as the transformed indicator, this would simply be: \\[ x&#39; = \\ln(x) \\] This is a sensible choice because skewed distributions are often roughly log-normal, and taking the log of a log-normal distrinbution results in a normal distribution. However, this will not work for negative values. In that case, an alternative is: \\[ x&#39; = \\ln(x- \\text{min}(x)+a), \\; \\; \\text{where}\\; \\; a = 0.01(\\text{max}(x)-\\text{min}(x)) \\] The logic being that by subtracting the minimum and adding something, all values will be positive, so they can be safely log-transformed. This formula is similar to that used in the COIN Tool, but with an adjustment. In the COIN Tool, \\(a=1\\), which, depending on the range of the indicator, can give only a gentle and sometimes near-linear transformation. By setting \\(a\\) to be a set percentage of the range, it is ensured that the shape of the transformation is consistent. A general family of transformations are called the Box-Cox transformations. These are given as: \\[ x&#39;(\\lambda) = \\begin{cases} \\frac{x^\\lambda-1}{\\lambda} &amp; \\text{if}\\ \\lambda \\neq 0 \\\\ \\ln(x) &amp; \\text{if}\\ \\lambda = 0 \\end{cases} \\] In other words, a log transformation or a power transformation. The Box Cox transformation is often accompanied by an optimisation which chooses the \\(\\lambda\\) value to best approximate the normal distribution. Comparing to Winsorisation, log transforms and Box Cox transforms perform the transformation on every data point. To see the effect of this, we can take the log of one of the denominator indicators in WorldDenoms, the national GDPs of over 200 countries in 2019. # make df with original indicator and log-transformed version df &lt;- data.frame(GDP = WorldDenoms$Den_GDP, LogGDP = log(WorldDenoms$Den_GDP)) plotIndDist(df, type = &quot;Histogram&quot;) And there we see a beautiful normal distribution as a result. 8.3 Data treatment in COINr COINr data treatment has a number of options and gives the user a fairly high degree of control in terms of which kind of treatment should be applied, either for all indicators at once, or for individual indicators. We will first explore the default data treatment options that are applied to all indicators, then see how to make exceptions and specific treatment requests. 8.3.1 Global treatment The default treatment in COINr is similar to that applied in the COIN Tool, and widely used in composite indicator construction. It follows a basic process of trying to Winsorise up to a specified limit, followed by a transformation if the Winsorisation does not sufficiently correct the distribution. In short, for each indicator it: Checks skew and kurtosis values If absolute skew is greater than a threshold (default 2) AND kurtosis is greater than a threshold (default 3.5): Successively Winsorise up to a specified maximum number of points. If either skew or kurtosis goes below thresholds, stop. If after reaching the maximum number of points, both thresholds are still exceeded, then: Perform a transformation This is the default process, although individual specifications can be made for individual indicators - see the next section. The data treatment function in COINr is called treat(). To perform a default data treatment, we can simply use the COIN in, COIN out approach as usual. # build ASEM data set, up to denomination ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ASEM &lt;- denominate(ASEM) # treat at defaults ASEM &lt;- treat(ASEM, dset = &quot;Denominated&quot;, winmax = 5) # Check which indicators were treated, and how library(dplyr, quietly = T, verbose = F) library(magrittr) ASEM$Analysis$Treated$TreatSummary %&gt;% filter(Treatment != &quot;None&quot;) ## IndCode Low High TreatSpec Treatment ## V2 Services 0 4 Default, winmax = 5 Winsorised 4 points ## V3 FDI 0 2 Default, winmax = 5 Winsorised 2 points ## V5 ForPort 0 3 Default, winmax = 5 Winsorised 3 points ## V6 CostImpEx 0 1 Default, winmax = 5 Winsorised 1 points ## V7 Tariff 0 3 Default, winmax = 5 Winsorised 3 points ## V12 StMob 0 2 Default, winmax = 5 Winsorised 2 points ## V15 CultServ 0 2 Default, winmax = 5 Winsorised 2 points ## V21 Flights 0 1 Default, winmax = 5 Winsorised 1 points ## V23 Bord 0 3 Default, winmax = 5 Winsorised 3 points ## V25 Gas 0 3 Default, winmax = 5 Winsorised 3 points ## V35 Forest 0 2 Default, winmax = 5 Winsorised 2 points ## V36 Poverty 0 3 Default, winmax = 5 Winsorised 3 points ## V41 NGOs 0 3 Default, winmax = 5 Winsorised 3 points ## V43 FemLab 1 0 Default, winmax = 5 Winsorised 1 points The results of treat() are a new data set .Data$Treated with the treated data in it, and a details of the data treatment in $Analysis$Treated, including TreatSummary (shown above), which gives a summary of the data treatment specified and actually applied for each indicator. In this case, all indicators were successfully brought within the specified skew and kurtosis limits by Winsorisation, within the specified Winsorisation limit winmax of five points. To see what happens if winmax is exceeded, we will lower the threshold to winmax = 3. Additionally, we can control what type of log transform should be applied when winmax is exceeded, using the deflog argument. We will set deflog = \"CTlog\" which ensures that negative values will not cause errors. # treat at defaults ASEM &lt;- treat(ASEM, dset = &quot;Denominated&quot;, winmax = 3, deflog = &quot;CTlog&quot;) # Check which indicators were treated, and how ASEM$Analysis$Treated$TreatSummary %&gt;% filter(Treatment != &quot;None&quot;) ## IndCode Low High TreatSpec Treatment ## V2 Services 0 0 Default, winmax = 3 CTLog (exceeded winmax) ## V3 FDI 0 2 Default, winmax = 3 Winsorised 2 points ## V5 ForPort 0 3 Default, winmax = 3 Winsorised 3 points ## V6 CostImpEx 0 1 Default, winmax = 3 Winsorised 1 points ## V7 Tariff 0 3 Default, winmax = 3 Winsorised 3 points ## V12 StMob 0 2 Default, winmax = 3 Winsorised 2 points ## V15 CultServ 0 2 Default, winmax = 3 Winsorised 2 points ## V21 Flights 0 1 Default, winmax = 3 Winsorised 1 points ## V23 Bord 0 3 Default, winmax = 3 Winsorised 3 points ## V25 Gas 0 3 Default, winmax = 3 Winsorised 3 points ## V35 Forest 0 2 Default, winmax = 3 Winsorised 2 points ## V36 Poverty 0 3 Default, winmax = 3 Winsorised 3 points ## V41 NGOs 0 3 Default, winmax = 3 Winsorised 3 points ## V43 FemLab 1 0 Default, winmax = 3 Winsorised 1 points # Compare before and after treatment iplotIndDist2(ASEM, dsets = c(&quot;Denominated&quot;, &quot;Treated&quot;), icodes = &quot;Services&quot;, ptype = &quot;Histogram&quot;) Now we can see from the table that the Services indicator has had a log transformation applied, and from the histograms that the skewness has been treated. The range of the indicator is now different, and this introduces negative values, however this will be anyway fixed in the normalisation step. The important thing here is the shape of the distribution. Other options for deflog are log (a standard log transform), GIIlog, which is a scaled log transformation used by the Global Innovation Index, and boxcox, which uses a Box-Cox transformation with the \\(\\lambda\\) parameter set by the boxlam argument to treat(). Optimised \\(\\lambda\\) values are not currently available in COINr, though this may be considered for future versions, along with custom transformation functions. A further point to note is that Winsorisation can work in two ways in COINr, specified by the winchange argument. If set to FALSE, then Winsorisation works in the following way: Perform an initial check of skew and kurtosis (call them \\(s_0\\) and \\(k_0\\)). Only if both exceed thresholds, continue to 2. Otherwise no treatment is applied. If \\(s_0\\) is positive (usually the case), begin by Winsorising the highest value (assign the value of the second-highest point). If \\(s_0\\) is negative, begin by Winsorising the lowest value (assign the value of the second-lowest point). If either skew or kurtosis thesholds are below thresholds, stop, otherwise Go to 2. if \\(s_0\\) was positive, or 3. if \\(s_0\\) was negative. Notably, the direction of the Winsorisation here is always the same, i.e. if \\(s_0\\) was positive, Winsorisation will always be applied to the highest values. The only drawback here is that the sign of the skew can conceivably change during the Winsorisation process. For that reason, if you set winchange = TRUE (which is anyway default), the function will check after every Winsorised point to see whether to Winsorise high values or low values. It might seem unlikely that the skew could change sign from one iteration to the next, and still remain outside the absolute threshold. But this has been observed to happen in some cases. Finally, you can set the skewness and kurtosis threshold values to your favourite values using the t_skew and t_kurt arguments to treat(). 8.3.2 Individual treatment If you want more control over the treatment of individual indicators, this can be done by specifying the optional individual and indiv_only arguments. individual is a data frame which specifies the treatment to apply to specific indicators. It looks like this: # Example of &quot;individual&quot; data frame spec individual = data.frame( IndCode = c(&quot;Bord&quot;, &quot;Services&quot;, &quot;Flights&quot;, &quot;Tariff&quot;), Treat = c(&quot;win&quot;, &quot;log&quot;, &quot;none&quot;, &quot;boxcox&quot;), Winmax = c(10, NA, NA, NA), Thresh = c(&quot;thresh&quot;, NA, NA, NA), Boxlam = c(NA, NA, NA, 2) ) individual ## IndCode Treat Winmax Thresh Boxlam ## 1 Bord win 10 thresh NA ## 2 Services log NA &lt;NA&gt; NA ## 3 Flights none NA &lt;NA&gt; NA ## 4 Tariff boxcox NA &lt;NA&gt; 2 The IndCode column is the list of indicators to apply a specific treatment. The Treat column dictates which treatment to apply: options are either Winsorise (win), or any of the deflog options described previously. The Winmax column gives the maximum number of points to Winsorise for that indicator. This value is only used if the corresponding row of Treat is set to win. The Thresh column specifies whether to use skew/kurtosis thresholds or not. If thresh, it uses the thresholds specified in the thresh argument of treat(). If instead it is set at NA, then it will ignore the thresholds and force the Winsorisation to Winsorise exactly winmax points. Finally, the Boxlam column allows an individual setting of the boxlam parameter if the corresponding entry of the Treat column is set to boxcox. If Individual is specified, all of these columns need to be present regardless of the individual treatments specified. Where entries are not needed, you can simply put NA. An important point is that if the treatment specified in the Treat column is forced upon the specified indicator, for example, if CTlog is specified, this will be applied, without any Winsorisation first. Conversely, if win is specified, this will be applied, without any subsequent log transform even if it fails to correct the distribution. Finally, the indiv_only argument specifies if TRUE, ONLY the indicators listed in individual are treated, and the rest get no treatment. Otherwise, if FALSE then the remaining indicators are subject to the default treatment process. Lets now put this into practice. ASEM &lt;- treat(ASEM, dset = &quot;Denominated&quot;, individual = individual, indiv_only = FALSE) # Check which indicators were treated, and how ASEM$Analysis$Treated$TreatSummary %&gt;% filter(Treatment != &quot;None&quot;) ## IndCode Low High TreatSpec Treatment ## V2 Services 0 0 Forced log Log ## V3 FDI 0 2 Default, winmax = 6 Winsorised 2 points ## V5 ForPort 0 3 Default, winmax = 6 Winsorised 3 points ## V6 CostImpEx 0 1 Default, winmax = 6 Winsorised 1 points ## V7 Tariff 0 0 Forced Box-Cox Box Cox with lambda = 2 ## V12 StMob 0 2 Default, winmax = 6 Winsorised 2 points ## V15 CultServ 0 2 Default, winmax = 6 Winsorised 2 points ## V23 Bord 0 3 Forced Win, winmax = 10 Winsorised 3 points ## V25 Gas 0 3 Default, winmax = 6 Winsorised 3 points ## V35 Forest 0 2 Default, winmax = 6 Winsorised 2 points ## V36 Poverty 0 3 Default, winmax = 6 Winsorised 3 points ## V41 NGOs 0 3 Default, winmax = 6 Winsorised 3 points ## V43 FemLab 1 0 Default, winmax = 6 Winsorised 1 points And this shows how some indicators have had certain treatments applied. Of course here, the individual treatments have been chosen arbitrarily, and the default treatment would have sufficed. In practice, you would apply individual treatment for specific indicators for good reasons, such as excessive skew, or not wanting to treat certain indicators for conceptual reasons. 8.4 Interactive visualisation As we have seen in this chapter, its essential to visualise your indicator distributions and check what treatment has been applied, and what the main differences are. This can be achieved with the plotting functions previously described, but could become cumbersome if you want to check many indicators. COINr has an built-in Shiny app which lets you compare indicator distributions interactively. Shiny is an R package which allows you to build interactive apps, dashboards and gadgets that can run R code. Shiny is a powerful tool for interactively exploring, communicating and presenting results, and can be used to host apps on the web. If you would like to know more about Shiny, I would recommend the book Mastering Shiny. To run the indicator visualisation app, simply run indDash(ASEM). This will open a window which should look like this: Figure 8.1: indDash screenshot The indDash() app is purely illustrative - it does not return anything back to R, but simply displays the distributions of the existing indicators. In the side panel, all data sets that exist in the .$Data folder are accessible, as well as any denominators, if they exist. Any of the indicators from any of these data sets can be plotted against each other. In the context of data treatment, this gives a fast way to check the effects of treating indicators. In the ASEM example, we can set the the data set of indicator 1 to Denominated, and the data set of indicator 2 to Treated. This will compare denominated against treated indicators, which is a sensible comparison since treat() was applied to .Data$Denominated in this case. When a treated data set is selected, a table of treated indicators appears in the side panel - this lists all indicators that had treatment applied to them, and gives the details of the treatments applied. You can click on column headings to sort by values, and this can give an easy way to see which indicators were treated. You can then select an indicator of interest using the dropdown menus. In the screenshot above, we are comparing Gas in the denominated and treated data sets. This gives several comparison plots: histograms of each indicator, violin plots, and a scatter plot of one indicator against the other, as well as overlaid histograms. As with any Plotly plot, any of these can be instantly downloaded using the camera button that appears when you hover over the plot. This might be useful for generating quick reports, for example. Below the violin plots are details of the skew and kurtosis of the indicator, and details of the treatment applied, if any. Finally, when comparing a treated indicator against its un-treated version, it can be helpful to check or un-check the Plot indicators on the same axis range box. Checking this box gives a like-for-like comparison which shows how the data points have moved, particularly useful in the case of Winsorisation. Unchecking it may be more helpful to compare the shapes of the two distributions. While indDash() has specific features for treated data, it is also useful as a general purpose tool for fast indicator visualisation and exploration. It can help, for example, to observe trends and relationships between indicators and denominators, and will work with the aggregated data, i.e. to see how the index may depend on its indicators. "],["normalisation.html", "Chapter 9 Normalisation 9.1 Approaches 9.2 Normalisation in COINr 9.3 Individual normalisation", " Chapter 9 Normalisation Normalisation is the operation of bringing indicators onto comparable scales so that they can be aggregated more fairly. To see why this is necessary, consider aggregating GDP values (billions or trillions of dollars) with percentage tertiary graduates (tens of percent). Average values here would make no sense because one is on a completely different scale to the other. 9.1 Approaches 9.1.1 First: adjust direction Indicators can either be positively or negatively related to the concept that you are trying to measure. For example, in an index of quality of life, median income would probably be a positive indicator. Prevalance of malnourishment would be a negative indicator (higher values should give lower scores in quality of life). Accounting for these differences is considered part of the normalisation step. Indicators loaded into COINr should usually have a Direction column in the IndMeta input to assemble(), which is 1 for positive indicators, and -1 for negative indicators. With this information, normalisation is a two step procedure: Multiply the values of each indicator by their corresponding direction value (either 1 or -1). Apply one of the normalisation methods described below. Its that simple. COINr has this built in, so you dont need to do anything other than specify the directions. 9.1.2 Linear transformations Normalisation is relatively simple but there are still a number of different approaches which have different properties. Perhaps the most straightforward and intuitive option (and therefore probably the most widely used) is called the min-max transformation. This is a simple linear function which rescales the indicator to have a minimum value \\(l\\), a maximum value \\(u\\), and consequently a range \\(u-l\\), and is as follows: \\[ \\tilde{x}_{\\text{min}} = \\frac{ x - x_{\\text{min}} }{ x_{\\text{max}} - x_{\\text{min}} } \\times (u-l) + l\\] where \\(\\tilde{x}\\) is the normalised indicator value. For example, if \\(l=0\\) and \\(u=100\\) this will rescale the indicator to lie exactly onto the interval \\([0, 100]\\). The transformation is linear because it does not change the shape of the distribution, it simply shrinks or expands it, and moves it. A similar transformation is to take z-scores, which instead use the mean and standard deviation as reference points: \\[ \\tilde{x}_{\\text{min}} = \\frac{ x - \\mu_x }{ \\sigma_x } \\times a + b\\] where \\(\\mu_x\\) and \\(\\sigma_x\\) are the mean and standard deviation of \\(x\\). The indicator is first re-scaled to have mean zero and standard deviation of one. Then it is scaled by a factor \\(a\\) and moved by a distance \\(b\\). This is very similar to the min-max transformation in that it can be reduced to multiplying by a factor and adding a constant, which is the definition of a linear transformation. However, the two approaches have different implications. One is that Z scores will generally be less sensitive to outliers, because the standard deviation is less dependent on an outlying value than the minimum or maximum. Following the min-max and z-score, the general linear transformation is defined as: \\[ \\tilde{x} = \\frac{ x - p }{ q } \\times a + b\\] and it is fairly straightforward to see how z-scores and the min-max transformations are special cases of this. 9.1.3 Nonlinear transformations A simple nonlinear transformation is the rank transformation. \\[ \\tilde{x} = \\text{rank}(x)\\] where the ranks should be defined so that the lowest indicator value has a rank of 1, the second lowest a rank of 2, and so on. The rank transformation is attractive because it automatically eliminates any outliers. Therefore there would not usually be any need to treat the data previously. However, it converts detailed indicator scores to simple ranks, which might be too reductionist for some. Its worth pointing out that there are different ways to rank values, because of how ties (units with the same score) are handled. To read about this, just call ?rank in R. Similar approaches to simple ranks include Borda scores, which are simply the ranks described above but minus 1 (so the lowest score is 0 instead of 1), and percentile ranks. 9.1.4 Distances Another approach is to use the distance of each score to some reference value. Possibilities here are the (normalised) distance to the maximum value of the indicator: \\[ \\tilde{x} = 1 - \\frac{\\text{max}(x) - x}{\\text{max}(x) - \\text{min}(x)}\\] the fraction of the maximum value of the indicator: \\[ \\tilde{x} = \\frac{x}{\\text{max}(x)}\\] the distance to a specified unit value in the indicator: \\[ \\tilde{x} = 1 - \\frac{x_u - x}{\\text{max}(x) - \\text{min}(x)}\\] where \\(x_u\\) is the value of unit \\(u\\) in the indicator. This is useful for benchmarking against a reference country, for example. Another possibility is to normalise against indicator targets. This approach is used for example in the EU2020 Index, which used European targets on environment, education and employment issues (among others). \\[ \\tilde{x} = \\text{min} \\left[1, \\ 1 - \\frac{\\text{targ}(x) - x}{\\text{max}(x) - \\text{min}(x)} \\right]\\] where \\(\\text{targ}(x)\\) is the target for the indicator. In this case, any value that exceeds the target is set to 1, i.e. exceeding the target is counted the same as exactly meeting it. There is also an issue of what to use to scale the distance: here the range of the indicator is used, but one could also use \\(\\text{targ}(x) - \\text{min}(x)\\) or perhaps some other range. 9.2 Normalisation in COINr The normalisation function in COINr is imaginatively named normalise(). It has the following main features: A wide range of normalisation methods, including custom func Customisable parameters for normalisation Possibility to specify detailed individual treatment for each indicator The function looks like this: # don&#39;t run this chunk, just for illustration (will throw error if run) normalise &lt;- function(COIN, ntype=&quot;minmax&quot;, npara = NULL, icodes = NULL, dset = &quot;Raw&quot;, directions = NULL, individual = NULL, indiv_only = FALSE, out2 = NULL){ As an input, it takes a COIN or data frame, as usual. It outputs a new dataset to the COIN .$Data$Normalised, or a normalised data frame if out2 = \"df\". Default normalisation (min-max, scaled between 0 and 100) can be achieved by simply calling: library(COINr) # Build ASEM index up to denomination ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ASEM &lt;- denominate(ASEM) # Default normalisation (min max scaled between 0 and 100) on denominated data set ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;) # compare one of the indicators iplotIndDist2(ASEM, dsets = c(&quot;Denominated&quot;, &quot;Normalised&quot;), icodes = &quot;TertGrad&quot;, ptype = &quot;Scatter&quot;) This plot also illustrates the linear nature of the min-max transformation. You can select the normalisation type of the normalise() function using the ntype and accompanying npara arguments, where the latter is an object which specifies any parameters for the type of normalisation selected. ntype = \"minmax\" yields a min-max transformation that scales each indicator onto an interval specified by npara, e.g. if npara$minmax = c(0,10) the indicators will scale to [0, 10]. ntype = \"zscore\" scales the indicator to have a mean and standard deviation specified by npara, e.g. if npara$zscore = c(0,1) the indicator will have mean zero and standard deviation 1. ntype = \"scaled\" is a general linear transformation defined by npara$scaled = c(a,b) which subtracts a and divides by b. ntype = \"rank\" replaces indicator scores with their corresponding ranks, such that the highest scores have the largest rank values. Ties take average rank values. Here npara is not used. ntype = \"borda\" is similar to ntype = \"rank\" but uses Borda scores, which are simply rank values minus one. npara is not used. ntype = \"prank\" gives percentile ranks. npara is not used. ntype = \"fracmax\" scales each indicator by dividing the value by the maximum value of the indicator. npara is not used. ntype = \"dist2ref\" gives the distance to a reference unit, defined by npara. For example, if npara$dist2ref = \"AUT\" then all scores will be normalised as the distance to the score of unit AUT in each indicator (divided by the range of the indicator). This is useful for benchmarking against a set unit, e.g. a reference country. Scores ntype = \"dist2max\" gives the normalised distance to the maximum of each indicator. ntype = \"dist2targ\" gives the normalised distance to indicator targets. Targets are specified, in the IndMeta argument of assemble(), and will be in the COIN at .$Input$IndMeta. Any scores that exceed the target will be capped at a normalised value of one. ntype = \"custom\" allows to pass a custom function to apply to every indicator. For example, npara$custom = function(x) {x/max(x, na.rm = T)} would give the fracmax normalisation method described above. ntype = \"none\" the indicator is not normalised. The normalise() function also allows you to specify the directions of the indicators using the argument directions. If this is not specified, the directions will be taken from the indicator metadata input to assemble(). If they are not present here, all directions will default to positive. 9.3 Individual normalisation Indicators can be normalised using individual specifications in a similar way to the treat() function. This is specified by the following two arguments to normalise(): individual A list of named lists specifiying individual normalisation to apply to specific indicators. Should be structured so that the name of each sublist should be the indicator code. The the list elements are: .$ntype is the type of normalisation to apply (any of the options mentioned above) .$npara is a corresponding object or parameters that are used by ntype indiv_only Logical. As with treat(), if this is set to FALSE (default), then the indicators that are not specified in individual will be normalised according to the ntype and npara arguments specified in the function argument. Otherwise if TRUE, only the indicators in individual will be normalised, and the others will be unaffected. The easiest way to clarify this is with an example. In the following, we will apply min-max, scaled to [0, 1] to all indicators, except for Flights which will be normalised using Borda scores, Renew which will remain un-normalised, and Ship which will be scaled as a distance to the value of Singapore. indiv = list( Flights = list(ntype = &quot;borda&quot;), Renew = list(ntype = &quot;none&quot;), Ship = list(ntype = &quot;dist2ref&quot;, npara = list(dist2ref = &quot;SGP&quot;)) ) # Minmax in [0,1] for all indicators, except custom individual normalisation # for those described above ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;, ntype = &quot;minmax&quot;, npara = list(minmax = c(0,1)), individual = indiv, indiv_only = FALSE) We can visualise the new ranges of the data. plotIndDist(ASEM, dset = &quot;Normalised&quot;, icodes = c(&quot;Flights&quot;, &quot;Renew&quot;, &quot;Ship&quot;, &quot;Goods&quot;), type = &quot;Dot&quot;) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. This example is meant to be illustrative of the functionality of normalise(), rather than being a sensible normalisation strategy, because the indicators are now on very different ranges, from the default minmax range of [0, 1] (Goods in this case), to the unchanged scale of Renew and the Borda rank scale of Flights. Notice also the scaling of Ship, which has values above zero because the reference country, Singapore, is does not have the maximum value of the indicator. In practice, if different normalisation strategies are selected, it is a good idea to keep the indicators on similar ranges, otherwise the effects will be very unequal in the aggregation step. "],["aggregation.html", "Chapter 10 Aggregation 10.1 Weighting 10.2 Approaches 10.3 Aggregation in COINr", " Chapter 10 Aggregation Aggregation is the operation of combining multiple indicators into one value. Many composite indicators have a hierarchical structure, so in practice this often involves multiple aggregations, for example aggregating groups of indicators into aggregate values, then aggregating those values into higher-level aggregates, and so on, until the final index value. Aggregating should almost always be done on normalised data, unless the indicators are already on very similar scales. Otherwise the relative influence of indicators will be very uneven. Of course you dont have to aggregate indicators at all, and you might be content with a scoreboard, or perhaps aggregating into several aggregate values rather than a single index. However, consider that aggregation should not substitute the underlying indicator data, but complement it. Overall, aggregating indicators is a form of information compression - you are trying to combine many indicator values into one, and inevitably information will be lost. As long as this is kept in mind, and indicator data is presented and made available along side aggregate values, then aggregate (index) values can complement indicators and be used as a useful tool for summarising the underlying data, and identifying overall trends and patterns. 10.1 Weighting Many aggregation methods involve some kind of weighting, i.e. coefficients that define the relative weight of the indicators/aggregates in the aggregation. In order to aggregate, weights need to first be specified, but to effectively adjust weights it is necessary to aggregate. This chicken and egg conundrum is best solved by aggregating initially with a trial set of weights, perhaps equal weights, then seeing the effects of the weighting, and making any weight adjustments necessary. For this reason, weighting is dealt with in the following chapter on Weighting. 10.2 Approaches 10.2.1 Means The most straightforward and widely-used approach to aggregation is the weighted arithmetic mean. Denoting the indicators as \\(x_i \\in \\{x_1, x_2, ... , x_d \\}\\), a weighted arithmetic mean is calculated as: \\[ y = \\frac{\\sum_{i=1}^d x_iw_i}{\\sum_{i=1}^d w_i} \\] where the \\(w_i\\) are the weights corresponding to each \\(x_i\\). Here, if the weights are chosen to sum to 1, it will simplify to the weighted sum of the indicators. In any case, the weighted mean is scaled by the sum of the weights, so weights operate relative to each other. Clearly, if the index has more than two levels, then there will be multiple aggregations. For example, there may be three groups of indicators which give three separate aggregate scores. These aggregate scores would then be fed back into the weighted arithmetic mean above to calculate the overall index. The arithmetic mean has perfect compensability, which means that a high score in one indicator will perfectly compensate a low score in another. In a simple example with two indicators scaled between 0 and 10 and equal weighting, a unit with scores (0, 10) would be given the same score as a unit with scores (5, 5)  both have a score of 5. An alternative is the weighted geometric mean, which uses the product of the indicators rather than the sum. \\[ y = \\left( \\prod_{i=1}^d x_i^{w_i} \\right)^{1 / \\sum_{i=1}^d w_i} \\] This is simply the product of each indicator to the power of its weight, all raised the the power of the inverse of the sum of the weights. The geometric mean is less compensatory than the arithmetic mean  low values in one indicator only partially substitute high values in others. For this reason, the geometric mean may sometimes be preferred when indicators represent essentials. An example might be quality of life: a longer life expectancy perhaps should not compensate severe restrictions on personal freedoms. A third type of mean, in fact the third of the so-called Pythagorean means is the weighted harmonic mean. This uses the mean of the reciprocals of the indicators: \\[ y = \\frac{\\sum_{i=1}^d w_i}{\\sum_{i=1}^d w_i/x_i} \\] The harmonic mean is the the least compensatory of the the three means, even less so than the geometric mean. It is often used for taking the mean of rates and ratios. 10.2.2 Other methods The weighted median is also a simple alternative candidate. It is defined by ordering indicator values, then picking the value which has half of the assigned weight above it, and half below it. For ordered indicators \\(x_1, x_2, ..., x_d\\) and corresponding weights \\(w_1, w_2, ..., w_d\\) the weighted median is the indicator value \\(x_m\\) that satisfies: \\[ \\sum_{i=1}^{m-1} w_i \\leq \\frac{1}{2}, \\: \\: \\text{and} \\sum_{i=m+1}^{d} w_i \\leq \\frac{1}{2} \\] The median is known to be robust to outliers, and this may be of interest if the distribution of scores across indicators is skewed. Another somewhat different approach to aggregation is to use the Copeland method. This approach is based pairwise comparisons between units and proceeds as follows. First, an outranking matrix is constructed, which is a square matrix with \\(N\\) columns and \\(N\\) rows, where \\(N\\) is the number of units. The element in the \\(p\\)th row and \\(q\\)th column of the matrix is calculated by summing all the indicator weights where unit \\(p\\) has a higher value in those indicators than unit \\(q\\). Similarly, the cell in the \\(q\\)th row and \\(p\\)th column (which is the cell opposite on the other side of the diagonal), is calculated as the sum of the weights unit where \\(q\\) has a higher value than unit \\(p\\). If the indicator weights sum to one over all indicators, then these two scores will also sum to 1 by definition. The outranking matrix effectively summarises to what extent each unit scores better or worse than all other units, for all unit pairs. The Copeland score for each unit is calculated by taking the sum of the row values in the outranking matrix. This can be seen as an average measure of to what extent that unit performs above other units. Clearly, this can be applied at any level of aggregation and used hierarchically like the other aggregation methods presented here. In some cases, one unit may score higher than the other in all indicators. This is called a dominance pair, and corresponds to any pair scores equal to one (equivalent to any pair scores equal to zero). The percentage of dominance pairs is an indication of robustness. Under dominance, there is no way methodological choices (weighting, normalisation, etc.) can affect the relative standing of the pair in the ranking. One will always be ranked higher than the other. The greater the number of dominance (or robust) pairs in a classification, the less sensitive country ranks will be to methodological assumptions. 10.3 Aggregation in COINr Many will be amazed to learn that the function to aggregate in COINr is called aggregate(). First, lets build the ASEM data set up to the point of normalisation, then aggregate it using the default approaches. library(COINr) # Build ASEM data up to the normalisation step # Assemble data and metadata into a COIN ASEM &lt;- assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta, AggMeta = COINr::ASEMAggMeta) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- # Denominate as specified in metadata ASEM &lt;- denominate(ASEM) # Normalise using default method: min-max scaled between 0 and 100. ASEM &lt;- normalise(ASEM, dset = &quot;Denominated&quot;) # Now aggregate using default method: arithmetic mean, using weights found in the COIN ASEM &lt;- aggregate(ASEM, dset = &quot;Normalised&quot;) # show last few columns of aggregated data set ASEM$Data$Aggregated[(ncol(ASEM$Data$Aggregated)-5): ncol(ASEM$Data$Aggregated)] ## # A tibble: 51 x 6 ## Environ Social SusEcFin Conn Sust Index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.0 67.0 65.7 50.4 67.9 59.1 ## 2 55.9 86.3 53.5 54.8 65.3 60.0 ## 3 56.4 54.0 62.5 42.4 57.6 50.0 ## 4 76.0 54.8 55.5 45.9 62.1 54.0 ## 5 61.9 59.5 31.1 45.5 50.8 48.2 ## 6 53.7 61.1 72.0 48.9 62.3 55.6 ## 7 72.5 79.8 62.7 51.0 71.7 61.4 ## 8 35.7 68.3 70.1 49.5 58.0 53.8 ## 9 52.9 82.1 63.8 48.2 66.3 57.2 ## 10 66.5 60.8 54.2 47.0 60.5 53.7 ## # ... with 41 more rows By default, the aggregation function performs the following steps: Uses the weights that were attached to IndMeta and AggMeta in assemble() Aggregates hierarchically (with default method of weighted arithmetic mean), following the index structure specified in IndMeta and using the data specified in dset Creates a new data set .$Data$Aggregated, which consists of the data in dset, plus extra columns with scores for each aggregation group, at each aggregation level. Like other COINr functions aggregate() has arguments dset which specifies which data set to aggregate, and out2 which specifies whether to output an updated COIN, or a data frame. Unlike other COINr functions however, aggregate() only works on COINs, because it requires a number of inputs, including data, weights, and the index structure. The types of aggregation supported by aggregate() are specified by agtype from the following options: agtype = arith_mean uses the arithmetic mean, and is the default. agtype = geom_mean uses the geometric mean. This only works if all data values are positive, otherwise it will throw an error. agtype = harm_mean uses the harmonic mean. This only works if all data values are non-zero, otherwise it will throw an error. agtype = median uses the weighted median agtype = copeland uses the Copeland method. This may take a few seconds to process depending on the number of units, because it involves pairwise comparisons across units. agtype = custom allows you to pass a custom aggregation function. For the last option here, if agtype = custom then you need to also specify the custom function via the agfunc argument. As an example: # define an aggregation function which is the weighted minimum weightedMin &lt;- function(x,w) min(x*w, na.rm = TRUE) # pass to aggregate() and aggregate ASEM &lt;- aggregate(ASEM, dset = &quot;Normalised&quot;, agtype = &quot;custom&quot;, agfunc = weightedMin, out2 = &quot;df&quot;) Any custom function should have the form functionName &lt;- function(x,w), i.e. it accepts x and w as vectors of indicator values and weights respectively, and returns a scalar aggregated value. Ensure that NAs are handled (e.g. set na.rm = T) if your data has missing values, otherwise NAs will be passed through to higher levels. A number of sophisticated aggregation approaches and linked weighting methods are available in the compind package. These can nicely complement the features in COINr and may be of interest to those looking for more technical approaches to aggregation, such as those based on benefit of the doubt methods. A further argument, agtype_bylevel, allows specification of different normalisation types at different aggregation levels. For example, agtype_bylevel = c(\"arith_mean\", \"geom_mean\", \"median\") would use the arithmetic mean at the indicator level, the geometric mean at the pillar level, and the median at the sub-index level (for the ASEM data structure). Finally, alternative weights can be used for aggregation by specifying the agweights argument. This can either be: NULL, in which case it will use the weights that were attached to IndMeta and AggMeta in GII_assemble (if they exist), or A character string which corresponds to a named list of weights stored in .$Parameters$Weights. You can either add these manually or through rew8r (see chapter on Weighting). E.g. entering agweights = \"Original\" will use the original weights read in on assembly. This is equivalent to agweights = NULL. A list of weights to use in the aggregation, where each element of the list is a numeric vector of weights for a given level. Now that the data has been aggregated, a natural next step is to explore the results. This is dealt with in the chapter on [Results visualisation]. "],["weighting-1.html", "Chapter 11 Weighting 11.1 Approaches to weighting 11.2 Weighting tools in COINr 11.3 Effective weights 11.4 Final points", " Chapter 11 Weighting Strictly speaking, weighting comes before aggregation. However, in order to understand the effects of weights, we need to aggregate the index first. Weighting in composite indicators is a thorny issue, which attracts considerable attention and is often one of the main focuses of critics. Weighting openly expresses a subjective opinion on the relative importance of each indicator relative to the others, and this opinion can easily be contested. While this criticism definitely has some basis, weights can be viewed as a type of model parameter, and any model (e.g. engineering models, climate models, economic models) is full of uncertain parameters. In large models, these parameters are less evident since they are inside rather complex model code. Weights in composite indicators are easy to criticise since the model of a composite indicator is quite simple, usually using simple averages of indicator values. That said, weights do need to be carefully considered, taking into account at least: The relative conceptual importance of indicators: is indicator A more, less, or equally important to indicator B? The statistical implications of the weights How to communicate the weights to end users The last point is important if the index is for advocacy/awareness. Weights may be fine tuned to account for statistical considerations, but the result may make little sense to the public or the media. Overall, weighting is a large topic which cannot be covered in detail here. Nevertheless, this chapter gives some introduction and points to further references. 11.1 Approaches to weighting Broadly, weighting approaches can be divided into those that are based on expert judgment, and those that are based on statistical considerations. Then there are approaches that combine the two. 11.1.1 Equal weighting The most common approach to weighting is simply to make all weights equal to one another. This may seem like an unjustifiable simplification, but in practice, experts often give near-equal weights when asked, and statistical approaches may give weights that are so unequal that it may be hard to justify them to stakeholders. This is why many composite indicators use equal weights. A further consideration is that the results of a composite indicator are often less sensitive to their weights than you might think. That said, a number of other methods exist, some of which are discussed in the following sections. 11.1.2 Budget allocation Fundamentally, composite indicators involve breaking down a complex multidimensional concept into sub-concepts, and possibly breaking those sub-concepts down into sub-sub-concepts, and so on. Generally, you keep doing this until you arrive at the point where the (sub-)^n-concepts are sufficiently specific that they can be directly measured with a small group of indicators. Regardless of the aggregation level that you are at, however, some of the indicators (or aggregates) in the group are likely to be more or less important to the concept you are trying to measure than others. Take the example of university rankings. One might consider the following indicators to be relevant to the notion of excellence for a university: Number of publications in top-tier journals (research output) Teaching reputation, e.g. through a survey (teaching quality) Research funding from private companies (industry links) Percentage of international students (global renown) Mean earning of graduates (future prospects of students) These may all be relevant for the concept, but the relative importance is definitely debatable. For example, if you are a prospective student, you might prioritise the teaching quality and graduate earnings. If you are researcher, you might priorities publications and research funding. And so on. The two points to make here are: Indicators are often not equally important to the concept, and Different people have different opinions on what should be more or less important. Bringing this back to the issue of weights, it is important to make sure that the relative contributions of the indicators to the index scorers reflect the intended importance. This can be achieved by changing the weights attached to each indicator, and the weights of higher aggregation levels. How then do we understand which indicators should be the most important to the concept? One way is to simply use our own opinion. But this does not reflect a diversity of viewpoints. The budget allocation method is a simple way of eliciting weights from a group of experts. The idea is to get a group of experts on the concept, stakeholders and end-users, ideally from different backgrounds. Then each member of the group is given 100 coins which they can spend on the indicators. Members allocate their budget to each indicator, so that they give more of the budget to important indicators, and less to less important ones. This is a simple way of distributing weight to indicators, in a way that is easy for people to understand and to give their opinions. You can take the results, and take the average weight for each indicator. You can even infer a weight-distribution over each indicator which could feed into an uncertainty analysis. 11.1.3 Principle component analysis A very different approach is to use principle component analysis (PCA). PCA is a statistical approach, which rotates your indicator data into a new set of coordinates with special properties. One way of looking at indicator data is as data points in a multidimensional space. If we only have two indicators, then any unit can be plotted as a point on a two-dimensional plot, where the x-axis is the first indicator, and the y-axis is the second. Ind1 &lt;- 35 Ind2 &lt;- 60 plot(Ind1, Ind2, main = &quot;World&#39;s most boring plot&quot;) Each point on this beautiful base-R plot represents a unit (here we only have one). If we have three indicators, then we will have three axes (i.e. a 3D plot). If we have more than three, then the point lives in a hyperspace which we cant really visualise. Anyway, the main point here is that indicator data can be plotted as points in a space, where each axis (dimension) is an indicator. What PCA does, is that it rotates the data so that the axes are no longer indicators, but instead are linear combinations of indicators. And this is done so that the first axis is the linear combination of indicators that explains the most variance. If this is not making much sense (and if this is the first time you have encountered PCA then it probably wont), I would recommend to look at one of the many visualisations of PCA on the internet, e.g. this one. PCA is useful for composite indicators, because if you use an arithmetic mean, then you are using a linear combination of indicators. And the first principle component gives the weights that maximise the variance of the units. In other words, if you use PCA weights, you will have the best weights for discriminating between units, and for capturing as much information from the underlying indicators. This sounds perfect, but the downside is that PCA weights do not care about the relative importance of indicators. Typically, PCA assigns the highest weights to indicators with the highest correlations with other indicators, and this can result in a very unbalanced combination of indicators. Still, PCA has a number of nice properties, and has the advantage of being a purely statistical approach. 11.1.4 Weight optimisation If you choose to go for the budget allocation approach, to match weights to the opinions of experts, or indeed your own opinion, there is a catch that is not very obvious. Put simply, weights do not directly translate into importance. To understand why, we must first define what importance means. Actually there is more than one way to look at this, but one possible measure is to use the (nonlinear) correlation between each indicator and the overall index. If the correlation is high, the indicator is well-reflected in the index scores, and vice versa. If we accept this definition of importance, then its important to realise that this correlation is affected not only by the weights attached to each indicator, but also by the correlations between indicators. This means that these correlations must be accounted for in choosing weights that agree with the budgets assigned by the group of experts. In fact, it is possible to reverse-engineer the weights either analytically using a linear solution or numerically using a nonlinear solution. While the former method is far quicker than a nonlinear optimisation, it is only applicable in the case of a single level of aggregation, with an arithmetic mean, and using linear correlation as a measure. Therefore in COINr, the second method is used. 11.2 Weighting tools in COINr As seen in the chapter on Aggregation, weights are used to calculate the aggregated data set, and are one of the inputs to the aggregate() function. These can either be directly specified as a list, or as a character string which points to one of the sets of weights stored under .$Parameters$Weights. There is no limit to the number of sets of weights that can be stored here. So how do we create new sets of weights? 11.2.1 Manual weighting The simplest approach is to make manual adjustments. First, make a copy of the existing weights. library(COINr) # quietly build data set ASEM &lt;- build_ASEM() NewWeights &lt;- ASEM$Parameters$Weights$Original head(NewWeights) ## AgLevel Code Weight ## 1 1 Goods 1 ## 2 1 Services 1 ## 3 1 FDI 1 ## 4 1 PRemit 1 ## 5 1 ForPort 1 ## 6 1 CostImpEx 1 The structure of the weights is a data frame, with each row being either an indicator or aggregation. The AgLevel column points to the aggregation level, and the Weight column is the (relative) weight. We can just directly modify this. Lets say we want to set the weight of the connectivity sub-index to 0.5. NewWeights$Weight[NewWeights$Code == &quot;Conn&quot;] &lt;- 0.5 tail(NewWeights) ## AgLevel Code Weight ## 55 2 Environ 1.0 ## 56 2 Social 1.0 ## 57 2 SusEcFin 1.0 ## 58 3 Conn 0.5 ## 59 3 Sust 1.0 ## 60 4 Index 1.0 Remember that weights are relative, and are re-scaled to sum to 1 during aggregation. To actually use these weights, we can either (a) input them directly in the aggregate() function, or first attach them to the COIN and the call aggregate(), pointing to the new set of weights. The latter option seems more sensible because that way, all our sets of weights are neatly stored. Also, we can access this in the reweighting app as explained in the next section. # put new weights in the COIN ASEM$Parameters$Weights$NewWeights &lt;- NewWeights # Aggregate again to get new results ASEM &lt;- aggregate(ASEM, agweights = &quot;NewWeights&quot;) Now the ASEM COIN contains the updated results using the new weights. 11.2.2 Interactive re-weighting with ReW8R NOTE: at this moment, rew8r is temporarily out of action until I get it updated to the new weight format. Re-weighting manually, as we have just seen, is quite simple. However, depending on your objectives, weighting can be an iterative process along the lines of: Try a set of weights Examine correlations Check results, rankings Adjust weights Return to 1. Doing this at the command line can be time consuming, so COINr includes a re-weighting app called rew8r() which lets you interactively adjust weights and explore the effects of doing so. To run rew8r() you must have already aggregated your data using aggregate(). This is because every time weights are adjusted, rew8r() re-aggregates to find new results, and it needs to know which aggregation method you are using. So, with your pre-aggregated COIN at hand, simply run: ASEM &lt;- rew8r(ASEM) At this point, a window/tab should open in your browser with the rew8r() app, which looks something like this: Figure 11.1: rew8r() screenshot This may look a little over-complicated to begin with, but lets go through it step by step. 11.2.2.1 Correlations First of all, rew8r() is based around correlations. Why are correlations important? Because they give an idea of how each indicator is represented in the final index, and at other aggregation levels (see earlier discussion in this chapter). The rew8r() app allows you to check the correlations of anything against anything else. The two dropdown menus in the top left allow you to select which aggregation level to correlate against which other. In the screenshot above, the indicators (Level 1) are being correlated against the pillars (Level 2), but this can be changed to any of the levels in the index (the first level should always be below the second level otherwise it can cause errors). You can also select which type of correlation to examine, either Pearson (i.e. standard correlation), Spearman rank correlation, or Kendalls tau (an alternative rank correlation). The correlations themselves are shown in two plots - the first is the scatter plot in the upper right part of the window, which plots the correlation of each indicator in Level 1 on the horizontal axis with its parent in Level 2, against its weight (in Level 1) on the vertical axis. We will come back to this shortly. Figure 11.2: Heatmap screenshot (in rew8r) The second plot is a heatmap of the correlation matrix (between Level 1 and Level 2 in this case) which is shown in the lower right corner. This gives the correlation values between all the indicators/groups of each level as colours according to a discrete colour map. This colour map intends to highlight highly correlated indicators (green), as well as negative or low-correlated indicators (red). These thresholds can be adjusted by adjusting the low and high correlation threshold dropdown menus in the side panel (on the left). The aggregation groups are also shown as overlaid rectangles, and correlation values can be turned on and off using the checkbox below the heatmap. The point of this plot is to show at a glance, where there are very high or low correlations, possibly within aggregation groups. For example, indicators that are negatively correlated with their aggregation group can be problematic. Note that correlation heatmaps can also be generated independently of the app using the plotCorr() function - this is described more in the Multivariate analysis section. Highly-correlated indicators are also listed in a table at the lower middle of the window. The Ind-Ind tab flags any indicators that are above the high correlation threshold, and within the same aggregation group (the aggregation level immediately above the indicator level). Note that this is not dependent on the weights since it regards correlations between indicators only. The Cross-level tab instead gives a table which flags any correlations across the two levels selected in side panel, which are above the threshold. 11.2.2.2 Weighting Let us now turn back to the scatter plot. The scatter plot shows the correlation values on the horizontal axis, but also shows the weight of each indicator or aggregate in the first level selected in the dropdown menu. The purpose here is to show at a glance the weighting, and to make adjustments. To adjust a weight, either click on one of the points in the plot (hovering will show the names of the indicators or aggregates), or select the indicator/aggregate in the dropdown menu in the side panel under the Weighting subsection. Then adjust the weight using the slider. Doing this will automatically update all of the plots and tables, so you can see interactively how the correlations change as a result. Figure 11.3: Correlation scatter plot screenshots in rew8r(). Figure 11.4: Correlation scatter plot screenshots in rew8r(). Importantly, the scatter plot only shows correlations between each indicator/aggregate in the first level with its parent in the second selected level. When everything is plotted on one plot, this may not be so clear, so the Separate groups checkbox in the side panel changes the plot to multiple sub-plots, one for each group in the second selected level. This helps to show how correlations are distributed within each group. The results of the index, in terms of ranks and scores of units, can be found by switching to the Results tab in the main window. Other than adjusting individual weights, any set of weights that is stored in .$Parameters$Weights can be selected, and this will automatically show the correlations. The button Equal weights sets weights to be equal at the current level. Finally, if you have adjusted the weights and you wish to save the new set of weights back to the COIN, you can do this in the Weights output subsection of the side panel. The current set of weights is saved by entering a name and clicking the Save button. This saves the set of weights to a list, and when the app is closed (using the Close app button), it will return them to the updated COIN under .$Parameters$Weights, with the specified name. You can save multiple sets of weights in the same session, by making adjustments, and assigning a name, and clicking Save. When you close the app, all the weight sets will be attached to the COIN. If you dont click Close app, any new weight sets created in the session will be lost. 11.2.2.3 Remarks The rew8r() app is something of a work in progress - it is difficult to strike a balance between conveying the relevant information and conveying too much information. In future versions, it may be updated to become more streamlined. 11.2.3 Automatic re-weighting As discussed earlier, weighting can also be performed statistically, by optimising or targeting some statistical criteria of interest. COINr includes a few possibilities in this respect. 11.2.3.1 With PCA The cPCA() function is used as a quick way to perform PCA on COINs. This does two things: first, to output PCA results for any aggregation level; and second, to provide PCA weights corresponding to the loadings of the first principle component. As discussed above, this is the linear combination that results in the most explained variance. The other outputs of cPCA() are left to the Multivariate analysis section. Here, we will focus on weighting. To obtain PCA weights, simply run something like: ASEM &lt;- cPCA(ASEM, dset = &quot;Aggregated&quot;, aglev = 2) This calls the aggregated data set, and performs PCA on aggregation level 2, i.e. the pillars in the case of ASEM. The result is an updated COIN, with a new set of weights called .Parameters$Weights$PCA_AggregatedL2. This is automatically named according to the data set used and the aggregation level. An important point here is that this weighting represents optimal properties if the components (pillars here) are aggregated directly into a single value. In this example, it is not strictly correct because above the pillar level we have another aggregation (sub-index) before reaching the final index. This means that these weights wouldnt be strictly correct, because they dont account for the intermediate level. However, depending on the aggregation structure and weighting (i.e. if it is fairly even) it may give a fairly good approximation. Note: PCA weights have not been fully tested at the time of writing, treat with skepticism. 11.2.3.2 Weight optimisation An alternative approach is to optimise weight iteratively. This is in a way less elegant (and slower) because it uses numerical optimisation, but allows much more flexibility. The weightOpt() function gives several options in this respect. In short, it allows weights to be optimised to agree with either (a) some pre-specified importances (perhaps obtained by the budget allocation method, or perhaps simply equal importance), or (b) to maximise the information conveyed by the composite indicator. Starting with the first option, this is how it can be applied to our example data: ASEM &lt;- weightOpt(ASEM, itarg = &quot;equal&quot;, aglev = 3, out2 = &quot;COIN&quot;) ## iterating... squared difference = 0.000814548399872482 ## iterating... squared difference = 0.00116234608542398 ## iterating... squared difference = 0.00052279688155717 ## iterating... squared difference = 0.000271428850497901 ## iterating... squared difference = 4.19003122660241e-05 ## iterating... squared difference = 1.66726715407502e-06 ## iterating... squared difference = 0.000166401854209433 ## iterating... squared difference = 0.000357199714112592 ## iterating... squared difference = 4.91262049731255e-05 ## iterating... squared difference = 0.000189074238231704 ## iterating... squared difference = 2.15217457991648e-06 ## iterating... squared difference = 4.02429806062667e-05 ## iterating... squared difference = 1.04404840785835e-05 ## iterating... squared difference = 1.00833908145386e-05 ## iterating... squared difference = 2.40420217478906e-06 ## Optimisation successful! This targets the aggregation level 3 (sub-index), and the itarg argument here is set to equal, which means that the objective is to make the correlations between each sub-index and the index equal to one another. More details of the results of this optimisation are found in the .$Analysis$Weights folder of the object. In particular, we can see the final correlations and the optimised weights: ASEM$Analysis$Weights$OptimsedLev3$CorrResults ## Desired Obtained OptWeight ## 1 0.5 0.8971336 0.400 ## 2 0.5 0.8925119 0.625 In this case, the weights are slightly uneven but seem reasonable. The correlations are also almost exactly equal. The full set of weights is found as a new weight set under .$Parameters$Weights. Other possibilities with the weightOpt() function include: Specifying an unequal vector of target importances using the itarg argument. E.g. taking the sub-pillar level, specifying itarg = c(0.5, 1) would optimise the weights so that the first sub-pillar has half the correlation of the second sub-pillar with the index. Optimising the weights at other aggregation levels using the aglev argument Optimising over other correlation types, e.g. rank correlations, using the cortype argument Aiming to maximise overall correlations by setting optype = infomax Its important to point out that only one level can be optimised at a time, and the optimisation is conditional on the weights at other levels. For example, optmising at the sub-index level produces optimal correlations for the sub-indexes, but not for any other level. If we then optimise at lower levels, this will change the correlations of the sub-indexes! So probably the most sensible strategy is to optimise for the last aggregation level (before the index). That said, an advantage of weightOpt() is that since it is numerical, it can be used for any type of aggregation method, any correlation type, and with any index structure. It calls the aggregation method that you last used to aggregate the index, so it rebuilds the index according to your specifications. Other correlation types may be of interest when linear correlation (i.e. Pearson correlation) is not a sufficient measure. This is the case for example, when distributions are highly skewed. Rank correlations are robust to outliers and can handle some nonlinearity (but not non-monotonicity). Fully nonlinear correlation methods also exist but most of the time, relationships between indicators are fairly linear. Nonlinear correlation may be included in a future version of COINr. Setting optype = infomax changes the optimisation criterion, to instead maximise the sum of the correlations, which is equivalent to maximising the information transferred to the index, and is similar to PCA. Like PCA, this can however result in fairly unequal weights, and not infrequently in negative weights. The weightOpt() function uses a numerical optimisation algorithm which does not always succeed in finding a good solution. Optimisation is a large field in its own right, and is more difficult the more variables you optimise over. If weightOpt() fails to find a good solution, try: Decreasing the toler argument - this decreases the acceptable error between the target importance and the importance with the final set of weights. Increasing the maxiter argument - this is the number of iterations allowed to find a solution within the specified tolerance. By default it is set at 500. Even then, the algorithm may not always converge, depending on the problem. 11.3 Effective weights By now, you may know or have realised that the weight of an indicator is not only affected by its own weight, but also the weights of any aggregate groups to which it belongs. If the aggregation method is the arithmetic mean, the effective weight, i.e. the final weight of each indicator, including its parent weights, can be be easily obtained by COINrs effectiveWeight() function: # get effective weight info EffWeights &lt;- effectiveWeight(ASEM) EffWeights$EffectiveWeightsList ## AgLevel Code EffectiveWeight ## 1 1 Goods 0.02000000 ## 2 1 Services 0.02000000 ## 3 1 FDI 0.02000000 ## 4 1 PRemit 0.02000000 ## 5 1 ForPort 0.02000000 ## 6 1 CostImpEx 0.01666667 ## 7 1 Tariff 0.01666667 ## 8 1 TBTs 0.01666667 ## 9 1 TIRcon 0.01666667 ## 10 1 RTAs 0.01666667 ## 11 1 Visa 0.01666667 ## 12 1 StMob 0.01250000 ## 13 1 Research 0.01250000 ## 14 1 Pat 0.01250000 ## 15 1 CultServ 0.01250000 ## 16 1 CultGood 0.01250000 ## 17 1 Tourist 0.01250000 ## 18 1 MigStock 0.01250000 ## 19 1 Lang 0.01250000 ## 20 1 LPI 0.01250000 ## 21 1 Flights 0.01250000 ## 22 1 Ship 0.01250000 ## 23 1 Bord 0.01250000 ## 24 1 Elec 0.01250000 ## 25 1 Gas 0.01250000 ## 26 1 ConSpeed 0.01250000 ## 27 1 Cov4G 0.01250000 ## 28 1 Embs 0.03333333 ## 29 1 IGOs 0.03333333 ## 30 1 UNVote 0.03333333 ## 31 1 Renew 0.03333333 ## 32 1 PrimEner 0.03333333 ## 33 1 CO2 0.03333333 ## 34 1 MatCon 0.03333333 ## 35 1 Forest 0.03333333 ## 36 1 Poverty 0.01851852 ## 37 1 Palma 0.01851852 ## 38 1 TertGrad 0.01851852 ## 39 1 FreePress 0.01851852 ## 40 1 TolMin 0.01851852 ## 41 1 NGOs 0.01851852 ## 42 1 CPI 0.01851852 ## 43 1 FemLab 0.01851852 ## 44 1 WomParl 0.01851852 ## 45 1 PubDebt 0.03333333 ## 46 1 PrivDebt 0.03333333 ## 47 1 GDPGrow 0.03333333 ## 48 1 RDExp 0.03333333 ## 49 1 NEET 0.03333333 ## 50 2 Physical 0.10000000 ## 51 2 ConEcFin 0.10000000 ## 52 2 Political 0.10000000 ## 53 2 Instit 0.10000000 ## 54 2 P2P 0.10000000 ## 55 2 Environ 0.16666667 ## 56 2 Social 0.16666667 ## 57 2 SusEcFin 0.16666667 ## 58 3 Conn 0.50000000 ## 59 3 Sust 0.50000000 ## 60 4 Index 1.00000000 A nice way to visualise this is to call plotframework() (see Initial visualisation and analysis). effectiveWeight() also gives other info on the parents of each indicator/aggregate which may be useful for some purposes (e.g. some types of plots external to COINr). 11.4 Final points Weighting and correlations is a complex topic, which is important to explore and address. On the other hand, weighting and correlations are just one part of a composite indicator, and balancing and optimising weights probably not be pursued at the expense of building a confusing composite indicator that makes no sense to most people (depending on the context and application). "],["visualising-results.html", "Chapter 12 Visualising results 12.1 Tables 12.2 Plots 12.3 Interactive exploration", " Chapter 12 Visualising results The main results of a composite indicator are the aggregated scores and ranks. These can be explored and visualised in different ways, and the way you do this will depend on the context. For example, in building the index, it is useful to check the results at an initial stage, again after methodological adjustments, and so on through the construction process. Short static plots and tables are useful for concisely communicating results in reports or briefs. When the index is complete, it can be presented using a dedicated web platform, which allows users to explore the results in depth. Some interesting examples of this include: The Lowy Asia Power Index The Cultural and Creative Cities Monitor The SDG Index and Dashboards These latter platforms are not usually built in R, but in Javascript. However, COINr does have a simple results viewer that can be used for prototyping. 12.1 Tables To see the aggregated results, one way is to look in .$Data$Aggregated, which contains the aggregated scores, along with any grouping variables that were attached with the original data. However, this is not always convenient because the table can be quite large, and also the index itself will be the last column. You can of course manually select columns, but to make life easier COINr has a dedicated function, resultsTable(). library(COINr) library(magrittr) # build example data set ASEM &lt;- build_ASEM() %&gt;% suppressMessages() # get results table, top 10 resultsTable(ASEM) %&gt;% head(10) ## UnitCode UnitName Index Rank ## 1 CHE Switzerland 68.38 1 ## 2 NLD Netherlands 64.82 2 ## 3 DNK Denmark 64.80 3 ## 4 NOR Norway 64.47 4 ## 5 BEL Belgium 63.54 5 ## 6 SWE Sweden 63.00 6 ## 7 AUT Austria 61.90 7 ## 8 LUX Luxembourg 61.58 8 ## 9 DEU Germany 60.75 9 ## 10 MLT Malta 60.36 10 By default, resultsTable() gives a table of the highest level of aggregation (i.e. typically the index), along with the scores, and the ranks. Scores are rounded to two decimal places, and this can be controlled by the nround argument. To see all aggregate scores, we can change the tab_type argument: resultsTable(ASEM, tab_type = &quot;Aggregates&quot;) %&gt;% reactable::reactable() Here, we have used the reactable package to display the full table interactively. Notice that the aggregation levels are ordered from the highest level downwards, which is a more intuitive way to read the table. Other options for tab_type are Full, which shows all columns in .$Data$Aggregated (but rearranged to make them easier to read), and FullWithDenoms, which also attaches the denominators. These latter two are probably mostly useful for further data processing, or passing the results on to someone else. Additionally to the resultsTable() function, the iplotTable() function also plots tables, but interactively (and indeed using the reactable package underneath). Like many other COINr functions it takes the isel and aglev arguments to select subsets of the indicator data. Moreover, it colours table cells, similar to conditional formatting in Excel. As an example, we can request to see all the pillar scores inside the Connectivity sub-index. The function sorts the units by the first column by default. This function is useful for generating quick tables inside HTML reports. iplotTable(ASEM, dset = &quot;Aggregated&quot;, isel = &quot;Conn&quot;, aglev = 2) Recall that since the output of this function is a reactable table, we can also edit it further by using reactable functions. 12.2 Plots Tables are useful but can be a bit dry, so you can spice this up using the bar chart and map options already mentioned in Initial visualisation and analysis. iplotBar(ASEM, dset = &quot;Aggregated&quot;, isel = &quot;Index&quot;, usel = &quot;GBR&quot;) And heres a map: iplotMap(ASEM, dset = &quot;Aggregated&quot;, isel = &quot;Index&quot;) The map and bar chart options here are very useful for quickly presenting results. However, if you want to make a really beautiful map, you should consider one of the many mapping packages in R, such as leaflet. Apart from plotting individual indicators, we can inspect and compare unit scores using a radar chart. The function iplotRadar() allows the scores of one or more units, in a set of indicators, to be plotted on the same chart. It follows a similar syntax to iplotTable() and others, since it calls groups of indicators, so the isel and aglev arguments should be specified. Additionally, it requires a set of units to plot. iplotRadar(ASEM, dset = &quot;Aggregated&quot;, usel = c(&quot;CHN&quot;, &quot;DEU&quot;), isel = &quot;Conn&quot;, aglev = 2) Radar charts, which look exciting to newbies, but have are often grumbled about by data-vis veterans, should be used in the right context. In the first place, they are mainly good for comparisons between units - if you simply want to see the scores of one unit, a bar chart may be a better choice because it is easier to see the scores and to compare scores between one indicator and another. Second, they are useful for a smallish number of indicators. A radar chart with two or three indicators looks silly, and with ten indicators or more is hard to read. The chart above, with five indicators, is a clear comparison between two countries (in my opinion), showing that one country scores higher in all dimensions of connectivity than another, and the relative differences. The iplotRadar() function is powered by Plotly, therefore it can be edited using Plotly commands. It is an interactive graphic, so you can add and remove units by clicking on the legend. 12.3 Interactive exploration COINr also offers a Shiny app called resultsDash() for quickly and interactively exploring the results. It features the plots described above, but in an interactive app format. The point of this is to give a tool which can be used to explore the results during development, and to demonstrate the results of the index in validation sessions with experts and stakeholders. It is not meant to replace a dedicated interactive platform. It could however be a starting point for a Shiny-based web platform. To call the app, simply run: resultsDash(ASEM) This will open a browser window or tab with the results dashboard app. The app allows you to select any of the data sets present in .$Data, and to plot any of the indicators inside either on a bar chart or map (see the toggle map/bar box in the bottom right corner). Units (i.e. countries here) can be compared by clicking on the bar chart or the map to select one or more countries - this plots them on the radar chart in the bottom left. The indicator groups plotted in the radar chart are selected by the Aggregation level and Aggregation group dropdowns. Finally, the Table tab switches to a table view, which uses the iplotTable() function mentioned above. Figure 12.1: resultsDash screenshot "],["adjustments-and-comparisons.html", "Chapter 13 Adjustments and comparisons 13.1 Regeneration 13.2 Adjustments 13.3 Comparisons", " Chapter 13 Adjustments and comparisons Its fairly common to make adjustments to the index, perhaps in terms of alternative data sets, indicators, methodological decisions, and so on. COINr allows you to (a) make fast adjustments, and (b) to compare alternative versions of the index relatively easily. 13.1 Regeneration One of the key advantages of working within the COINrverse is that (nearly) all the methodology that is applied when building a composite indicator (COIN) is stored automatically in a folder of the COIN called .$Method. To see what this looks like: # load COINr if not loaded library(COINr) # build example COIN ASEM &lt;- build_ASEM() ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean # look in ASEM$Method folder in R Studio... Figure 13.1: Method folder in ASEM example The easiest way to view it is by looking in the viewer of R Studio as in the screenshot above. Essentially, the .$Method folder has one entry for each COINr function that was used to build the COIN, and inside each of these folders are the arguments to the function that were input. Notice that the names inside .$Method$denominate correspond exactly to arguments to denominate(), for example. Every time a COINr construction function is run, the inputs to the function are automatically recorded inside the COIN. Construction functions are any of the following seven: Function Description assemble() Assembles indicator data/metadata into a COIN checkData() Data availability check and unit screening denominate() Denominate (divide) indicators by other indicators impute() Impute missing data using various methods treat() Treat outliers with Winsorisation and transformations normalise() Normalise data using various methods aggregate() Aggregate indicators into hierarchical levels, up to index These are the core functions that are used to build a composite indicator, from assembling from the original data, up to aggregation. One reason to do this is simply to have a record of what you did to arrive at the results. However, this is not the main reason (and in fact, it would be anyway good practice to make your own record by creating a script or markdown doc which records the steps). The real advantage is that results can be automatically regenerated with a handy function called regen(). To regenerate the results, simply run e.g.: ASEM2 &lt;- regen(ASEM) ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean The regen() function reruns all functions that are recorded in the .$Method folder in the order they appear in the folder. In this example, it runs, in order, assemble(), denominate(), impute(), treat(), normalise()and aggregate(). This replicates exactly the results. But what is the point of that? Well, it means that we can make changes to the index, generally by altering parameters in the .$Method folder, and then rerun everything very quickly with a single command. This will be demonstrated in the following section. After that, we will also see how to compare between alternative versions of the index. Before that, there is one extra feature of regen() which is worth mentioning. The regen() function only runs the seven construction functions as listed above. These functions are very flexible and should encompass most needs for constructing a composite indicator. But what happens if you want to build in an extra operation, or operations, that are outside of these seven functions? It is possible to also include custom chunks of code inside the COIN. Custom chunks should be written manually using the quote() function, to a special folder .$Method$Custom. These chunks can be any type of code, but the important thing is to also know when to run the code, i.e. at what point in the construction process. More specifically, custom code chunks are written as a named list. The name of the item in the list specifies when to perform the operation. For example, after_treat means to perform this immediately after the treatment step. Other options are e.g. after_normalise or after_impute  in general it should be after_ followed by a name of one of the names of the construction functions. The corresponding value in the list should be an operation which must be enclosed in the quote() function. Clearly, the list can feature multiple operations at different points in construction. This is slightly complicated, but can be clarified a bit with an example. Lets imagine that after Winsorisation, we would like to reset one of the Winsorised points to its original value. This is currently not possible inside the treat() function so has to be done manually. But we would like this operation to be kept when we regenerate the index with variations in methodology (e.g. trying different aggregation functions, weights, etc.). We create a list with one operation: resetting the point after the treatment step. # Create list. NOTE the use of the quote() function! custlist = list(after_treat = quote(ASEM$Data$Treated$Bord[ASEM$Data$Treated$UnitCode==&quot;BEL&quot;] &lt;- ASEM2$Data$Imputed$Bord[ASEM$Data$Imputed$UnitCode==&quot;BEL&quot;])) # Add the list to the $Method$Custom folder ASEM2$Method$Custom &lt;- custlist Specifically, we have replaced the Winsorised value of Belgium, for the Bord indicator, with its imputed value (i.e. the value it had before it was treated). Now, when we regenerate the COIN using regen(), it will insert this extra line of code immediately after the treatment step. Using custom code may seem a bit confusing but it adds an extra layer of flexibility. It is however intended for small snippets of custom code, rather than large blocks of custom operations. If you are doing a lot of operations outside COINr, it may be better to do this in your own dedicated script or function, rather than trying to encode absolutely everything inside the COIN. 13.2 Adjustments Let us now explore how the COIN can be adjusted and regenerated. This will (hopefully) clarify why regenerating is a useful thing. The general steps for adjustments are: Copy the object Adjust the index methodology or data by editing the .$Method folder and/or the underlying data Regenerate the results Compare alternatives Copying the object is straightforward, and regeneration has been dealt with in the previous section. Comparison is also addressed in the following section. Here we will focus on adjustments and changing methodology. 13.2.1 Adding/removing indicators First of all, lets consider an alternative index where we decide to add or remove one or more indicators. There are different ways that we could consider doing this. A first possibility would be to manually create a new data frame of indicator data and indicator metadata, i.e. the IndData and IndMeta inputs for assemble(). This is fine, but we would have to start the index again from scratch and rebuild it. A better idea is that when we first supply the set of indicators to assemble(), we include all indicators that we might possibly want to include in the index, including e.g. alternative indicators with alternative data sources. Then we build different versions of the index using subsets of these indicators. This allows us to use regen() and therefore to make fast copies of the index. To illustrate, consider again the ASEM example. We can rebuild the ASEM index using a subset of the indicators by using the include or exclude arguments of the assemble() function. Because these are stored in .$Method, we can easily regenerate the new results. # Make a copy ASEM_NoLPIShip &lt;- ASEM # Edit method: exclude two indicators ASEM_NoLPIShip$Method$assemble$exclude &lt;- c(&quot;LPI&quot;, &quot;Ship&quot;) # Regenerate results (suppress any messages) ASEM_NoLPIShip &lt;- regen(ASEM_NoLPIShip, quietly = TRUE) Note that, in the ASEM example, by default, ASEM$Method$assemble doesnt exist because the include or exclude arguments of assemble() are empty, and these are the only arguments to assemble() that are recorded in .$Method$assemble. In summary, we have removed two indicators, then regenerated the results using exactly the same methodology as used before. Importantly, include and exclude operate relative to the original data input to assemble(), i.e. the data found in .$Input$Original. This means that if we now were to make a copy of the version excluding the two indicators above, and exclude another different indicator: # Make a copy ASEM_NoBord &lt;- ASEM_NoLPIShip # Edit method: exclude two indicators ASEM_NoBord$Method$assemble$exclude &lt;- &quot;Bord&quot; # Regenerate results ASEM_NoBord &lt;- regen(ASEM_NoBord, quietly = TRUE) ASEM_NoBord$Parameters$IndCodes ## [1] &quot;Goods&quot; &quot;Services&quot; &quot;FDI&quot; &quot;PRemit&quot; &quot;ForPort&quot; &quot;CostImpEx&quot; ## [7] &quot;Tariff&quot; &quot;TBTs&quot; &quot;TIRcon&quot; &quot;RTAs&quot; &quot;Visa&quot; &quot;StMob&quot; ## [13] &quot;Research&quot; &quot;Pat&quot; &quot;CultServ&quot; &quot;CultGood&quot; &quot;Tourist&quot; &quot;MigStock&quot; ## [19] &quot;Lang&quot; &quot;LPI&quot; &quot;Flights&quot; &quot;Ship&quot; &quot;Elec&quot; &quot;Gas&quot; ## [25] &quot;ConSpeed&quot; &quot;Cov4G&quot; &quot;Embs&quot; &quot;IGOs&quot; &quot;UNVote&quot; &quot;Renew&quot; ## [31] &quot;PrimEner&quot; &quot;CO2&quot; &quot;MatCon&quot; &quot;Forest&quot; &quot;Poverty&quot; &quot;Palma&quot; ## [37] &quot;TertGrad&quot; &quot;FreePress&quot; &quot;TolMin&quot; &quot;NGOs&quot; &quot;CPI&quot; &quot;FemLab&quot; ## [43] &quot;WomParl&quot; &quot;PubDebt&quot; &quot;PrivDebt&quot; &quot;GDPGrow&quot; &quot;RDExp&quot; &quot;NEET&quot; we see that LPI and Ship are once again present. In fact, COINr has an even quicker way to add and remove indicators, which is a short cut function called indChange(). In one command you can add or remove indicators and regenerate the results. Unlike the method above, indChange() also adds and removes relative to the existing index, which may be more convenient in some circumstances. To demonstrate this, we can use the same example as above: # Make a copy ASEM_NoBord2 &lt;- indChange(ASEM_NoLPIShip, drop = &quot;Bord&quot;, regen = TRUE) ## COIN has been regenerated using new specs. ASEM_NoBord2$Parameters$IndCodes ## [1] &quot;Goods&quot; &quot;Services&quot; &quot;FDI&quot; &quot;PRemit&quot; &quot;ForPort&quot; &quot;CostImpEx&quot; ## [7] &quot;Tariff&quot; &quot;TBTs&quot; &quot;TIRcon&quot; &quot;RTAs&quot; &quot;Visa&quot; &quot;StMob&quot; ## [13] &quot;Research&quot; &quot;Pat&quot; &quot;CultServ&quot; &quot;CultGood&quot; &quot;Tourist&quot; &quot;MigStock&quot; ## [19] &quot;Lang&quot; &quot;Flights&quot; &quot;Elec&quot; &quot;Gas&quot; &quot;ConSpeed&quot; &quot;Cov4G&quot; ## [25] &quot;Embs&quot; &quot;IGOs&quot; &quot;UNVote&quot; &quot;Renew&quot; &quot;PrimEner&quot; &quot;CO2&quot; ## [31] &quot;MatCon&quot; &quot;Forest&quot; &quot;Poverty&quot; &quot;Palma&quot; &quot;TertGrad&quot; &quot;FreePress&quot; ## [37] &quot;TolMin&quot; &quot;NGOs&quot; &quot;CPI&quot; &quot;FemLab&quot; &quot;WomParl&quot; &quot;PubDebt&quot; ## [43] &quot;PrivDebt&quot; &quot;GDPGrow&quot; &quot;RDExp&quot; &quot;NEET&quot; And here we see that now, Bord has been excluded as well as LPI and Ship. 13.2.2 Other adjustments We can make any methodological adjustments we want by editing any parameters in .$Method and then running regen(). For example, we can change the imputation method: # Make a copy ASEMAltImpute &lt;- ASEM # Edit .$Method ASEMAltImpute$Method$impute$imtype &lt;- &quot;indgroup_median&quot; # Regenerate ASEMAltImpute &lt;- regen(ASEMAltImpute, quietly = TRUE) We could also change the normalisation method, e.g. to use Borda scores: # Make a copy ASEMAltNorm &lt;- ASEM # Edit .$Method ASEMAltNorm$Method$normalise$ntype &lt;- &quot;borda&quot; # Regenerate ASEMAltNorm &lt;- regen(ASEMAltNorm, quietly = TRUE) and of course this extends to any parameters of any of the construction functions. We can even alter the underlying data directly if we want, e.g. by altering values in .$Input$Original$Data. In short, anything inside the COIN can be edited and then the results regenerated. This allows a fast way to make different alternative indexes and explore the effects of different methodology very quickly. 13.3 Comparisons A logical follow up to making alternative indexes is to try to understand the differences between these indexes. This can of course be done manually. But to make this quicker, COINr includes a comparison app. NOTE: the comparison app and tools need some bug fixing, so for the moment they are out of action. Stay tuned. "],["sensitivity-analysis.html", "Chapter 14 Sensitivity analysis 14.1 About 14.2 Five steps 14.3 Sensitivity in COINr", " Chapter 14 Sensitivity analysis Composite indicators, like any model, have many associated uncertainties. Sensitivity analysis can help to quantify the uncertainty in the scores and rankings of the composite indicator, and to identify which assumptions are driving this uncertainty, and which are less important. 14.1 About Sensitivity analysis is often confused with uncertainty analysis. Uncertainty analysis involves estimating the uncertainty in the ouputs of a system (here, the scores and ranks of the composite indicator), given the uncertainties in the inputs (here, methodological decisions, weights, etc.). The results of an uncertainty include for example confidence intervals over the ranks, median ranks, and so on. Sensitivity analysis is an extra step after uncertainty analysis, and estimates which of the input uncertainties are driving the output uncertainty, and by how much. A rule of thumb, known as the Pareto Principle (or the 80/20 Rule) suggests that often, only a small proportion of the input uncertainties are causing the majority of the output uncertainty. Sensitivity analysis allows us to find which input uncertainties are significant (and therefore perhaps worthy of extra attention), and which are not important. In reality, sensitivity analysis and uncertainty analysis can be performed simultaneously. However in both cases, the main technique is to use Monte Carlo methods. This essentially involves re-calculating the composite indicator many times, each time randomly varying the uncertain variables (assumptions, parameters), in order to estimate the output distributions. At first glance, one might think that sensitivity analysis can be performed by switching one assumption at a time, using the tools outlined in Adjustments and comparisons. However, uncertainties interact with one another, and to properly understand the impact of uncertainties, one must vary uncertain parameters and assumptions simultaneously. Sensitivity analysis and uncertainty analysis are large topics and are in fact research fields in their own right. To better understand them, a good starting point is Global Sensitivity Analysis: The Primer, and a recent summary of sensitivity analysis research can be found here. 14.2 Five steps To perform an uncertainty or sensitivity analysis, one must define several things: The system or model (in this case it is a composite indicator, represented as a COIN) Which assumptions to treat as uncertain The alternative values or distributions assigned to each uncertain assumption Which output or outputs to target (i.e. to calculate confidence intervals for) Methodological specifications for the sensitivity analysis itself, for example the method and the number of replications to run. This should dispel the common idea that one can simply run a sensitivity analysis. In fact, all of these steps require some thought and attention, and the results of the sensitivity analysis will be themselves dependent on these choices. Lets go through them one by one. 14.2.1 Specifying the model First, the system or model. This should be clear: you need to have already built your (nominal) composite indicator in order to check the uncertainties. Usually this would involve calculating the results up to and including the aggregated index. The choices in this model should represent your best choices for each methodological step, for example your preferred aggregration method, preferred set of weights, and so on. 14.2.2 Which assumptions to vary Specifying which assumptions to vary is more complicated. It is impossible to fully quantify the uncertainty in a composite indicator (or any model, for that matter) because there are simply so many sources of uncertainty, ranging from the input data, the choice of indicators, the structure of the index, and all the methodological steps along the way (imputation, treatment, normalisation, etc.). A reasonable approach is to identify specific assumptions and parameters that could have plausible alternatives, and can be practically varied. The construction of composite indicators in COINr (deliberately) lends itself well to uncertainty analysis, because as we have seen in the Adjustments and comparisons chapter, all the methodological choices used to build the composite indicator are recorded inside the COIN, and changes can be made by simply altering these parameters and calling the regen() function. Sensitivity and uncertainty analysis is simply an extension of this concept - where we create a large number of alternative COINs, each with methodological variations following the distributions assigned to each uncertain assumption. The upshot here is that (at least in theory) any parameter from any of the construction functions (see again the table in Adjustments and comparisons) can be varied as part of a sensitivity analysis. This includes, for example: Inclusion and exclusion of indicators Data availability thresholds for screening units Alternative choices of denominators Alternative imputation methods Alternative data treatment, Winsorisation thresholds, skew and kurtosis values, transformations Alternative normalisation methods and parameters Alternative aggregration methods and weight sets On top of this, it is possible to randomly perturb weights at each level by a specified noise factor. This is explained in more detail later in this chapter. The reason that these can be varied in theory is because there may arise conflicts between methodological choices. For example, if a normalisation method results in negative values, we cannot use a default geometric mean aggregation method. For this reason, it is recommended to focus on key uncertainties and start modestly with a sensitivity analysis, working up to a more complex version if required. 14.2.3 Alternative values Having selected some key uncertain assumptions, we must assign plausible alternative values. For example, lets say that an uncertain assumption is the normalisation method. By default, we have used min-max, but perhaps other methods could be reasonable alternatives. The question is then, which alternatives to test? The answer here should not be to blindly apply all possible alternatives available, but rather to select some alternatives that represent plausible alternatives, ruling out any that do not fit the requirements of the index. For example, using rank normalisation is a robust method that neatly deals with outliers but by doing so, also ignores whether a unit is an exceptional performer. This may be good or bad depending on what you want to capture. If it is good then it could be considered as an alternative. If it does not fit the objectives, it is not a plausible alternative, so should not be included in the sensitivity analysis. Finding plausible alternatives is not necessarily an easy task, and we must recall that in any case we will end up with a lower bound on the uncertainty, since we cannot fully test all uncertainties, as discussed previously (we recall that this is the same in any model, not just in composite indicators). Yet, we can still do a lot better than no uncertainty analysis at all. In the end, for each uncertain parameter, we should end up with a list of alternative plausible values. Note that at the moment, COINr assumes equal probability for all alternatives, i.e. uniform distributions. This may be extended to other distributions in future releases. 14.2.4 Selecting the output Composite indicators have multidimensional outputs - one value for each unit and for each aggregate or normalised indicator. Typically, the most interesting outputs to look at in the context of a sensitivity analysis are the final index values, and possibly some of the underlying aggregate scores (sub-indexes, pillars, etc.). COINr allows us to select which inputs to target, and this is explained more below. 14.2.5 SA methodology Finally, we have to specify what kind of analysis to perform, and how many model runs. Currently, COINr offers either an uncertainty analysis (resulting in distributions over ranks), or a sensitivity analysis (additionally showing which input uncertainties cause the most output uncertainty). As mentioned, sensitivity analysis methodology is a rabbit hole in itself, and interested readers could refer to the references at the beginning of this chapter to find out more. Sensitivity analysis usually requires more model runs (replications of the composite indicator). Still, composite indicators are fairly cheap to evaluate, depending on the number of indicators and the complexity of the construction. Regardless of whether you run an uncertainty or sensitivity analysis, more model runs is always better because it increases the accuracy of the estimations of the distributions over ranks. If you are prepared to wait some minutes or possibly hour(s), normally this is enough to perform a fairly solid sensitivity analysis. 14.3 Sensitivity in COINr To run a sensitivity analysis in COINr, the function of interest is sensitivity(). We must follow the steps outlined above, so first, you should have a nominal composite indicator constructed up to and including the aggregation step. Here, we use our old friend the ASEM index. library(COINr) # build ASEM index ASEM &lt;- build_ASEM() ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean 14.3.1 General specifications Next, we have to decide which parameters to treat as uncertain. Here, we will consider three things: The imputation method The normalisation method The weights These are chosen as a limited set to keep things relatively simple, and because imputation is always a significant uncertainty (we are basically guessing data points). We will also consider a couple of alternative normalisation methods. Finally, we will randomly perturb the weights. For the distributions (i.e. the plausible alternatives), we will consider the following: Imputation using either indicator group mean, indicator mean, or no imputation Normalisation using either min-max, rank, or distance to maximum Perturb pillar and sub index weights by +/-25% Finally, we will consider the index as the target. Lets enter these specifications. The most complex part is specifying the SA_specs argument of the sensitivity() function, which specifies which parameters to vary and which alternative values to use. To explain how this works, the easiest way is to construct the specifications for our example. # define noise to be applied to weights nspecs &lt;- data.frame(AgLevel = c(2,3), NoiseFactor = c(0.25,0.25)) # create list specifying assumptions to vary and alternatives SAspecs &lt;- list( impute = list(imtype = c(&quot;indgroup_mean&quot;, &quot;ind_mean&quot;, &quot;none&quot;)), normalise = list(ntype = c(&quot;minmax&quot;, &quot;rank&quot;, &quot;dist2max&quot;)), weights = list(NoiseSpecs = nspecs, Nominal = &quot;Original&quot;) ) Leaving aside the nspecs line for now (but see below for details), we focus on the list SAspecs, which will be used as the SA_specs argument of sensitivity(). The names of the first level of this list should be any of the seven construction functions. Each named element of the list is itself a list, which specifies the parameters of that function to vary, and the alternative values. In summary, the general format is: &lt;function_name&gt; = list(&lt;parameter name&gt; = &lt;vector or list of alternatives&gt;) There is no restriction on the number of parameters that can be varied, or the number of alternatives. You can have multiple parameters from the same function, for example. 14.3.2 Varying weights The exception to the above format is regarding weights. Weights are input as an argument of the aggregate() function, but since this is a single argument, it only behaves as a single parameter. That means that we could test alternative weight sets as part of the sensitivity analysis: # example entry in SA_specs list of sensitivity() [not used here] aggregate = list(agweights = c(&quot;Weights1&quot;, &quot;Weights2&quot;, &quot;Weights3&quot;)) where Weights1 and friends are alternative sets of weights that we have stored in .$Parameters$Weights. This is a useful test but only gives a limited picture of uncertainty because we test between a small set of alternatives. It may be the best approach if we have two or three fairly clear alternatives for weighting (note: this can also be a way to exclude indicators or even entire aggregation groups, by setting certain weights to zero). If the uncertainty is more general, i.e. we have elicited weights but we feel that there is a reasonable degree of uncertainty surrounding all weight values (which usually there is), COINr also includes the option to apply random noise to the weights. With this approach, for each replication of the composite indicator, a random value is added to each weight, of the form: \\[ w&#39;_i = w_i + \\epsilon_i, \\; \\; \\epsilon_i \\sim U[-\\phi w_i, \\phi w_i] \\] where \\(w_i\\) is a weight, \\(\\epsilon_i\\) is the added noise, and \\(\\phi\\) is a noise factor. This means that if we set \\(\\phi = 0.25\\), for example, it would let \\(w_i\\) vary between +/-25% of its nominal value, following a uniform distribution. The noise factor can be different from one aggregation level to another in COINr. That means we can choose which aggregation level weights to apply noise to, and how much for each level. To specify this in the sensitivity() function, we add it as a special weights entry in the SAspecs list described above. It is special in the sense that this is the only entry that is allowed that is not a construction function name. The weights entry was already defined above, but lets look at it again here: # define noise to be applied to weights nspecs &lt;- data.frame(AgLevel = c(2,3), NoiseFactor = c(0.25,0.25)) nspecs # create list specifying assumptions to vary and alternatives weights = list(NoiseSpecs = nspecs, Nominal = &quot;Original&quot;) The weights list has two entries. The first is a data frame where each row is the specifications for noise to apply to the weights of an aggregation level. It has two columns: AgLev which is the aggregation level, and NoiseFactor, which is the noise factor described above. In the example here, we have specified that at aggregation levels 2 and 3 (pillars and sub-indexes), there should be noise factors of 0.25 (+/-25% of nominal weight values). Indicator weights remain always at nominal values since they are not specified in this table. The second entry, Nominal, is the name of the weight set to use as the nominal values (i.e. the baseline to apply the noise to). This should correspond to a weight set present in .$Parameters$Weights. Here, we have set it to Original, which is the original weights that were input to assemble(). 14.3.3 Running an uncertainty analysis To actually run the analysis, we call the sensitivity() function, using the specifications in the SAspecs list, and setting v_targ = \"Index\", meaning that the index will be the target of the sensitivity analysis. Further, we specify that there should be 100 replications (this number is kept fairly low to limit the time taken to compile this book), and that the type of analysis should be an uncertainty analysis. # run uncertainty analysis SAresults &lt;- sensitivity(ASEM, v_targ = &quot;Index&quot;, SA_specs = SAspecs, N = 100, SA_type = &quot;UA&quot;) ## Iteration 1 of 100 ... 1% complete ## Iteration 2 of 100 ... 2% complete ## Iteration 3 of 100 ... 3% complete ## Iteration 4 of 100 ... 4% complete ## Iteration 5 of 100 ... 5% complete ## Iteration 6 of 100 ... 6% complete ## Iteration 7 of 100 ... 7% complete ## Iteration 8 of 100 ... 8% complete ## Iteration 9 of 100 ... 9% complete ## Iteration 10 of 100 ... 10% complete ## Iteration 11 of 100 ... 11% complete ## Iteration 12 of 100 ... 12% complete ## Iteration 13 of 100 ... 13% complete ## Iteration 14 of 100 ... 14% complete ## Iteration 15 of 100 ... 15% complete ## Iteration 16 of 100 ... 16% complete ## Iteration 17 of 100 ... 17% complete ## Iteration 18 of 100 ... 18% complete ## Iteration 19 of 100 ... 19% complete ## Iteration 20 of 100 ... 20% complete ## Iteration 21 of 100 ... 21% complete ## Iteration 22 of 100 ... 22% complete ## Iteration 23 of 100 ... 23% complete ## Iteration 24 of 100 ... 24% complete ## Iteration 25 of 100 ... 25% complete ## Iteration 26 of 100 ... 26% complete ## Iteration 27 of 100 ... 27% complete ## Iteration 28 of 100 ... 28% complete ## Iteration 29 of 100 ... 29% complete ## Iteration 30 of 100 ... 30% complete ## Iteration 31 of 100 ... 31% complete ## Iteration 32 of 100 ... 32% complete ## Iteration 33 of 100 ... 33% complete ## Iteration 34 of 100 ... 34% complete ## Iteration 35 of 100 ... 35% complete ## Iteration 36 of 100 ... 36% complete ## Iteration 37 of 100 ... 37% complete ## Iteration 38 of 100 ... 38% complete ## Iteration 39 of 100 ... 39% complete ## Iteration 40 of 100 ... 40% complete ## Iteration 41 of 100 ... 41% complete ## Iteration 42 of 100 ... 42% complete ## Iteration 43 of 100 ... 43% complete ## Iteration 44 of 100 ... 44% complete ## Iteration 45 of 100 ... 45% complete ## Iteration 46 of 100 ... 46% complete ## Iteration 47 of 100 ... 47% complete ## Iteration 48 of 100 ... 48% complete ## Iteration 49 of 100 ... 49% complete ## Iteration 50 of 100 ... 50% complete ## Iteration 51 of 100 ... 51% complete ## Iteration 52 of 100 ... 52% complete ## Iteration 53 of 100 ... 53% complete ## Iteration 54 of 100 ... 54% complete ## Iteration 55 of 100 ... 55% complete ## Iteration 56 of 100 ... 56% complete ## Iteration 57 of 100 ... 57% complete ## Iteration 58 of 100 ... 58% complete ## Iteration 59 of 100 ... 59% complete ## Iteration 60 of 100 ... 60% complete ## Iteration 61 of 100 ... 61% complete ## Iteration 62 of 100 ... 62% complete ## Iteration 63 of 100 ... 63% complete ## Iteration 64 of 100 ... 64% complete ## Iteration 65 of 100 ... 65% complete ## Iteration 66 of 100 ... 66% complete ## Iteration 67 of 100 ... 67% complete ## Iteration 68 of 100 ... 68% complete ## Iteration 69 of 100 ... 69% complete ## Iteration 70 of 100 ... 70% complete ## Iteration 71 of 100 ... 71% complete ## Iteration 72 of 100 ... 72% complete ## Iteration 73 of 100 ... 73% complete ## Iteration 74 of 100 ... 74% complete ## Iteration 75 of 100 ... 75% complete ## Iteration 76 of 100 ... 76% complete ## Iteration 77 of 100 ... 77% complete ## Iteration 78 of 100 ... 78% complete ## Iteration 79 of 100 ... 79% complete ## Iteration 80 of 100 ... 80% complete ## Iteration 81 of 100 ... 81% complete ## Iteration 82 of 100 ... 82% complete ## Iteration 83 of 100 ... 83% complete ## Iteration 84 of 100 ... 84% complete ## Iteration 85 of 100 ... 85% complete ## Iteration 86 of 100 ... 86% complete ## Iteration 87 of 100 ... 87% complete ## Iteration 88 of 100 ... 88% complete ## Iteration 89 of 100 ... 89% complete ## Iteration 90 of 100 ... 90% complete ## Iteration 91 of 100 ... 91% complete ## Iteration 92 of 100 ... 92% complete ## Iteration 93 of 100 ... 93% complete ## Iteration 94 of 100 ... 94% complete ## Iteration 95 of 100 ... 95% complete ## Iteration 96 of 100 ... 96% complete ## Iteration 97 of 100 ... 97% complete ## Iteration 98 of 100 ... 98% complete ## Iteration 99 of 100 ... 99% complete ## Iteration 100 of 100 ... 100% complete ## Time elapsed = 17.98s, average 0.18s/rep. Running a sensitivity/uncertainty analysis can be time consuming and depends on the complexity of the model and the number of runs specified. This particular analysis took less than a minute for me, but could take more or less time depending on the speed of your computer. The output of the analysis is a list with several entries: # see summary of analysis summary(SAresults) ## Length Class Mode ## Scores 101 data.frame list ## Parameters 2 data.frame list ## Ranks 101 data.frame list ## RankStats 6 data.frame list ## Nominal 3 data.frame list ## t_elapse 1 -none- numeric ## t_average 1 -none- numeric ## ParaNames 3 -none- character We will go into more detail in a minute, but first we can plot the results of the uncertainty analysis using a dedicated function plotSARanks(): plotSARanks(SAresults) This plot orders the units by their nominal ranks, and plots the median rank across all the replications of the uncertainty analysis, as well as the 5th and 95th percentile rank values. The ranks are the focus of an uncertainty analysis because scores can change drastically depending on the method. Ranks are the only comparable metric across different composite indicator methodologies2. Let us now look at the elements in SAresults. The data frames SAresults$Scores and SAresults$Ranks give the scores and ranks respectively for each replication of the composite indicator. The SAresults$Parameters data frame gives the uncertain parameter values used for each iteration: head(SAresults$Parameters) ## imtype ntype ## 1 ind_mean dist2max ## 2 none minmax ## 3 ind_mean minmax ## 4 ind_mean dist2max ## 5 none rank ## 6 indgroup_mean minmax Here, each column is a parameter that was varied in the analysis. Note that this does not include the weights used for each iteration since these are themselves data frames. The SAresults$RankStats table gives a summary of the main statistics of the ranks of each unit, including mean, median, and percentiles: SAresults$RankStats ## UnitCode Nominal Mean Median Q5 Q95 ## 1 AUT 7 6.94 7.0 5.00 8.00 ## 2 BEL 5 5.44 5.0 3.00 9.00 ## 3 BGR 30 29.50 29.0 28.00 31.00 ## 4 HRV 18 20.60 20.0 17.00 25.00 ## 5 CYP 29 30.19 30.0 28.00 33.00 ## 6 CZE 17 17.11 17.0 16.00 19.00 ## 7 DNK 3 2.72 2.0 2.00 5.00 ## 8 EST 22 19.31 19.0 12.95 26.00 ## 9 FIN 13 13.48 14.0 11.00 16.05 ## 10 FRA 21 20.61 20.5 18.00 24.00 ## 11 DEU 9 8.76 8.0 7.00 11.00 ## 12 GRC 32 32.03 32.0 30.00 34.00 ## 13 HUN 20 20.81 21.0 18.00 24.00 ## 14 IRL 12 12.30 12.0 10.00 16.00 ## 15 ITA 28 27.84 28.0 26.95 29.00 ## 16 LVA 23 22.43 23.0 20.00 25.00 ## 17 LTU 16 14.81 16.0 11.00 17.00 ## 18 LUX 8 8.87 9.0 6.00 12.00 ## 19 MLT 10 13.44 12.0 9.00 19.00 ## 20 NLD 2 3.07 3.0 2.00 4.00 ## 21 NOR 4 3.55 3.0 2.00 5.00 ## 22 POL 26 26.08 26.0 24.00 28.00 ## 23 PRT 27 23.24 26.0 15.00 27.00 ## 24 ROU 25 23.93 25.0 18.95 27.00 ## 25 SVK 24 23.53 23.0 22.00 26.00 ## 26 SVN 11 9.96 10.0 8.00 13.00 ## 27 ESP 19 22.58 22.0 18.00 27.00 ## 28 SWE 6 6.03 6.0 5.00 8.00 ## 29 CHE 1 1.00 1.0 1.00 1.00 ## 30 GBR 15 14.08 15.0 11.00 16.00 ## 31 AUS 35 35.80 35.0 34.00 39.05 ## 32 BGD 46 45.55 46.0 40.95 49.00 ## 33 BRN 40 38.84 38.0 35.00 45.05 ## 34 KHM 37 36.51 37.0 34.90 39.00 ## 35 CHN 49 49.28 49.5 46.95 51.00 ## 36 IND 45 45.00 46.0 41.95 48.00 ## 37 IDN 43 45.44 45.0 40.00 49.00 ## 38 JPN 34 33.45 34.0 30.00 35.00 ## 39 KAZ 47 45.49 45.0 42.00 49.00 ## 40 KOR 31 31.06 31.0 29.00 33.05 ## 41 LAO 48 42.39 41.5 35.00 49.00 ## 42 MYS 39 39.77 39.0 36.00 46.00 ## 43 MNG 44 45.61 46.0 41.00 49.00 ## 44 MMR 41 41.85 42.0 38.00 47.00 ## 45 NZL 33 33.04 33.0 31.00 35.05 ## 46 PAK 50 48.39 49.0 43.00 51.00 ## 47 PHL 38 40.81 41.0 38.00 44.00 ## 48 RUS 51 50.64 51.0 50.00 51.00 ## 49 SGP 14 13.69 13.0 9.00 19.00 ## 50 THA 42 41.60 41.0 39.00 44.00 ## 51 VNM 36 37.55 38.0 36.00 40.00 It also includes the nominal ranks for comparison. Finally, SAresults$Nominal gives a summary of nominal ranks and scores. If this information is not enough, the store_results argument to sensitivity() gives options of what information to store for each iteration. For example, store_results = \"results+method\" returns the information described so far, plus the full .$Method list of each replication (i.e. the full specifications used to build the composite indicator). Setting store_results = \"results+COIN\" stores all results and the complete COIN of each replication. Of course, this latter option in particular will take up more memory. 14.3.4 Running a sensitivity analysis Running a sensitivity analysis, i.e. understanding the individual contributions of input uncertainties, is similar to an uncertainty analysis, but there are some extra considerations to keep in mind. The first is that the target of the sensitivity analysis will be different. Whereas when you run an uncertainty analysis, you can calculate confidence intervals for each unit, with a sensitivity analysis a different approach is taken because otherwise, you would have a set of sensitivity indices for each single unit, and this would be hard to interpret. Instead, COINr calculates sensitivity with respect to the average absolute rank change, between nominal and perturbed values. The second consideration is that the number of replications required to a sensitivity analysis is quite a bit higher than the number required for an uncertainty analysis. In fact, when you specify N in the sensitivity() function, and set to SAtype = \"SA\", the actual number of replications is \\(N_T = N(d +2)\\), where \\(d\\) is the number of uncertain input parameters/assumptions. To run the sensitivity analysis, the format is very similar to the uncertainty analysis. We simply run sensitivity() with SA_specs in exactly the same format as previously (we can use the same list), and set SA_type = \"SA\". # Not actually run here. If you run this, it will take a few minutes SAresults &lt;- sensitivity(ASEM, v_targ = &quot;Index&quot;, SA_specs = SAspecs, N = 500, SA_type = &quot;SA&quot;, Nboot = 1000) The output of the sensitivity analysis is a list. This is in fact an extended version of the list when SA_type = \"UA\", in that it has the recorded scores for each iteration, ranks for each iteration, as well as confidence intervals for ranks, and all the other outputs associated with SA_type = \"SA\". Additionally it outputs a data frame .$Sensitivity, which gives first and total order sensitivity indices for each input variable. Moreover, if Nboot is specified in sensitivity(), it provides estimated confidence intervals for each sensitivity index using bootstrapping. roundDF(SAresults$Sensitivity) ## Variable Si STi Si_q5 Si_q95 STi_q5 STi_q95 ## 1 imtype 0.00 0.04 -0.04 0.05 0.03 0.04 ## 2 ntype 0.54 0.80 0.30 0.77 0.72 0.88 ## 3 weights 0.23 0.17 0.12 0.34 0.15 0.19 Recall that the target output in the sensitivity analysis is the mean absolute rank change (lets call it the MARC from here). How do we interpret these results? The first order sensitivity index \\(S_i\\) can be interpreted as the uncertainty caused by the effect of the \\(i\\)th uncertain parameter/assumption on its own. The total order sensitivity index is the uncertainty caused by the effect of the \\(i\\)th uncertain parameter/assumption, including its interactions with other inputs. To help understand this, we will use COINrs plotting function for sensitivity analysis, plotSA. # plot bar chart plotSA(SAresults, ptype = &quot;bar&quot;) This plot shows the three uncertainties that we introduced on the x-axis: the imputation type, the normalisation type, and the weights. The y-axis is the sensitivity index, and the total height of each bar is the total effect index \\(S_{Ti}\\), i.e. the uncertainty caused by the variable on its own (the main effect) as well as its interactions. Then each bar is divided into the interaction effects and the main effects. What this shows is that the normalisation type is the most important uncertainty, followed by the weights, and last by the imputation type. In fact, the choice of imputation method (between the ones specified) is effectively insignificant. We can also see that the nomalisation has a significant interaction effect, probably with the imputation method. Whereas the weights dont seem to interact with the other inputs. Another way of looking at this is in a pie chart: # plot bar chart plotSA(SAresults, ptype = &quot;pie&quot;) Here we can see that more than half of the uncertainty is caused by the normalisation method choice alone, while a bit less than a quarter is caused by the weights, and the remainder by interactions. It is likely that the normalisation method is important because one of the choices was rank normalisation, which radically alters the distribution of each indicator. Finally, to see the uncertainty on the estimates of each sensitivity index we can use a box plot (or error bars): # plot bar chart plotSA(SAresults, ptype = &quot;box&quot;) This will not work unless Nboot was specified. Bootstrapping allows confidence intervals to be estimated, and this shows that the estimates of total effect indices (\\(S_{Ti}\\)) are quite reliable. Whereas the confidence intervals are much wider on the first order indices (\\(S_{i}\\)). Still, with the number of runs applied, it is possible to draw robust conclusions. If the confidence intervals were still too wide here, you could consider increasing N - this should lead to narrower intervals, although you might have to wait for a bit. This may give a more general hint that focusing on scores in composite indicators is usually not a good idea, because the scores can be extremely different depending on the methodology, while the ranks are much more stable "],["helper-functions.html", "Chapter 15 Helper functions 15.1 R Interfaces 15.2 Selecting data sets and indicators 15.3 Rounding data frames", " Chapter 15 Helper functions 15.1 R Interfaces 15.1.1 Data import COINr has a couple of useful functions to help import and export data and metadata. You might have heard of the COIN Tool which is an Excel-based tool for building and analysing composite indicators, similar in fact to COINr3. With the coinToolIn() function you can import data directly from the COIN Tool to cross check or extend your analysis in COINr. To demonstrate, we can take the example version of the COIN Tool, which you can download here. Then its as simple as running: library(COINr) # This is the file path and name where the COIN Tool is downloaded to # You could also just put it in your project directory. fname &lt;- &quot;C:/Users/becke/Downloads/COIN_Tool_v1_LITE_exampledata.xlsm&quot; dflist &lt;- COINToolIn(fname) ## Imported 15 indicators and 28 units. The output of this function is a list with the three data frame inputs to assemble(). dflist ## $IndData ## # A tibble: 28 x 17 ## UnitName UnitCode ind.01 ind.02 ind.03 ind.04 ind.05 ind.06 ind.07 ind.08 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Austria AT 13.3 80.4 492. 14.9 68.8 34 3.7 87.6 ## 2 Belgium BE 15.1 71.8 503. 7 59.6 24 5.2 81.2 ## 3 Bulgaria BG 12.3 78.1 440. 2.2 51.3 15 10.6 72 ## 4 Cyprus CY 14 76 438. 6.9 16.7 23 3.6 73.4 ## 5 Czech R~ CZ 13.5 87.6 491. 8.8 73.2 27 3.9 86.7 ## 6 Germany DE 9.7 80.2 508. 8.5 46.3 30 5.6 90.1 ## 7 Denmark DK 9.7 73 504. 27.7 40.6 39 3.8 83.9 ## 8 Estonia EE 8.6 83.3 524. 15.7 35.7 37 4.1 77.1 ## 9 Greece EL 11.8 70 458. 4 31.5 30 3.9 49.2 ## 10 Spain ES 14.9 57.4 491. 9.4 34.8 33 11.4 68 ## # ... with 18 more rows, and 7 more variables: ind.09 &lt;dbl&gt;, ind.10 &lt;dbl&gt;, ## # ind.11 &lt;dbl&gt;, ind.12 &lt;dbl&gt;, ind.13 &lt;dbl&gt;, ind.14 &lt;dbl&gt;, ind.15 &lt;dbl&gt; ## ## $IndMeta ## # A tibble: 15 x 9 ## IndCode IndName GPupper GPlower Direction IndWeight Agg1 Agg2 Agg3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ind.01 Pre-primary pu~ NA NA -1 0.4 sp.01 p.01 Index ## 2 ind.02 Share of popul~ NA NA 1 0.3 sp.01 p.01 Index ## 3 ind.03 Reading, maths~ NA NA 1 0.3 sp.01 p.01 Index ## 4 ind.04 Recent training NA NA 1 0.3 sp.02 p.01 Index ## 5 ind.05 VET students NA NA 1 0.35 sp.02 p.01 Index ## 6 ind.06 High computer ~ NA NA 1 0.35 sp.02 p.01 Index ## 7 ind.07 Early leavers ~ NA NA -1 0.7 sp.03 p.02 Index ## 8 ind.08 Recent graduat~ NA NA 1 0.3 sp.03 p.02 Index ## 9 ind.09 Activity rate ~ NA NA 1 0.5 sp.04 p.02 Index ## 10 ind.10 Activity rate ~ NA NA 1 0.5 sp.04 p.02 Index ## 11 ind.11 Long-term unem~ NA NA -1 0.4 sp.05 p.03 Index ## 12 ind.12 Underemployed ~ NA NA -1 0.6 sp.05 p.03 Index ## 13 ind.13 Higher educati~ NA NA -1 0.4 sp.06 p.03 Index ## 14 ind.14 ISCED 5-8 prop~ NA NA -1 0.1 sp.06 p.03 Index ## 15 ind.15 Qualification ~ NA NA -1 0.5 sp.06 p.03 Index ## ## $AggMeta ## # A tibble: 10 x 4 ## AgLevel Code Name Weight ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 Index European Skills Index 1 ## 2 3 p.01 Skills Development 0.3 ## 3 3 p.02 Skills Activation 0.3 ## 4 3 p.03 Skills Matching 0.4 ## 5 2 sp.01 Compulsory education 0.5 ## 6 2 sp.02 Training and tertiary education 0.5 ## 7 2 sp.03 Transition to work 0.5 ## 8 2 sp.04 Activity rates 0.5 ## 9 2 sp.05 Unemployment 0.4 ## 10 2 sp.06 Skills mismatch 0.6 Because the COIN Tool uses numeric codes for indicators such as ind.01, you might want slightly more informative codes. The best way to do this is to name the codes yourself, but a quick solution is to set makecodes = TRUE in COINToolIn(). This generates short codes based on the indicator names. It will not yield perfect results, but for a quick analysis it might be sufficient. At least, you could use this and then modify the results by hand. dflist &lt;- COINToolIn(fname, makecodes = TRUE) ## Imported 15 indicators and 28 units. dflist$IndMeta$IndCode ## [1] &quot;Pre-Pupi&quot; &quot;SharPopu&quot; &quot;ReadMath&quot; &quot;ReceTrai&quot; &quot;Stud&quot; ## [6] &quot;HighComp&quot; &quot;EarlLeav&quot; &quot;ReceGrad&quot; &quot;ActiRate&quot; &quot;ActiRate_1&quot; ## [11] &quot;LongUnem&quot; &quot;UndePart&quot; &quot;HighEduc&quot; &quot;Isce5-Pr&quot; &quot;QualMism&quot; While the codes could certainly be improved, its a lot better than uninformative numbers. Finally, we can assemble the output into a COIN and begin the construction. ESI &lt;- assemble(dflist[[1]],dflist[[2]],dflist[[3]],) ## ----------------- ## No denominators detected. ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 15 ## Number of units = 28 ## No Year column detected in input data. Assuming you only have one year of data. ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 6 aggregate groups: sp.01, sp.02, sp.03, sp.04, sp.05, sp.06 ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 3 aggregate groups: p.01, p.02, p.03 ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- 15.1.2 Export to Excel Trigger warning for R purists! Sometimes its easier to look at your data in Excel. There, I said it. R is great for doing all kinds of complicated tasks, but if you just want to look at big tables of numbers and play around with them, maybe make a few quick graphs, then Excel is a great tool. Actually Excel is kind of underrated by many people who are used to programming in R or Python, Matlab or even Stata. It has a lot of clever tools that not many people know about. But more importantly, Excel is a lingua franca between all kinds of professions - you can pass an Excel spreadsheet to almost anyone and they will be able to take a look at it and use the data. Try doing that with an R or Python script. It just boils down to using the right tool for the right job. Anyway, with that aside, lets look at COINrs coin_2excel() function. You put in your COIN, and it will write a spreadsheet. # Build ASEM index ASEM &lt;- build_ASEM() # Get some statistics ASEM &lt;- getStats(ASEM, dset = &quot;Raw&quot;) # write to Excel coin_2excel(ASEM, fname = &quot;ASEMresults.xlsx&quot;) The spreadsheet will contain a number of tabs, including: The indicator data, metadata and aggregation metadata that was input to COINr All data sets in the .$Data folder, e.g. raw, treated, normalised, aggregated, etc. (almost) All data frames found in the .$Analysis folder, i.e. statistics tables, outlier flags, correlation tables. 15.2 Selecting data sets and indicators The getIn() function is widely used by many COINr functions. It is used for selecting specific data sets, and returning subsets of indicators. While some of this can be achieved fairly easily with base R, or dplyr::select(), subsetting in a hierarchical context can be more awkward. Thats where getIn() steps in to help. Although it was made to be used internally, it might also help in other contexts. Note that this can work on COINs or data frames, but is most useful with COINs. Lets take some examples. First, we can get a whole data set. getIn() will retrieve any of the data sets in the .$Data folder, as well as the denominators. # Build data set first, if not already done ASEM &lt;- build_ASEM() ## ----------------- ## Denominators detected - stored in .$Input$Denominators ## ----------------- ## ----------------- ## Indicator codes cross-checked and OK. ## ----------------- ## Number of indicators = 49 ## Number of units = 51 ## Number of reference years of data = 1 ## Years from 2018 to 2018 ## Number of aggregation levels = 3 above indicator level. ## ----------------- ## Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin ## Cross-check between metadata and framework = OK. ## Aggregation level 2 with 2 aggregate groups: Conn, Sust ## Cross-check between metadata and framework = OK. ## Aggregation level 3 with 1 aggregate groups: Index ## Cross-check between metadata and framework = OK. ## ----------------- ## Missing data points detected = 65 ## Missing data points imputed = 65, using method = indgroup_mean # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Raw&quot;) datalist$ind_data_only ## # A tibble: 51 x 49 ## Goods Services FDI PRemit ForPort CostImpEx Tariff TBTs TIRcon RTAs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 278. 108. 5 5.07 809. 0 1.6 1144 1 30 ## 2 598. 216. 5.71 13.4 1574. 0 1.6 1348 1 30 ## 3 42.8 13.0 1.35 1.04 15.5 52 1.6 1140 1 30 ## 4 28.4 17.4 0.387 1.56 16.9 0 1.6 1179 1 30 ## 5 8.77 15.2 1.23 0.477 40.8 100 1.6 1141 1 30 ## 6 274. 43.5 3.88 4.69 108. 0 1.6 1456 1 30 ## 7 147. 114. 9.1 2.19 1021. 0 1.6 1393 1 30 ## 8 28.2 10.2 0.580 0.589 17.4 0 1.6 1153 1 30 ## 9 102. 53.8 6.03 1.51 748. 70 1.6 1215 1 30 ## 10 849. 471. 30.9 30.2 6745. 0 1.6 1385 1 30 ## # ... with 41 more rows, and 39 more variables: Visa &lt;dbl&gt;, StMob &lt;dbl&gt;, ## # Research &lt;dbl&gt;, Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, ## # MigStock &lt;dbl&gt;, Lang &lt;dbl&gt;, LPI &lt;dbl&gt;, Flights &lt;dbl&gt;, Ship &lt;dbl&gt;, ## # Bord &lt;dbl&gt;, Elec &lt;dbl&gt;, Gas &lt;dbl&gt;, ConSpeed &lt;dbl&gt;, Cov4G &lt;dbl&gt;, Embs &lt;dbl&gt;, ## # IGOs &lt;dbl&gt;, UNVote &lt;dbl&gt;, Renew &lt;dbl&gt;, PrimEner &lt;dbl&gt;, CO2 &lt;dbl&gt;, ## # MatCon &lt;dbl&gt;, Forest &lt;dbl&gt;, Poverty &lt;dbl&gt;, Palma &lt;dbl&gt;, TertGrad &lt;dbl&gt;, ## # FreePress &lt;dbl&gt;, TolMin &lt;dbl&gt;, NGOs &lt;dbl&gt;, CPI &lt;dbl&gt;, FemLab &lt;dbl&gt;, ## # WomParl &lt;dbl&gt;, PubDebt &lt;dbl&gt;, PrivDebt &lt;dbl&gt;, GDPGrow &lt;dbl&gt;, RDExp &lt;dbl&gt;, ## # NEET &lt;dbl&gt; The output, here datalist is a list containing the full data set .$ind_data, the data set .$ind_data_only only including numerical (indicator) columns, as well as unit codes, indicator codes and names, and the object type. More usefully, we can get specific indicators: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Raw&quot;, icodes = c(&quot;Flights&quot;, &quot;LPI&quot;)) datalist$ind_data_only ## # A tibble: 51 x 2 ## Flights LPI ## &lt;dbl&gt; &lt;dbl&gt; ## 1 29.0 4.10 ## 2 31.9 4.11 ## 3 9.24 2.81 ## 4 9.25 3.16 ## 5 8.75 3.00 ## 6 15.3 3.67 ## 7 32.8 3.82 ## 8 3.13 3.36 ## 9 18.9 3.92 ## 10 97.6 3.90 ## # ... with 41 more rows More usefully still, we can get groups of indicators based on their groupings. For example, we can ask for indicators that belong to the Political group: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Raw&quot;, icodes = &quot;Political&quot;, aglev = 1) datalist$ind_data_only ## # A tibble: 51 x 3 ## Embs IGOs UNVote ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 88 227 42.6 ## 2 84 248 43.0 ## 3 67 209 43.0 ## 4 62 197 42.7 ## 5 43 172 42.3 ## 6 84 201 42.2 ## 7 77 259 42.8 ## 8 46 194 42.9 ## 9 74 269 42.7 ## 10 100 329 40.4 ## # ... with 41 more rows To do this, you have to specify the aglev argument, which specifies which level to retrieve the indicators from. Before the data set is aggregated, you can anyway only select the indicators, but for the aggregated data set, the situation is more complex. To illustrate, we can call the Connectivity sub-index, first asking for all indicators: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Aggregated&quot;, icodes = &quot;Conn&quot;, aglev = 1) datalist$ind_data_only ## # A tibble: 51 x 30 ## Goods Services FDI PRemit ForPort CostImpEx Tariff TBTs TIRcon RTAs Visa ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 40.7 33.0 6.24 16.8 34.2 100 79.2 34.8 100 64.4 86.8 ## 2 80.1 58.7 5.79 40.9 55.7 100 79.2 23.2 100 64.4 86.8 ## 3 47.1 28.4 15.8 26.9 4.82 91.3 79.2 35.0 100 64.4 85.7 ## 4 29.7 41.6 2.26 43.9 5.45 100 79.2 32.8 100 64.4 84.6 ## 5 21.6 100 43.1 33.6 33.6 83.2 79.2 35.0 100 64.4 85.7 ## 6 88.9 25.5 11.6 33.9 9.12 100 79.2 17.0 100 64.4 85.7 ## 7 24.4 46.0 19.0 7.73 55.1 100 79.2 20.6 100 64.4 87.9 ## 8 75.4 55.4 15.4 35.8 12.3 100 79.2 34.3 100 64.4 85.7 ## 9 20.8 25.9 15.7 6.50 51.9 88.2 79.2 30.7 100 64.4 87.9 ## 10 15.1 21.1 6.04 15.7 45.3 100 79.2 21.0 100 64.4 87.9 ## # ... with 41 more rows, and 19 more variables: StMob &lt;dbl&gt;, Research &lt;dbl&gt;, ## # Pat &lt;dbl&gt;, CultServ &lt;dbl&gt;, CultGood &lt;dbl&gt;, Tourist &lt;dbl&gt;, MigStock &lt;dbl&gt;, ## # Lang &lt;dbl&gt;, LPI &lt;dbl&gt;, Flights &lt;dbl&gt;, Ship &lt;dbl&gt;, Bord &lt;dbl&gt;, Elec &lt;dbl&gt;, ## # Gas &lt;dbl&gt;, ConSpeed &lt;dbl&gt;, Cov4G &lt;dbl&gt;, Embs &lt;dbl&gt;, IGOs &lt;dbl&gt;, ## # UNVote &lt;dbl&gt; Or we can call all the pillars belonging to Connectivity, i.e. the level below: # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Aggregated&quot;, icodes = &quot;Conn&quot;, aglev = 2) datalist$ind_data_only ## # A tibble: 51 x 5 ## ConEcFin Instit P2P Physical Political ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26.2 77.5 54.1 41.1 78.2 ## 2 48.2 75.6 43.3 72.0 80.8 ## 3 24.6 75.9 27.1 28.4 67.5 ## 4 24.6 76.8 39.1 47.6 62.6 ## 5 46.4 74.6 59.8 29.5 48.7 ## 6 33.8 74.4 39.6 41.5 71.0 ## 7 30.4 75.4 51.0 50.5 78.1 ## 8 38.9 77.3 53.5 42.5 55.4 ## 9 24.1 75.1 39.1 44.5 77.9 ## 10 20.6 75.4 28.4 42.4 87.6 ## # ... with 41 more rows Finally, if we want Conn itself, we can just call it directly with no aglev specified. # Get raw data set datalist &lt;- getIn(ASEM, dset = &quot;Aggregated&quot;, icodes = &quot;Conn&quot;) datalist$ind_data_only ## # A tibble: 51 x 1 ## Conn ## &lt;dbl&gt; ## 1 55.4 ## 2 64.0 ## 3 44.7 ## 4 50.1 ## 5 51.8 ## 6 52.1 ## 7 57.1 ## 8 53.5 ## 9 52.1 ## 10 50.9 ## # ... with 41 more rows We can also use getIn() with data frames, and it will behave in more or less the same way, except a data frame has no information about the structure of the index. Here, getIn() returns what it can, and arguments like dset and aglev are ignored. # use the ASEM indicator data frame directly datalist &lt;- getIn(ASEMIndData, icodes = c(&quot;LPI&quot;, &quot;Goods&quot;)) datalist$ind_data_only ## # A tibble: 51 x 2 ## LPI Goods ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4.10 278. ## 2 4.11 598. ## 3 2.81 42.8 ## 4 3.16 28.4 ## 5 3.00 8.77 ## 6 3.67 274. ## 7 3.82 147. ## 8 3.36 28.2 ## 9 3.92 102. ## 10 3.90 849. ## # ... with 41 more rows 15.3 Rounding data frames The roundDF function is a small helper function for rounding data frames that contain a mix of numeric and non-numeric columns. This is very handy for presenting tables generated by COINr in documents. # use the ASEM indicator data frame directly library(magrittr) ASEMIndData %&gt;% roundDF(decimals = 3) %&gt;% reactable::reactable(defaultPageSize = 5, highlight = TRUE, wrap = F) By default, numbers are rounded to two decimal places. Full disclosure, I was also involved in the development of the COIN Tool "],["appendix-r-resources.html", "Chapter 16 Appendix: R Resources 16.1 Introduction to R 16.2 Advanced resources", " Chapter 16 Appendix: R Resources One of the great things about R is the sheer number of freely-available resources that are out there. Not just the software and packages, but also online books and materials to learn everything you need to know about pretty much anything. Here are a list of resources for users who (a) are interested in R and want to get started, and (b) are proficient R users but want to learn more. I am focusing here on resources that have really helped me in the work that I have done. Of course there is far more out there and you only have to look. 16.1 Introduction to R If youre just starting out, these are good places to start. R for Data Science is a modern classic that starts from the beginning and leads you into the world of R, from a data science perspective. It uses the tidyverse approach which is developed by R Guru Hadley Wickham. Even advanced users can probably learn something here. https://r4ds.had.co.nz/index.html Swirl is an R package which lets you learn R at the command line. Also very good for beginners. https://swirlstats.com/ 16.2 Advanced resources 16.2.1 Programming If you really want to sharpen your programming skills in R, Hadley Wickham has another book: this one digs around in the roots of R and teaches you all kinds of tricks and quirks. https://adv-r.hadley.nz/ Want to build your own R package? Hadley come to the rescue, again. https://r-pkgs.org/ 16.2.2 Visualisation Plotly is a big R package which generates interactive graphics using Javascript (there are many others, by the way). This book tells you all you need to know about that. https://plotly-r.com/ Shiny is another R package which lets you build interactive web apps based on R code. Its tricky to get your head around at first, but this book really helps. https://mastering-shiny.org/index.html 16.2.3 Other If youre not using GitHub, ask yourself, why not? GitHub is the best way to collaborate and share code, and you can also host documentation and websites. This book is hosted on GitHub, to take a random example. The Happy Git with R Book gives an easy introduction to hooking up R Studio to work seamlessly with Git and Github. https://happygitwithr.com/ You wouldnt think that R would be a good tool for writing books, but actually it turns out that its a pretty good tool for writing books. The Bookdown package lets you build nifty online books with a simple and neat layout. You can include equations and importantly, R code, outputs and HTML widgets, etc. https://bookdown.org/yihui/bookdown/ Finally, you wouldnt think that R would be a good tool for building website, but actually it turns out its a pretty good tool for building websites. OK, yes, if you want to build something really complicated and/or highly customised, then its not the way forward. But for building fairly simple sites and blogs, personal pages etc (especially if you want to stick in some R code), then the Blogdown package gives a great way to do this. And guess what, you can link it to Github and it automatically updates your website when you push any changes from R Studio. And its all free. An example of this is my very humble website which you can find at http://www.bluefoxdata.eu. Whats that you say, if only there were a book to teach me how to do all this? Well youre in luck - here it is. https://bookdown.org/yihui/blogdown/ "],["acknowledgements.html", "Chapter 17 Acknowledgements", " Chapter 17 Acknowledgements The construction of this package was funded by the European Commissions Joint Research Centre, in particular by the Competence Centre for Composite Indicators and Scoreboards. It was also developed with and inspired by colleagues from the same group. COINr, like all R packages, has been built off of the back of many other packages. Some that COINr uses in particular are: Plotly, for beautiful javascript plots ggplot2, for beautiful static plots reactable, for nice interactive tables shiny, for apps Various elements of the tidyverse, particularly dplyr "]]
