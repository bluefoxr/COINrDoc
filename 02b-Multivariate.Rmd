# Multivariate analysis

Correlations, and other relationships between indicators, can help to understand the structure of the data and to see whether indicators are redundant or are mistakenly encoded.

## Correlations

Correlations are a measure of *statistical dependence* between one variable and another. Often, what is meant by "correlation", is the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), which is a measure of *linear* dependence between two variables.

It's worth spending a moment to consider what this the implication of the "linear" part actually means. Consider the following example:

```{r}
# numbers in [-1, 1]
x1 <- seq(-1, 1, 0.1)
# x2 is a function of x1
x2 <- x1^2
# plot
library(ggplot2)
qplot(x1, x2)
```

Now, clearly there is a very strong relationship between x1 and x2. In fact, x2 is completely dependent on x2 (there are no other variables or noise that control its values). So what happens if we check the correlation?

```{r}
cor(x1, x2, method = "pearson")
```

We end up with a correlation of zero. This is because although there is a very strong *nonlinear* relationship between $x_1$ and $x_2$, the *linear* relationship is zero. This is made even clearer when we fit a linear regression:

```{r}
qplot(x1, x2) +
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE)    # Don't add shaded confidence region
```

Obviously, this is a very contrived example, and it is very unlikely you will see a relationship like this in real indicator data. However the point is that sometimes, linear measures of dependence don't tell the whole story.

That said, relationships between indicators can often turn out to be fairly linear. Exceptions arise when indicators are highly skewed, for example. In these cases, a good alternative is to turn to *rank correlation* measures. Two well-known such measures are the [Spearman rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) and the [Kendall rank correlation](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient). Rank correlations can handle nonlinearity as long as the relationship is monotonic. You could also argue that since the focus of composite indicators is often on ranks, rank correlation also makes sense on a conceptual level.

COINr offers a few ways to check correlations that are more convenient than `stats::cor()`. The first is to call the `getStats()` function that was already mentioned in [Initial visualisation and analysis]. This can be pointed at any data set and indicator subset and will give correlation between all indicators present. For example, taking the raw data of our example data set:

```{r, message=F}
library(COINr)
library(magrittr)
ASEM <- build_ASEM()
statlist <- getStats(ASEM, dset = "Raw", out2 = "list")

# see a bit of correlation matrix
statlist$Correlations[1:5, 1:5]

# see a bit of correlations with denominators
statlist$DenomCorrelations[, 1:5]
```

The type of correlation can be changed by the `cortype` argument, which can be set to "pearson" or "kendall".

The `getStats()` function also summarises some of this information in its output in the `.$StatTable` dataframe:

```{r}
statlist$StatTable[ c("Indicator", "Collinearity", "Neg.Correls", "Denom.correlation")] %>%
  head(10)
```

This flags any indicators that have collinearity with any other indicators, or are significantly negatively correlated, or have correlations with denominators. These flags are activated based on thresholds that are inputs to the `getStats()` function (`t_colin` and `t_denom`).

Correlations can also be plotted using the `plotCorr()` function. This is a flexible plotting function, powered by ggplot2, which is adapted to the composite indicator context. It has a number of options because there are many ways to plot correlations in a composite indicator, including within an aggregation level, between levels, selecting certain groups, focusing on parents, and so on.

To see this, here are a few examples. First, let's plot correlations between raw indicators.

```{r}
plotCorr(ASEM, dset = "Raw", aglevs = 1, showvals = F)
```

Here, we have plotted raw indicators against indicators. A few things to note:

1. By default, if a level is plotted against itself, it only plots correlations within the groups of the next aggregation level above. Use the `grouplev` argument to control the grouping or disable.
2. Insignificant correlations (at 5% level) are excluded by default. Use the `pval` argument to change the significance threshold or to disable.

We can repeat the same plot with some variations. First, grouping by sub-index:

```{r}
plotCorr(ASEM, dset = "Raw", aglevs = 1, showvals = F, grouplev = 3)
```

Second, we can plot the whole correlation matrix, and also include insignificant correlations:

```{r}
plotCorr(ASEM, dset = "Raw", aglevs = 1, showvals = F, grouplev = 0, pval = 0)
```

It may often be more interesting to plot correlations between aggregation levels. Since there is a bit more space, we will also enable the values of the correlations using `showvals = TRUE`.

```{r}
plotCorr(ASEM, dset = "Aggregated", aglevs = c(1,2), showvals = T)
```

Note that again, by default the correlations are grouped with the parent level above. This is disabled by setting `withparent = "none"`. To see correlations with multiple parents at once, we can set `withparent = "family"`

```{r}
plotCorr(ASEM, dset = "Aggregated", aglevs = c(1,2), showvals = T, withparent = "family")
```

It is also possible to switch to a discrete colour map to highlight negative, weak and collinear values.

```{r}
plotCorr(ASEM, dset = "Aggregated", aglevs = c(2,3), showvals = T, withparent = "family",
         flagcolours = TRUE)
```

The thresholds for these colours can be controlled by the `flagthresh` argument. This type of plot helps to see at a glance where there might be a problematic indicator. In the example here, we see that TBTs are negatively correlated with all levels of the index.

Finally, it might be useful to simply have this information as a table rather than a plot. Setting `out2` to "dflong" or "dfwide" outputs a data frame in long or wide format, instead of a figure. This can be helpful for presenting the information in a report or doing custom formatting.

```{r}
plotCorr(ASEM, dset = "Aggregated", aglevs = c(2,3), showvals = T, withparent = "family",
         flagcolours = TRUE, out2 = "dfwide")
```

Arguably it's not so intuitive for a function called `plotCorr()` to output tables, but that's how it is for now.

If you like your correlation heatmaps interactive, COINr has another function called `iplotCorr()`. It functions in a similar way `plotCorr()`, but is designed in particular as a component of the `rew8r()` app and can be used in HTML documents. For a basic correlation map:

```{r}
iplotCorr(ASEM, aglevs = c(2, 3))
```

Unlike `plotCorr()`, indicators are grouped by rectangles around the groups. This might be better for small plots but can be a bit confusing when there are many elements plotted at the same time. The `iplotCorr()` function can also show a continuous colour scale, and values can be removed.

```{r}
iplotCorr(ASEM, aglevs = c(1,2), showvals = F, flagcolours = F)
```

Like `plotCorr()`, threshold values can be controlled for the discrete colour map. The function is powered by *plotly*, which means it can be further altered by using Plotly commands.

An additional tool that can be used in checking correlations is [Cronbach's alpha](https://en.wikipedia.org/wiki/Cronbach%27s_alpha) - this is a measure of internal consistency or "reliability" of the data, based on the strength of correlations between indicators. COINr has a simple in-built function for calculating this, which can be directed at any data set and subset of indicators as with other functions. To see the consistency of the entire raw data set:

```{r}
getCronbach(ASEM, dset = "Raw")
```

Often, it may be more relevant to look at consistency within aggregation groups. For example, the indicators within the "Physical" pillar of the ASEM data set:

```{r}
getCronbach(ASEM, dset = "Raw", icodes = "Physical", aglev = 1)
```

This shows that the consistency is much stronger within a sub-group. This can also apply to the aggregated data:

```{r}
getCronbach(ASEM, dset = "Aggregated", icodes = "Conn", aglev = 2)
```

This shows that the consistency of the sub-pillars within the Connectivity sub-index is quite high.

## PCA

Another facet of multivariate analysis is principle component analysis (PCA). PCA is used in at least two main ways in composite indicators, first to understand the structure of the data and check for latent variables (the subject of this section); and second as a weighting approach (see the chapter on [Weighting]).

PCA often seems to occupy an unusual niche where it's relatively well known, but not always so well-understood. I'm not going to attempt a full explanation of PCA here, because there are many nice explanations available in books and online. However, here is a very rough guide.

The first thing to know is that PCA is based on correlations between indicators, i.e. *linear* relationships (see caveats of this above). PCA tries to find the linear combination of your indicators that explains the most variance. It's a bit easier to understand this visually. Let's consider two variables which are strongly correlated:

```{r}
library(ggplot2)

# random numbers
x1 <- rnorm(50, 10, 1)
# x2 is 0.5*x1 + gaussian noise
x2 <- 0.5*x1 + rnorm(50, 0, 0.2)

#plot
qplot(x1, x2)
```

At the moment, each point/observation lives in a two dimensional space, i.e. it is defined by both its $x_1$ and $x_2$ values. But in a way, this is inefficient because actually the data more or less lives on a straight line.

What PCA does is to rotate the axes (or rotate the data, depending on which way you look at it), so that the first axis aligns itself with the "shape" or "spread" of the data, and the remaining axes are orthoginal to that. We can demonstrate this using R's built in PCA function.

```{r}
# perform PCA
PCAres <- stats::prcomp(cbind(x1, x2), scale = F, center = F)

# get rotated data
xr <- as.data.frame(PCAres$x)

# plot on principle components (set y axis to similar scale to orig data)
ggplot(xr, aes(x = -PC1, y = PC2)) +
  geom_point() +
  ylim(-(max(x2) - min(x2))/2, (max(x2) - min(x2))/2)
```

If you look carefully, you can see that the data corresponds to the previous plot, but just rotated slightly, so that the direction of greatest spread (i.e. greatest variance) is aligned with the first principle component. Here, I have set the y axis to a similar scale to the previous plot to make it easier to see.

There are many reasons for doing PCA, but they often correspond to two main categories: exploratory analysis and dimension reduction. In the context of composite indicators, both of these goals are relevant.

In terms of exploratory analysis, PCA provides somewhat similar information to correlation analysis and Cronbach's alpha. For any given set of indicators, or aggregates, we can check to see how much variance is explained by the first (few) principle component(s). If the data can be explained by a few PCs, then it suggests that there is a strong latent variable, i.e. the data is shaped something like the figure above, such that if we rotate it, we can find an axis (linear combination) which explains the data pretty well. This is quite similar to the idea of "consistency" in Cronbach's alpha.

R has built in functions for PCA (as shown above), and there is no need to reinvent the wheel here. But COINr has a function `getPCA()` which makes it easier to interact with COINs, and follows the familiar COINr syntax.

```{r, eval = F}
PCAres <- getPCA(ASEM, dset = "Raw", icodes = "Physical", aglev = 1)
```

TO FINISH
