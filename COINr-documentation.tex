% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Composite Indicator Development and Analysis in R with COINr},
  pdfauthor={William Becker},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Composite Indicator Development and Analysis in R with COINr}
\author{William Becker}
\date{2021-07-01}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{background}{%
\section{Background}\label{background}}

This documentation describes in detail the COINr package, which is an open source R package for developing and analysing composite indicators, developed by the European Commission's \href{https://knowledge4policy.ec.europa.eu/composite-indicators/about_en}{Joint Research Centre}. In fact, this is slightly more than technical documentation, and also gives some tips on composite indicator development along the way.

A \emph{composite indicator} is an aggregation of indicators which aims to measure a particular concept. Composite indicators are typically used to measure complex and multidimensional concepts which are difficult to define, and cannot be measured directly. Examples include innovation, human development, environmental performance, and so on. Composite indicators are closely related to scoreboards, which are also groups of indicators aiming to capture a concept. However, scoreboards do not aggregate indicator values. Composite indicators also usually use a hierarchical structure which breaks the concept down into elements, sometimes known as sub-pillars, pillars, sub-indexes, dimensions, and so on.

COINr is currently still under development, therefore the package itself, and this documentation, are a work in progress but are being continually updated. You can find the latest version of COINr at its \href{https://github.com/bluefoxr/COINr}{GitHub repo}, and also the source code for this documentation \href{https://github.com/bluefoxr/COINrDoc}{here}.

\hypertarget{installation}{%
\section{Installation}\label{installation}}

Although COINr is under development, a pre-beta version can be installed via Github. First, install the \texttt{devtools} package if you don't already have it, then run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"bluefoxr/COINr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This should directly install the package from Github, without any other steps. You may be asked to update packages. This might not be strictly necessary, so you can also skip this step.

At the time of writing (June 2021), the functionalities of the package are perhaps 90\% complete. Roughly speaking, the features that are described in detail in this documentation are more or less complete and have survived an initial round of testing and checks. Any functions that are not described in detail here should be treated with more caution and it is likely that bugs would be encountered here and there. The entire package will need to beta tested before a more official ``release''.

In case you do spot a bug, or have a suggestion, the best way is to \href{https://github.com/bluefoxr/COINr/issues}{open an issue} on the repo. Otherwise, you can also just \href{mailto:william.becker@bluefoxdata.eu}{email me}.

\hypertarget{what-does-it-do}{%
\section{What does it do?}\label{what-does-it-do}}

COINr is the first fully-flexible development and analysis environment for composite indicators and scoreboards. It is more than simply a collection of useful functions - it wraps up all data, methodology and analysis of a composite indicator into a single object called a ``COIN''. COINr functions are adapted to work with very few commands on COINs, but many will also work on data frames as well. This leads to a fast and neat environment for building and analysing composite indicators.

Using COINs, i.e.~the \emph{COINrverse approach}, enables the full feature set of COINr, because some functions only work on COINs - for example, exporting all results to Excel in one command, or running an uncertainty/sensitivity analysis. Indeed, COINr is built mainly for COINrverse use, but functions have been written with the flexibility to also accommodate independent use where possible.

The main features can be summarised as features for \emph{building}, features for \emph{analysis} and features for \emph{visualisation and presentation}.

\textbf{Building features}:

\begin{itemize}
\tightlist
\item
  Flexible and fast development of composite indicators with no limits on aggregation levels, numbers of indicators, highly flexible set of methodological choices.
\item
  Denomination by other indicators (including built in world denominators data set)
\item
  Screening units by data requirements
\item
  Imputation of missing data, by a variety of methods
\item
  Data treatment using Winsorisation and nonlinear transformations
\item
  Normalisation by more than ten methods, either for all indicators or for each individually
\item
  Weighting using either manual weighting, PCA weights or correlation optimised weights. COINr also includes a reweighting app which explores the effects of weights on correlations.
\item
  Aggregation of indicators using a variety of methods which can be different for each aggregation level.
\end{itemize}

\textbf{Analysis features:}

\begin{itemize}
\tightlist
\item
  Detailed indicator statistics, and data availability within aggregation groups
\item
  Multivariate analysis, including quick functions for PCA, and a detailed correlation analysis and visualisation
\item
  Easy ``what if'' analysis - very quickly checking the effects of adding and removing indicators, changing weights, methodological variations
\item
  Full global uncertainty and sensitivity analysis which can check the impacts of uncertainties in weighting and many methodological choices
\end{itemize}

\textbf{Visualisation and presentation:}

\begin{itemize}
\tightlist
\item
  Statistical plots of indicators - histograms, violin plots, dot plots, scatter plots and more, including interactive html plots and an app for exploring indicator data
\item
  Bar charts, stacked bar charts, maps, tables and radar charts for presenting indicator data and making comparisons between units
\item
  Static and interactive correlation plots for visualising correlations between indicators and between aggregation levels
\item
  An interactive app for visualising and presenting initial results
\item
  Automatic generation of unit reports (e.g.~country reports) using customisable R markdown templates
\end{itemize}

COINr also allows fast import from the \href{https://knowledge4policy.ec.europa.eu/composite-indicators/coin-tool_en}{COIN Tool} and fast export to Excel.

In short, COINr aims to allow composite indicators to be developed and prototyped very quickly and in a structured fashion, with the results immediately available and able to be explored interactively. Although it is built in R, it is a high-level package that aims to make command simple and intuitive, with the hard work performed behind the scenes, therefore it is also accessible to less experienced R users.

\hypertarget{how-to-use-this-manual}{%
\section{How to use this manual}\label{how-to-use-this-manual}}

COINr has many features, and it is unlikely you will want to read this manual from cover to cover. Roughly speaking, the chapters of this manual represent the major functions and groups of functions present in COINr. If you are looking for guidance on a specific function, simply look for the relevant chapter, or search for the function using the search box. You can also use \texttt{?function\_name} in R for help on each function.

If you are building a composite indicator, the following chapters should be useful in particular:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{coins-the-currency-of-coinr}{COINs: the currency of COINr}
\item
  \protect\hyperlink{normalisation-1}{Normalisation}
\item
  \protect\hyperlink{aggregation-1}{Aggregation}
\item
  \protect\hyperlink{appendix-building-a-composite-indicator-example}{Appendix: Building a Composite Indicator Example}
\end{itemize}

Then you may wish to see chapters on \protect\hyperlink{foundations}{Foundations}, \protect\hyperlink{initial-visualisation-and-analysis}{Initial visualisation and analysis}, \protect\hyperlink{denomination-1}{Denomination}, \protect\hyperlink{missing-data-and-imputation}{Missing data and Imputation}, \protect\hyperlink{data-treatment-1}{Data Treatment} and \protect\hyperlink{weighting-2}{Weighting} depending on what you want to do.

If you are analysing a composite indicator (and this may also be part of the construction process), some relevant chapters are:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{initial-visualisation-and-analysis}{Initial visualisation and analysis}
\item
  \protect\hyperlink{multivariate-analysis}{Multivariate analysis}
\item
  \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons}
\item
  \protect\hyperlink{sensitivity-analysis}{Sensitivity analysis}
\item
  \protect\hyperlink{appendix-analysing-a-composite-indicator-example}{Appendix: Analysing a Composite Indicator Example}
\end{itemize}

It is however worth browsing through the contents to pick out any further relevant chapters and sections that might be of interest.

\hypertarget{demo}{%
\section{Demo}\label{demo}}

COINr is highly customisable, but equally allows composite indicator construction in a few commands. Taking the built-in ASEM dataset, we can assemble a composite indicator in a few steps.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Attaching package: \textquotesingle{}COINr\textquotesingle{}}
\DocumentationTok{\#\# The following object is masked from \textquotesingle{}package:stats\textquotesingle{}:}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#     aggregate}

\CommentTok{\# assemble basic composite indicator object from input data}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ ASEMIndData,}
                 \AttributeTok{IndMeta =}\NormalTok{ ASEMIndMeta,}
                 \AttributeTok{AggMeta =}\NormalTok{ ASEMAggMeta)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Here, we have loaded the COINr package, and assembled a so-called ``COIN'', which is a structured object used universally within COINr. It contains a complete record of all data sets, parameters, methodological choices, analysis and results generated during the construction and analysis process. A COIN is initially assembled by supplying a table of indicator data, a table of indicator metadata (which also specifies the structure of the index), and a table of aggregation metadata. More details can be found in the chapter on \protect\hyperlink{coins-the-currency-of-coinr}{COINs: the currency of COINr}.

Currently, the COIN (composite indicator) only contains the input data set and other metadata. To actually get the aggregated results (i.e.~and index), we have to denominate, normalise and aggregate it. These operations are performed by dedicated functions in COINr.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# denominate data using specifications in ASEMIndMeta}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM)}
\CommentTok{\# normalise data}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{normalise}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{)}
\CommentTok{\# Aggregate normalised data}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Of course, there are many ways to aggregate data, and other steps could be performed before aggregating, such as (multivariate) analysis, data treatment and so forth. Here, we have simply taken three basic steps.

Each of the functions generates a new data set -- here, respectively ``Denominated'', ``Normalised'' and ``Aggregated'' data sets which are added to and stored within the COIN. Although in the example here the default specifications have been used, the functions have many options, and can be directed to operate on any of the data sets within the COIN. Details on this are left to later chapters in this book.

At this point, let us examine some of the outputs in the COIN. First, we may want to see the index structure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotframework}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-1-1.pdf}

In the online version of this documentation, this plot appears as an interactive ``HTML widget'' which can be embedded into HTML documents. COINr leverages existing R packages, and interactive graphics (like that of Figure 6) are generated using bindings to the plotly package, which is based on Javascript libraries. Static graphics are largely produced by the ggplot2 package.

The framework plot is called a ``sunburst plot'' and summarises the structure of the index and the relative weight of each indicator in each aggregation level and the index.

We can also get indicator statistics in table format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reactable)}
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getStats}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{)}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{StatTable }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  reactable}\SpecialCharTok{::}\FunctionTok{reactable}\NormalTok{(}\AttributeTok{defaultPageSize =} \DecValTok{5}\NormalTok{, }\AttributeTok{highlight =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{wrap =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-2-1.pdf}

Again, this table is an presented as an interactive HTML table and is easier explored using the online version of this book. In short, \texttt{getstats()} provides key statistics on each indicator, relating to data availability, outliers, summary statistics such as mean, median, skew, kurtosis, and so on. This can be applied to any data set within the COIN.

Indicator distributions can also be easily plotted:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotIndDist}\NormalTok{(ASEM, }\AttributeTok{type =} \StringTok{"Violindot"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Political"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-3-1.pdf}

Finally, results can be visualised and explored in an interactive dashboard. This can be useful as a rapid prototype of the index. The app may even be hosted online, or can be used as the basis for a more customised indicator visualisation platform. Here, a basic screenshot is provided.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/demo_resultsdash} 

}

\caption{Results dashboard screenshot}\label{fig:unnamed-chunk-4}
\end{figure}

Clearly, this is not meant to replace a dedicated composite indicator visualisation site, but is a fast first step.

\hypertarget{foundations}{%
\chapter{Foundations}\label{foundations}}

This chapter aims to explain some important principles about how COINr works, and explain some terminology. It also gives a map of the functions available in the package. If you want to get straight to using COINr, you could skip this chapter for now and look at some examples in the appendices or directly at the chapters relating to specific functions and operations (the next chapter onwards).

\hypertarget{terminology}{%
\section{Terminology}\label{terminology}}

Let's clearly define a few terms first to avoid confusion later on.

\begin{itemize}
\item
  An \emph{indicator} is a variable which has an observed value for each unit. Indicators might be things like life expectancy, CO2 emissions, number of tertiary graduates, and so on.
\item
  A \emph{unit} is one of the entities that you are comparing using indicators. Often, units are countries, but they could also be regions, universities, individuals or even competing policy options (the latter is the realm of multicriteria decision analysis).
\end{itemize}

Together, indicators and units form the main input data frame for COINr (units as rows, and indicators as columns):

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/inds_n_units} 

}

\caption{Indicators and units}\label{fig:unnamed-chunk-5}
\end{figure}

\emph{Composite indicators} are created by aggregating multiple indicators into a single value. This may often be repeated more than once, following a hierarchical structure, such as the following figure:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/CIhierarchy} 

}

\caption{Example of composite indicator structure.}\label{fig:unnamed-chunk-6}
\end{figure}

In this example, four pairs of indicators are each aggregated into a ``sub-pillar'', and sub-pillars are aggregated into two ``pillars''. Finally, the pillars are aggregated into a single index. Typically, the sub-pillars and pillars represent specific sub-concepts of the central concept. For example, in the \href{https://www.globalinnovationindex.org/Home}{Global Innovation Index}, which aims to measure innovation at the national level, pillars include things like ``Institutions'', ``Human Capital'', and ``Infrastructure'', each of which are populated with indicators which measure those specific concepts. The idea is that a complex multidimensional concept is broken down into sub-concepts which are easier to capture with a set of indicators. Additionally, the scores of pillars and sub-pillars can be interesting in their own right.

Whether each of these groups is called a ``pillar'', ``sub-index'', ``dimension'', or something else, depends on the index. For this reason, in COINr a more general terminology is adopted.

When a group of indicators is aggregated together to form a single sub-pillar, pillar or otherwise, this is called an \emph{aggregation group}. For example, the aggregation group of SP 1 in the diagram above consists of ``ind.01'' and ``ind.02''. The aggregation group of Pillar 1 consists of SP 1 and SP 2. And so on. The resulting value from aggregating an aggregation group is called an \emph{aggregate} or \emph{aggregate value}.

COINr also frequently refers to \emph{aggregation levels} or equivalently just \emph{levels}. This refers to the vertical position in the hierarchy shown above. In this example, therefore,

\begin{itemize}
\tightlist
\item
  Level 1 = Indicators
\item
  Level 2 = Pillars
\item
  Level 3 = Sub-indexes
\item
  Level 4 = Index
\end{itemize}

This means that the fictional index above has four levels in total. COINr can handle any number of levels and aggregation groups. The structure of the index is defined when the COIN is constructed - see the following chapter for details.

\hypertarget{coinr-syntax}{%
\section{COINr syntax}\label{coinr-syntax}}

Many COINr functions follow a common syntax which is related to some of the terminology described above.

\begin{itemize}
\tightlist
\item
  \texttt{dset} always refers to the name of the data set that is found in \texttt{.\$Data}. For example, \texttt{dset\ =\ "Normalised"} will return \texttt{.\$Data\$Normalised}. The only exception is \texttt{dset\ =\ "Denominators"}, which returns \texttt{.\$Input\$Denominators}. The argument \texttt{dset} is used in many COINr functions because they have to know which data set to operate on.
\item
  \texttt{icodes} is a character vector of indicator or aggregate codes
\item
  \texttt{aglev} is the aggregation level, or levels, to take the indicator data from. To understand how this works, see \protect\hyperlink{selecting-data-sets-and-indicators}{Selecting data sets and indicators}.
\item
  \texttt{out2} controls the output of many functions - if set to ``COIN'' it will append the results back to the COIN that was input to the function, otherwise if ``df'' or ``list'' it will return the results as a data frame or list depending on the function.
\end{itemize}

In particular, the combination of \texttt{icodes} and \texttt{aglev} is useful because it allows to call sets of indicators inside aggregation groups very quickly. See \protect\hyperlink{selecting-data-sets-and-indicators}{Selecting data sets and indicators} for more details.

\hypertarget{a-map-of-coinr}{%
\section{A map of COINr}\label{a-map-of-coinr}}

COINr consists of many functions that do a number of different types of operations. Here is a summary.

\hypertarget{construction}{%
\subsection{Construction}\label{construction}}

The following functions are the main functions for building a composite indicator in COINr.

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{assemble()} & Assembles indicator data/metadata into a COIN \\
\texttt{checkData()} & Data availability check and unit screening \\
\texttt{denominate()} & Denominate (divide) indicators by other indicators \\
\texttt{impute()} & Impute missing data using various methods \\
\texttt{treat()} & Treat outliers with Winsorisation and transformations \\
\texttt{normalise()} & Normalise data using various methods \\
\texttt{aggregate()} & Aggregate indicators into hierarchical levels, up to index \\
\texttt{regen()} & Regenerates COIN results using specifications stored in \texttt{.\$Method} \\
\bottomrule
\end{longtable}

\hypertarget{visualisation-and-presenting}{%
\subsection{Visualisation and presenting}\label{visualisation-and-presenting}}

These functions are for visualising and presenting data and results

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{iplotBar()} & Interactive bar chart for any indicator, includes stacked bar charts \\
\texttt{iplotCorr()} & Interactive correlation heatmap \\
\texttt{iplotIndDist()} & Interactive indicator distribution plots for a single indicator \\
\texttt{iplotIndDist2()} & Interactive indicator distribution plots for two indicators simultaneously \\
\texttt{iplotMap()} & Interactive choropleth map for any indicator (only works for countries) \\
\texttt{iplotRadar()} & Interactive radar chart for specified unit(s) and specified indicators \\
\texttt{iplotTable()} & Interactive results table with conditional formatting \\
\texttt{plotCorr()} & Correlation heatmap between any levels/groups of the index \\
\texttt{plotframework()} & Interactive sunburst plot visualising the indicator framework \\
\texttt{plotIndDist()} & Static plot of distributions for indicators and groups of indicators \\
\texttt{plotSA()} & Plot sensitivity analysis results (sensitivity indices) \\
\texttt{plotSARanks()} & Plot confidence intervals on ranks, following an uncertainty/sensitivity analysis \\
\texttt{colourTable()} & Conditionally-formatted interactive HTML table, given a data frame \\
\texttt{getResults()} & Gets quick results summary table \\
\texttt{getStrengthNWeak()} & Gets strengths and weaknesses (N top ranked indicators) of a selected unit \\
\texttt{getUnitReport()} & Generates a unit report following a template, and outputs either HTML, Word or pdf format \\
\texttt{getUnitSummary()} & Summarises scores and ranks of a selected unit, at specified aggregation levels \\
\bottomrule
\end{longtable}

\hypertarget{adjustment-and-comparison}{%
\subsection{Adjustment and comparison}\label{adjustment-and-comparison}}

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{compTable()} & Comparison table of selected indicator/aggregate between two COINs \\
\texttt{compTableMulti()} & Comparison table of selected indicator/aggregate between multiple COINs \\
\texttt{indChange()} & Short cut to add or remove indicators \\
\bottomrule
\end{longtable}

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

The following functions analyse indicator data.

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{getCorr()} & Get correlation matrices between various groups and levels \\
\texttt{getCronbach()} & Get Cronbach's alpha for specified group/level \\
\texttt{getPCA()} & Principle component analysis on a specified data set and subset of indicators. Also returns PCA weights. \\
\texttt{effectiveWeight()} & Calculates the effective weights of each element in the indicator hierarchy. \\
\texttt{getStats()} & Get table of indicator statistics for any data set \\
\texttt{sensitivity()} & Perform a Monte Carlo uncertainty analysis and/or a global sensitivity analysis on a COIN \\
\texttt{weightOpt()} & Weight optimisation according to a pre-specified vector of ``importances'' \\
\bottomrule
\end{longtable}

\hypertarget{interactive-apps}{%
\subsection{Interactive apps}\label{interactive-apps}}

These are interactive apps, built using Shiny, which allow fast interactive exploration and adjustments.

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{indDash()} & Indicator visualisation (distribution) dashboard for one or two indicators \\
\texttt{rew8r()} & Interactively re-weight indicators and check updated results and correlations \\
\texttt{resultsDash()} & Interactive dashboard for visualising and exploring results \\
\bottomrule
\end{longtable}

\hypertarget{importexport}{%
\subsection{Import/export}\label{importexport}}

Functions to import and export data and results to and from COINr and R.

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{COINToolIn()} & Import indicator data and metadata from COIN Tool \\
\texttt{coin2Excel()} W & rite data, analysis and results from a COIN to Excel \\
\bottomrule
\end{longtable}

\hypertarget{other-functions}{%
\subsection{Other functions}\label{other-functions}}

These are other functions that may be of interest, but do not neatly fit in the previous categories. Many of them are called from the other functions listed above, but may still be useful on their own.

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{BoxCox()} & Box Cox transformation on a vector of data \\
\texttt{build\_ASEM()} & Build ASEM (example) composite indicator in one command \\
\texttt{coin\_win()} & Winsorise one column of data according to skew/kurtosis thresholds \\
\texttt{copeland()} & Aggregates a data frame into a single column using the Copeland method. \\
\texttt{geoMean()} & Weighted geometric mean of a vector \\
\texttt{getIn()} & Useful function for subsetting indicator data. See \protect\hyperlink{helper-functions}{Helper functions}. \\
\texttt{harMean()} & Weighted harmonic mean of a vector \\
\texttt{loggish()} & Log-type transformation, of various types, for a vector \\
\texttt{names2Codes()} & Given a character vector of long names (probably with spaces), generates short codes. \\
\texttt{outrankMatrix()} & Outranking matrix based on a data frame of indicator data and corresponding weights \\
\texttt{roundDF()} & Round down a data frame (i.e.~for presentation) \\
\texttt{rankDF()} & Convert a data frame of scores to ranks (non-numerical columns are ignored) \\
\bottomrule
\end{longtable}

\hypertarget{data}{%
\subsection{Data}\label{data}}

COINr comes with some example data embedded into the package.

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{ASEMIndData} & ASEM (example) indicator data as input for \texttt{assemble()} \\
\texttt{ASEMIndMeta} & ASEM (example) indicator metadata as input for \texttt{assemble()} \\
\texttt{ASEMAggMeta} & ASEM (example) aggregate metadata as input for \texttt{assemble()} \\
\texttt{WorldDenoms} & National denomination data (GDP, population, etc) worldwide \\
\bottomrule
\end{longtable}

\hypertarget{finally}{%
\subsection{Finally}\label{finally}}

There are also a number functions which are mainly for internal use, and are not listed here.

\hypertarget{tips-for-using-coinr}{%
\section{Tips for using COINr}\label{tips-for-using-coinr}}

\hypertarget{use-scripts-and-r-markdown}{%
\subsection{Use scripts and R Markdown}\label{use-scripts-and-r-markdown}}

Like any good data science, anything you do in COINr should be reproducible. This means that ideally, you should be able to go from importing the data into R up to visualising the results. This can be either done by a script or a set of scripts, or perhaps better using an R markdown document or documents which combines text with code. If you haven't used R Markdown before, you probably should be using it. It is very similar to the idea of Jupyter notebooks in Python. Find out more about R Markdown in the free and excellent online book \href{https://bookdown.org/yihui/rmarkdown/}{R Markdown: The Definitive Guide}. You might also notice that this manual is written in R Markdown, and more specifically using the \href{https://bookdown.org/yihui/bookdown/}{bookdown package}.

Ideally, the whole process of building and analysing the composite indicator should be documented with an R Markdown file or files, or scripts. Apart from reproducibility, this makes it very easy to make retrospective changes and update the whole file. I have personally found that it may help to have several markdown notebooks - one for exploring the data and selecting indicators, another for data treatment, and another for construction, and so on. At the end of each notebook, the COIN(s) containing all data and analysis are saved to a folder and loaded back in in the next notebook. This avoids having one very long notebook. Of course, this is just one way of doing things and is just a suggestion.

\hypertarget{missing-data}{%
\subsection{Missing data}\label{missing-data}}

It's very important to understand the difference between missing data and zeros. A zero means that you know that the value of the indicator is zero, whereas missing data (in R this is denoted as \texttt{NA}) means you don't know what the value is at all.

When aggregating indicators, this difference becomes particularly important. If a data point is missing, it is often excluded from the aggregation, which effectively means reassigning it with the mean (or similar) value of the indicators in the same group. If you impute the data, missing values could be assigned with mean or median indicator values, for example. Clearly, these values will be very different from zeros.

This point is made here, because data may sometimes come with missing values denoted as zeros, or with zeros denoted as missing values. Ensure that these are checked carefully before proceeding with construction.

\hypertarget{plotting}{%
\subsection{Plotting}\label{plotting}}

COINr includes a number of plotting functions which return various types of plots. These are all either based on the ggplot2 or plotly packages. Think of COINr plots as ``starting points'' rather than finished plots. While I have tried to make the plots useful and even visually appealing, much more could be done to improve them and to fit particular uses and contexts.

Luckily, because of the way that plotly and ggplot2 work, plots generated by COINr can easily be modified by assigning them to an object and then modifying the object. Generally this would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# generate a plot, but assign to variable "plt"}
\NormalTok{plt }\OtherTok{\textless{}{-}} \FunctionTok{COINr\_plot\_function}\NormalTok{(COIN, plot\_options)}

\CommentTok{\# alter plt}
\NormalTok{plt }\OtherTok{\textless{}{-}}\NormalTok{ plt }\SpecialCharTok{+} \FunctionTok{alter\_plot\_function}\NormalTok{(options)}

\CommentTok{\# view the plot}
\NormalTok{plt}
\end{Highlighting}
\end{Shaded}

The \texttt{alter\_plot\_function} could be any of the very many layout and geometry functions from ggplot2, for example. This means you can change colours, axis labels, point styles, titles and many other things.

In summary, with COINr you may choose to either:

\begin{itemize}
\tightlist
\item
  Generate plots with COINr functions and be content with them as they are
\item
  Generate plots with COINr functions and tweak them to your tastes using the approach shown above
\item
  Make your own plots
\end{itemize}

\hypertarget{coins-the-currency-of-coinr}{%
\chapter{COINs: the currency of COINr}\label{coins-the-currency-of-coinr}}

Where possible COINr functions can be used as standalone functions, typically on data frames of indicator data, for operations such as normalisation, imputation and so on.

The full functionality of COINr is however harnessed by assembling the indicator data, and the structure of the index, into a ``COIN''. A COIN is a structured list, which neatly stores the various data sets, parameters, analyses and methodological decisions in one place. There are various reasons for doing this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It simplifies operations because functions know where data and parameters are. Therefore, COINr operations typically follow a syntax of the form \texttt{COIN\ \textless{}-\ COINr\_function(COIN,\ \textless{}methodological\ settings\textgreater{})}, updating the COIN with new results and data sets generated by the function. This avoids having to repeatedly specify indicator names, structure, etc, as separate arguments. It's like putting a coin in a vending machine and getting a packet of crisps. Except you also get your coin back\footnote{Depending on the function, you either get a packet of crisps (e.g.~a plot or analysis), or your coin plus interest (an updated COIN with a new data set or analysis)}.
\item
  It keeps things organised and avoids a workspace of dozens of variables.
\item
  It keeps a record of methodological decisions - this allows results to be easily regenerated following ``what if'' experiments, such as removing or changing indicators etc (see \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons}).
\end{enumerate}

The logic of doing this will become clearer as you build your index. If you only want to use the COINr functions on data frames, you can probably skip this chapter.

To build a COIN, COINr has a dedicated function called \texttt{assemble()}. This is explained below in more detail.

\hypertarget{inside-a-coin}{%
\section{Inside a COIN}\label{inside-a-coin}}

COINs are hierarchical lists, a bit like a folder system. In R they are sometimes called ``lists of lists'' or ``nested lists''. A COIN contains all the data, parameters, methodological choices and results of a composite indicator. It has a structure that looks like this:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/COIN_contents_tree} 

}

\caption{Inside a COIN}\label{fig:unnamed-chunk-7}
\end{figure}

When you first build the COIN, using the three input data frames explained in the next section, the COIN will only consist of the Input, Data, Parameters and possibly Method ``folders'' (sub-lists). As you construct the composite indicator, more things will be added to the COIN, for example extra data sets (denominated, normalised, aggregated), analysis (missing data, PCA, etc), and results (summary results tables).

Many analysis functions in COINr, for example, have an argument called \texttt{out2}, which specifies whether the result should be stored inside the COIN or output as a separate list or data frame.

It is worth exploring the contents of the COIN as it is built to understand where things are kept. A couple of things to keep in mind are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  One good reason to store things inside the COIN is that if you plan to export the results to Excel at any point, you can use the \texttt{coin2Excel()} function to export (almost) everything inside the COIN in one command. This includes all data sets, analysis tables and results tables.
\item
  A COIN is a list that has been labelled as a ``COIN'' using R's \texttt{class()} function. That means you can edit it as you would with any other list. This means that:

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    You can also store other things in the COIN if you want, and add folders and lists to it, but,
  \item
    You have to be careful editing it, because COINr functions expect things to be in certain places inside the COIN. So if you delete something, for example, some COINr functions may not work.
  \end{enumerate}
\end{enumerate}

\hypertarget{the-three-inputs}{%
\section{The three inputs}\label{the-three-inputs}}

To build a COIN, three ingredients (data frames) are needed to begin with, and are used as inputs to the \texttt{assemble()} function, which outputs a COIN. In short, they are the indicator data, the indicator metadata, and the aggregation metadata. Here, each will be explained separately. Inputting the data in the first place is where you will do most of the work, because you have to get your data in a format that COINr understands. But once you have got your data assembled, COINr should do most of the hard work, so hang in there!

\hypertarget{indicator-data}{%
\subsection{Indicator data}\label{indicator-data}}

The indicator data is a data frame which, shockingly, specifies the data for the indicators. However, it can do more than that, and also some rules have to be followed. The easiest way to explain is to start with an example.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}
\FunctionTok{head}\NormalTok{(ASEMIndData)}
\DocumentationTok{\#\# \# A tibble: 6 x 60}
\DocumentationTok{\#\#   UnitName UnitCode Group\_GDP Group\_GDPpc Group\_Pop Group\_EurAsia  Year Den\_Area}
\DocumentationTok{\#\#   \textless{}chr\textgreater{}    \textless{}chr\textgreater{}    \textless{}chr\textgreater{}     \textless{}chr\textgreater{}       \textless{}chr\textgreater{}     \textless{}chr\textgreater{}         \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\DocumentationTok{\#\# 1 Austria  AUT      L         XL          M         Europe         2018    83871}
\DocumentationTok{\#\# 2 Belgium  BEL      L         L           L         Europe         2018    30528}
\DocumentationTok{\#\# 3 Bulgaria BGR      S         S           M         Europe         2018   110879}
\DocumentationTok{\#\# 4 Croatia  HRV      S         M           S         Europe         2018    56594}
\DocumentationTok{\#\# 5 Cyprus   CYP      S         L           S         Europe         2018     9251}
\DocumentationTok{\#\# 6 Czech R\textasciitilde{} CZE      M         L           M         Europe         2018    78867}
\DocumentationTok{\#\# \# ... with 52 more variables: Den\_Energy \textless{}dbl\textgreater{}, Den\_GDP \textless{}dbl\textgreater{}, Den\_Pop \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   LPI \textless{}dbl\textgreater{}, Flights \textless{}dbl\textgreater{}, Ship \textless{}dbl\textgreater{}, Bord \textless{}dbl\textgreater{}, Elec \textless{}dbl\textgreater{}, Gas \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   ConSpeed \textless{}dbl\textgreater{}, Cov4G \textless{}dbl\textgreater{}, Goods \textless{}dbl\textgreater{}, Services \textless{}dbl\textgreater{}, FDI \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   PRemit \textless{}dbl\textgreater{}, ForPort \textless{}dbl\textgreater{}, Embs \textless{}dbl\textgreater{}, IGOs \textless{}dbl\textgreater{}, UNVote \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   CostImpEx \textless{}dbl\textgreater{}, Tariff \textless{}dbl\textgreater{}, TBTs \textless{}dbl\textgreater{}, TIRcon \textless{}dbl\textgreater{}, RTAs \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Visa \textless{}dbl\textgreater{}, StMob \textless{}dbl\textgreater{}, Research \textless{}dbl\textgreater{}, Pat \textless{}dbl\textgreater{}, CultServ \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   CultGood \textless{}dbl\textgreater{}, Tourist \textless{}dbl\textgreater{}, MigStock \textless{}dbl\textgreater{}, Lang \textless{}dbl\textgreater{}, Renew \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   PrimEner \textless{}dbl\textgreater{}, CO2 \textless{}dbl\textgreater{}, MatCon \textless{}dbl\textgreater{}, Forest \textless{}dbl\textgreater{}, Poverty \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Palma \textless{}dbl\textgreater{}, TertGrad \textless{}dbl\textgreater{}, FreePress \textless{}dbl\textgreater{}, TolMin \textless{}dbl\textgreater{}, NGOs \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   CPI \textless{}dbl\textgreater{}, FemLab \textless{}dbl\textgreater{}, WomParl \textless{}dbl\textgreater{}, PubDebt \textless{}dbl\textgreater{}, PrivDebt \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   GDPGrow \textless{}dbl\textgreater{}, RDExp \textless{}dbl\textgreater{}, NEET \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

COINr comes prepackaged with some example data - here we are looking at indicator data from the \href{https://composite-indicators.jrc.ec.europa.eu/asem-sustainable-connectivity/}{ASEM Sustainable Connectivity Portal}, which has an indicator data set covering 51 Asian and European countries.

The first thing to notice is that each row is an observation (here, a country), and each column is a variable (mostly indicators, but also other things). Look at the structure of the data frame, working from left to right:

\begin{itemize}
\tightlist
\item
  \texttt{UnitName} {[}\textbf{required}{]} gives the name of each unit. Here, units are countries, so these are the names of each country.
\item
  \texttt{UnitCode} {[}\textbf{required}{]} is a unique code assigned to each unit (country). This is very important, and is the main ``reference'' inside COINr for units. If your units are countries, I recommend using \href{https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3}{ISO Alpha-3 codes}, because these are recognised by COINr for generating maps. It also makes data processing generally easier.
\item
  \texttt{Group\_*} {[}optional{]} Any column name that starts with \texttt{Group\_} is recognised as a group column rather than an indicator. You don't have to have any groups, but some COINr functions support specific operations on groups (e.g.~imputation within group, and future updates plan to expand this capacity). You can have as many group columns as you want.
\item
  \texttt{Year} {[}optional{]} gives the reference year of the data. This allows you to have multiple years of data, for example, you can have a value for a given country for 2018, and another value for the same country for 2019, and so on. Like groups, this feature is not yet fully developed in COINr.
\item
  \texttt{Den\_*}{[}optional{]} Any column names that begin with \texttt{Den\_*} are recognised as \emph{denominators}, i.e.~indicators that are used to scale other indicators.
\item
  Finally, any column that begins with \texttt{x\_} will be ignored and passed through. This is not shown in the data set above, but is useful for e.g.~alternative codes or other variables that you want to retain. \emph{Any remaining columns that do not begin with \texttt{x\_} or use the other names in this list are recognised as indicators.}
\end{itemize}

You will notice that all column (variable/indicator) names use short codes. This is to keep things concise in tables, rough plots etc. Indicator codes should be short, but informative enough that you know which indicator it refers to (e.g.~``Ind1'' is not so helpful). In some COINr plots, codes are displayed, so you might want to take that into account. In any case, the full names of indicators, and other details, are also specified in the indicator metadata table - see the next section.

Some important rules and tips to keep in mind are:

\begin{itemize}
\tightlist
\item
  The following columns are \emph{required}. All other columns are optional:

  \begin{itemize}
  \tightlist
  \item
    UnitCode
  \item
    UnitName
  \item
    At least one indicator column
  \end{itemize}
\item
  Columns don't have to be in any particular order, columns are identified by names rather than positions.
\item
  You can have as many indicators and units as you like.
\item
  Indicator codes and unit codes must have unique names. You can't use the same code twice otherwise bad things will happen.
\item
  Avoid any accented characters or basically any characters outside of English - this can sometimes cause trouble with encoding.
\item
  Column names are case-sensitive. Most things in COINr are built to have a degree of flexibility where possible, but \emph{column names need to be written exactly as they appear here for COINr to recognise them}.,
\end{itemize}

\hypertarget{indicator-metadata}{%
\subsection{Indicator metadata}\label{indicator-metadata}}

The second data frame you need to input specifies the \emph{metadata} of each indicator. This serves two purposes: first, to give details about each indicator, such as its name, its units and so on; and second, to specify the structure of the index. Here's what this looks like, for our example ASEM data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(ASEMIndMeta)}
\DocumentationTok{\#\# \# A tibble: 6 x 10}
\DocumentationTok{\#\#   IndName IndCode Direction IndWeight Denominator IndUnit Target Agg1  Agg2 }
\DocumentationTok{\#\#   \textless{}chr\textgreater{}   \textless{}chr\textgreater{}       \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}chr\textgreater{}       \textless{}chr\textgreater{}    \textless{}dbl\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\DocumentationTok{\#\# 1 Logist\textasciitilde{} LPI             1         1 \textless{}NA\textgreater{}        Score \textasciitilde{}   4.12 Phys\textasciitilde{} Conn }
\DocumentationTok{\#\# 2 Intern\textasciitilde{} Flights         1         1 Den\_Pop     Thousa\textasciitilde{} 200.   Phys\textasciitilde{} Conn }
\DocumentationTok{\#\# 3 Liner \textasciitilde{} Ship            1         1 \textless{}NA\textgreater{}        Score    20.1  Phys\textasciitilde{} Conn }
\DocumentationTok{\#\# 4 Border\textasciitilde{} Bord            1         1 Den\_Area    Number\textasciitilde{} 116.   Phys\textasciitilde{} Conn }
\DocumentationTok{\#\# 5 Trade \textasciitilde{} Elec            1         1 Den\_Energy  TWh     105.   Phys\textasciitilde{} Conn }
\DocumentationTok{\#\# 6 Trade \textasciitilde{} Gas             1         1 Den\_Energy  Billio\textasciitilde{}  90.1  Phys\textasciitilde{} Conn }
\DocumentationTok{\#\# \# ... with 1 more variable: Agg3 \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

Notice that now, the table is flipped on its side (transposed), and each row is an indicator, and the columns specify other things. This is to keep the data in a ``tidy'' format. Let's go through the columns one by one.

\begin{itemize}
\tightlist
\item
  \texttt{IndName} {[}\textbf{required}{]} This is the full name of the indicator, which will be used in display plots.
\item
  \texttt{IndCode}{[}\textbf{required}{]} A reference code for each indicator. These \emph{must be the same codes as specified in the indicator metadata}. The codes must also be unique.
\item
  \texttt{Direction} {[}\textbf{required}{]} The ``direction'' of each indicator - this takes values of either 1 or -1 for each indicator. A value of 1 means that higher values of the indicator correspond to higher values of the index, whereas -1 means the opposite.
\item
  \texttt{IndWeight} {[}\textbf{required}{]} The initial weights assigned to each indicator. Weights are relative and do not need to sum to one, so you can simply put all 1s here if you don't know what else to put (the values can be adjusted later).
\item
  \texttt{Denominator} {[}\textbf{required}??{]} These should be the indicator codes of one of the denominator variables for each indicator to be denominated. E.g. here ``Den\_Pop'' specifies that the indicator should be denominated by the ``Den\_Pop'' indicators (population, in this case). For any indicators that do not need denominating, just set \texttt{NA}. Denominators can also be specified later, so if you want you can leave this column out. See \protect\hyperlink{denomination-1}{Denomination} for more information.
\item
  \texttt{IndUnit} {[}\textbf{optional}{]} The units of the indicator. This helps for keeping track of what the numbers actually mean, and can be used in plots.
\item
  \texttt{Target} {[}\textbf{optional}{]} Targets associated with each indicator. Here, artificial targets have been generated which are 95\% of the maximum score (accounting for the direction of the indicator). These are only used if the normalisation method is distance-to-target.
\item
  \texttt{Agg*} {[}\textbf{required}{]} Any column name that begins with \texttt{Agg} is recognised as a column specifying the aggregation group, and therefore the structure of the index. Aggregation columns should be in the order of the aggregation, but otherwise can have arbitrary names.
\end{itemize}

Let's look at the aggregation columns in a bit more detail. Each column represents a separate aggregation level, so in the ASEM example here we have three aggregation levels - the pillars, the two sub-indexes, and the overall index. The entry of each column specifies which group each indicator falls in. So, the first column \texttt{Agg1} specifies the pillar of each indicator. Again, each aggregation group (pillar, sub-index or index) is referenced by a unique code.

The next column \texttt{Agg2}, gives the sub-index that the indicator belongs to. There is a bit of redundancy here, because obviously indicators in the same pillar must also belong to the same sub-index. Finally, \texttt{Agg3} specifies that all indicators belong to the index.

You can have as many \texttt{Agg} columns as you like, and the names don't have to be \texttt{Agg1} etc, but could be e.g.~\texttt{Agg\_Pillar}, \texttt{Agg\_SubIndex}, etc. However, they \emph{must} begin with \texttt{Agg}, otherwise COINr will not recognise them. And they \emph{must} appear in the order of the aggregation, i.e.~lowest level of aggregation first, then working upwards.

\hypertarget{aggregation-metadata}{%
\subsection{Aggregation metadata}\label{aggregation-metadata}}

The final data input is the aggregation metadata, which is also the simplest. Here's our example for the ASEM data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(ASEMAggMeta)}
\DocumentationTok{\#\# \# A tibble: 6 x 4}
\DocumentationTok{\#\#   AgLevel Code      Name                         Weight}
\DocumentationTok{\#\#     \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}chr\textgreater{}                         \textless{}dbl\textgreater{}}
\DocumentationTok{\#\# 1       2 Physical  Physical                          1}
\DocumentationTok{\#\# 2       2 ConEcFin  Economic and Financial (Con)      1}
\DocumentationTok{\#\# 3       2 Political Political                         1}
\DocumentationTok{\#\# 4       2 Instit    Institutional                     1}
\DocumentationTok{\#\# 5       2 P2P       People to People                  1}
\DocumentationTok{\#\# 6       2 Environ   Environmental                     1}
\end{Highlighting}
\end{Shaded}

This data frame simply consists of three columns for each aggregation level:

\begin{itemize}
\tightlist
\item
  \texttt{*Code} {[}\textbf{required}{]} The aggregation group codes. This column must end with ``Code'', and the codes must match the codes in the corresponding column in the indicator metadata aggregation columns.
\item
  \texttt{*Name} {[}\textbf{required}{]} The aggregation group names. This column must end with ``Name''.
\item
  \texttt{*Weight} {[}\textbf{required}{]} The aggregation group weights. This column must end with ``Weight''. Again, these weights can be changed later on.
\end{itemize}

Columns must appear in the order of the aggregation groups.

\hypertarget{putting-everything-together}{%
\section{Putting everything together}\label{putting-everything-together}}

Having got all your data in the correct format, you can finally build it into a COIN. From here, things start to get a bit easier. The function to build the COIN is called \texttt{assemble()}. This function takes the three data frames mentioned and converts them into a so-called \emph{COIN}, which is a hierarchical list that is structured in a way that is recognised by all COINr functions. Let's run \texttt{assemble()} using the built-in data sets to show how it works.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ ASEMIndData, }\AttributeTok{IndMeta =}\NormalTok{ ASEMIndMeta, }\AttributeTok{AggMeta =}\NormalTok{ ASEMAggMeta)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

The three inputs here are as follows:

\begin{itemize}
\tightlist
\item
  \texttt{IndData} which is the indicator data
\item
  \texttt{IndMeta} which is the indicator metadata
\item
  \texttt{AggMeta} which is the aggregation metadata
\end{itemize}

And this outputs a ``COIN'' which you can name as you want (here we have called it ``ASEM''). There are two further arguments to \texttt{assemble()} which are not used here, and they are:

\begin{itemize}
\tightlist
\item
  \texttt{include} - this is a character vector of indicator codes (i.e.~codes that are found in \texttt{IndData\$IndCode}) which specifies which indicators to include out of the \texttt{IndData} and \texttt{IndMeta} inputs. By default, all indicators are included, but this gives the option to only include certain indicators, if for example, you have several alternative data sources.
\item
  \texttt{exclude}- this an analogous character vector of indicator codes, but specifying which indicators to \emph{exclude}, if any. Again, by default, nothing is excluded. This may be easier to specify when creating a subset of indicators.
\end{itemize}

The structure of the list is as follows. First, it is divided into five main sub-lists, which I am going to call ``folders'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Input} is the input data, and has the following entries:

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    \texttt{IndData} The indicator data, \emph{after} any exclusion of indicators
  \item
    \texttt{IndMeta} The indicator metadata, \emph{after} any exclusion of indicators
  \item
    \texttt{AggMeta} The aggregation metadata, \emph{after} any exclusion of indicators
  \item
    \texttt{Original} A list containing the original, unaltered inputs to \texttt{assemble()}
  \end{enumerate}
\item
  \texttt{Data} is where the indicator data sets are stored. As you go through the construction, this will populated with more data sets, e.g.~imputed data, treated data, aggregated data, and so on. On first assembly you will only see \texttt{Raw} present here.
\item
  \texttt{Parameters} stores some useful parameters that are used by COINr functions, and may be of general interest, such as the indicator codes, unit codes, weights, and so on.
\item
  \texttt{Analysis} is where analysis of the indicator data will be stored, including missing data analysis, indicator summary statistics, principal component analysis and so on.
\item
  \texttt{Method} keeps a record of the methodological decisions made when constructing the composite indicator, for example the normalisation method and parameters, data treatment specifications, and so on. Apart from keeping a record of the construction, this also allows the entire results to be regenerated using the information stored in the COIN, this will be explained better in \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons}.
\end{enumerate}

The logic of the COIN is that COINr functions can take all the data and parameters that they need from it, and you only have to specify some particular parameters of the function. Then, outputs, such as new data sets, are returned back to the COIN, which is added to and expanded as the analysis progresses.

Apart from building the COIN, \texttt{assemble()} does a few other things:

\begin{itemize}
\tightlist
\item
  It checks that indicator codes are consistent between indicator data and indicator metadata
\item
  It checks that required columns, such as indicator codes, are presenr
\item
  It returns some basic information about the data that was input, such as the number of indicators, the number of units, the number of aggregation levels and the groups in each level. This is done so you can check what you have entered, and that it agrees with your expectations.
\end{itemize}

\hypertarget{moving-on}{%
\section{Moving on}\label{moving-on}}

Now that you have a COIN, you can start using all the other functions in COINr to their full potential. Most functions will still work on standalone data frames, so you don't \emph{need} to work with COINs if you prefer not to.

\hypertarget{initial-visualisation-and-analysis}{%
\chapter{Initial visualisation and analysis}\label{initial-visualisation-and-analysis}}

One of the first things to do with indicator data is to look at it, in as many ways as possible. This helps to get a feel for how the data is distributed between units/countries, how it may be spatially distibuted, and how indicators relate to one another. COINr includes various tools for visualising and analysing indicator data and the index structure.

The types of plots generated by COINr fall into two categories: static plots, and interactive plots. static plots generate images in standard formats such as png, pdf and so on. Interactive plots generate javascript graphics, which have interactive elements such as zooming and panning, and information when you hover the mouse over. These latter type of plots are particularly useful for including in HTML documents, because they are self contained. For example, they can be used in R Markdown documents, then knitted to HTML, or embedded on websites (such as this one - see below), e.g.~via the blogdown or bookdown packages. Javascript plots can also be rendered to png and other formats, so can also be used in static documents.

The plotting tools here can be useful at any stage of building a composite indicator or scoreboard, from initial visualisation of the data, to checking the effects of data treatment, to visualising final index results.

\hypertarget{structure}{%
\section{Structure}\label{structure}}

Independently from the indicator data, a good way to begin is to check the structure of the index. This can be done visually with the \texttt{plotframework()} function, which generates a sunburst plot of the index structure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotframework}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-12-1.pdf}

The sunburst plot is useful for a few things. First, it shows the structure that COINr has understood. If you get an error here, it is probably an indication that something has gone wrong in the input of the structure, so go back to the input data and check. If it does successfully display the sunburst plot, you can check whether the structure agrees with your expectations.

Second, it shows the effective weight of each indicator (the value is visible by hovering over each segment). This can reveal which indicators are implicitly weighted more than others, by e.g.~having more or less indicators in the same aggregation groups.

Finally, it can be a good way to communicate your index structure to other people.

\hypertarget{distributions}{%
\section{Distributions}\label{distributions}}

Individual indicator distributions can be visualised in several different ways. For static plots, the main tool is \texttt{plotIndDist()} which generates histograms, boxplots, violin plots, dot plots and violin-dot plots. This is powered by \texttt{ggplot2}, and if you want to customise plots, you should use that directly. However, COINr plotting functions are intended as quick tools to visualise data, with easy access to the hierarchical data set. You can plot individual indicators:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotIndDist}\NormalTok{(ASEM, }\AttributeTok{type =} \StringTok{"Histogram"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"LPI"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-13-1.pdf}

And you can also plot groups of indicators by calling aggregate names (notice that when multiple indicators are plotted, the indicator codes are used to label each plot, rather than indicator names, to save space):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotIndDist}\NormalTok{(ASEM, }\AttributeTok{type =} \StringTok{"Violindot"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Physical"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-14-1.pdf}

The \texttt{plotIndDist()} function has several arguments. In the first place, any indicator or aggregation (pillar, dimension, index etc) can be plotted by using the \texttt{dset} argument. If you have only just assembled the COIN, you will only have the ``Raw'' dataset, but any other dataset can be accessed, e.g.~treated data set, aggregated data set, and so on. You can also target different levels using the \texttt{aglev} argument - for more details see the chapter on \protect\hyperlink{helper-functions}{Helper functions}.

Stand-alone data frames are also supported by \texttt{plotIndDist()} (this can also be achieved directly by \texttt{ggplot} without too much effort):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{90}\NormalTok{),}\AttributeTok{nrow =} \DecValTok{30}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{))}
\FunctionTok{colnames}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Dogs"}\NormalTok{, }\StringTok{"Cats"}\NormalTok{, }\StringTok{"Rabbits"}\NormalTok{)}
\FunctionTok{plotIndDist}\NormalTok{(df, }\AttributeTok{type =} \StringTok{"Box"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-15-1.pdf}

COINr also includes some interactive plots, which are embedded into apps (see later), but can be used for your own purposes, such as embedding in HTML documents or websites.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotIndDist}\NormalTok{(ASEM, }\StringTok{"Raw"}\NormalTok{, }\StringTok{"Renew"}\NormalTok{, }\AttributeTok{ptype =} \StringTok{"Violin"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-16-1.pdf}

Since all the plotting functions output plot objects (plotly objects for \texttt{iplotIndDist}, and ggplot2 plot objects for \texttt{plotIndDist}), you can also modify them if you want to customise the plots. This might be a helpful workflow - to use COINr's default options and then tweak the plot to your liking. In a very simple example, here we just change the title.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotIndDist}\NormalTok{(ASEM, }\StringTok{"Raw"}\NormalTok{, }\StringTok{"Flights"}\NormalTok{, }\AttributeTok{ptype =} \StringTok{"Histogram"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  plotly}\SpecialCharTok{::}\FunctionTok{layout}\NormalTok{(}\AttributeTok{title =} \StringTok{"Customised plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-17-1.pdf}

If you are purely interested in exploring the data, rather than presenting it to someone else, the plots here are also embedded into a Shiny app which lets you quickly explore and compare indicator distributions - see \protect\hyperlink{data-treatment-1}{Data Treatment} for more details on this.

\hypertarget{ranks-and-maps}{%
\section{Ranks and Maps}\label{ranks-and-maps}}

While the previous functions concerned plotting the statistical distributions of each indicator, functions are also available for plotting the indicator values in order or on a map.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotBar}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Embs"}\NormalTok{, }\AttributeTok{usel =} \StringTok{"SGP"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-18-1.pdf}

Here, a single indicator is plotted in order as a bar chart. There is an optional argument to highlight one or more units, using the \texttt{usel} argument.

From a different perspective, we can plot the same data on a map:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotMap}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Embs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-19-1.pdf}

Note that this only works if \texttt{IndData\$UnitCode} correspond to ISO alpha-3 country codes. If you want to do some more sophisticated mapping R, Plotly has \href{https://plotly.com/r/maps/}{many mapping options}, but R in general has all kinds of mapping packages, you just have to search for them. COINr uses Plotly maps to keep things simple and to not depend on too many packages.

COINr has yet more tools to plot data, but let's leave it at that for the moment. Other tools will be introduced in other chapters.

\hypertarget{statistics-and-analysis}{%
\section{Statistics and analysis}\label{statistics-and-analysis}}

Aside from plots, COINr gives a fairly detailed statistical analysis of initial indicator data. The function \texttt{getStats()} returns a series of statistics which can be aimed at any of the data sets in the \texttt{.\$Data} folder. You can also specify if you want the output to be returned back to the COIN, or to a separate list.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get stats}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getStats}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{out2 =} \StringTok{"COIN"}\NormalTok{)}
\DocumentationTok{\#\# Number of collinear indicators =  3}
\DocumentationTok{\#\# Number of signficant negative indicator correlations =  322}
\DocumentationTok{\#\# Number of indicators with high denominator correlations =  7}

\CommentTok{\# display in table using Reactable}
\CommentTok{\# (note the use of helper function roundDF() to round the values to a sensible number of decimals)}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{StatTable }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{roundDF}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{reactable}\SpecialCharTok{::}\FunctionTok{reactable}\NormalTok{(}\AttributeTok{resizable =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{bordered =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{highlight =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{defaultPageSize =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-20-1.pdf}

The columns of this table give all kind of information from max, min, standard deviation, etc, to the presence of outliers and amount of missing data.

Apart from the overall statistics for each indicator, \texttt{getStats} also returns a few other things:

\begin{itemize}
\tightlist
\item
  \texttt{.\$Outliers}, which flags individual outlying points using the relation to the interquartile range
\item
  \texttt{.\$Correlations}, which gives a correlation matrix between all indicators in the data set
\item
  \texttt{.\$DenomCorrelations}, which gives the correlations between indicators and any denominators
\end{itemize}

Each of these aspects will be explained in more detail in later chapters (\textbf{to add which}), so for the moment it is enough to mention that they exist.

\hypertarget{multivariate-analysis}{%
\chapter{Multivariate analysis}\label{multivariate-analysis}}

Correlations, and other relationships between indicators, can help to understand the structure of the data and to see whether indicators are redundant or are mistakenly encoded.

\begin{itemize}
\tightlist
\item
  Correlations
\item
  PCA
\item
  Cronbach
\end{itemize}

To finish.

COINr uses pairwise correlation.

\hypertarget{missing-data-and-imputation}{%
\chapter{Missing data and Imputation}\label{missing-data-and-imputation}}

Imputation is the process of estimating missing data points. This can be done in any number of ways, and as usual, the ``best'' way depends on the problem.

Of course, you don't \emph{have} to impute data. You can also simply delete any indicator or unit that has missing values, although in many cases this can be too restrictive. Reasonable results can still be obtained despite small amounts of missing data, although if too much data is missing, the uncertainty can be too high to give a meaningful analysis. As usual, it is a balance.

A good first step is to check how much data is missing, and where (see below). Units with very high amounts of missing data can be screened out. Small amounts of missing data can then be imputed.

\hypertarget{concept}{%
\section{Concept}\label{concept}}

The simplest imputation approach is to use values of other units to estimate the missing point. Typically, this could involve the sample mean or median of the indicator. Here's some data from the ASEM data set regarding the average connection speed of each country. Towards the end there are some missing values, so let's view the last few rows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}
\NormalTok{Ind1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Country =}\NormalTok{ ASEMIndData}\SpecialCharTok{$}\NormalTok{UnitName, }\AttributeTok{ConSpeed =}\NormalTok{ ASEMIndData}\SpecialCharTok{$}\NormalTok{ConSpeed)}
\NormalTok{Ind1[}\DecValTok{40}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(Ind1),]}
\DocumentationTok{\#\#               Country ConSpeed}
\DocumentationTok{\#\# 40              Korea     28.6}
\DocumentationTok{\#\# 41            Lao PDR       NA}
\DocumentationTok{\#\# 42           Malaysia      8.9}
\DocumentationTok{\#\# 43           Mongolia       NA}
\DocumentationTok{\#\# 44            Myanmar       NA}
\DocumentationTok{\#\# 45        New Zealand     14.7}
\DocumentationTok{\#\# 46           Pakistan       NA}
\DocumentationTok{\#\# 47        Philippines      5.5}
\DocumentationTok{\#\# 48 Russian Federation     11.8}
\DocumentationTok{\#\# 49          Singapore     20.3}
\DocumentationTok{\#\# 50           Thailand     16.0}
\DocumentationTok{\#\# 51            Vietnam      9.5}
\end{Highlighting}
\end{Shaded}

Using our simple imputation method, we just replace the \texttt{NA} values with the sample mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ind1}\SpecialCharTok{$}\NormalTok{ConSpeed }\OtherTok{\textless{}{-}} \FunctionTok{replace}\NormalTok{(Ind1}\SpecialCharTok{$}\NormalTok{ConSpeed, }\FunctionTok{is.na}\NormalTok{(Ind1}\SpecialCharTok{$}\NormalTok{ConSpeed), }\FunctionTok{mean}\NormalTok{(Ind1}\SpecialCharTok{$}\NormalTok{ConSpeed, }\AttributeTok{na.rm =}\NormalTok{ T))}
\NormalTok{Ind1[}\DecValTok{40}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(Ind1),]}
\DocumentationTok{\#\#               Country ConSpeed}
\DocumentationTok{\#\# 40              Korea 28.60000}
\DocumentationTok{\#\# 41            Lao PDR 14.28605}
\DocumentationTok{\#\# 42           Malaysia  8.90000}
\DocumentationTok{\#\# 43           Mongolia 14.28605}
\DocumentationTok{\#\# 44            Myanmar 14.28605}
\DocumentationTok{\#\# 45        New Zealand 14.70000}
\DocumentationTok{\#\# 46           Pakistan 14.28605}
\DocumentationTok{\#\# 47        Philippines  5.50000}
\DocumentationTok{\#\# 48 Russian Federation 11.80000}
\DocumentationTok{\#\# 49          Singapore 20.30000}
\DocumentationTok{\#\# 50           Thailand 16.00000}
\DocumentationTok{\#\# 51            Vietnam  9.50000}
\end{Highlighting}
\end{Shaded}

This approach can be reasonable if countries are somehow similar in that indicator. However, other perhaps more informative ways are available:

\begin{itemize}
\tightlist
\item
  Substituting by the mean or median of the country group (e.g.~income group or continent)
\item
  If time series data is available, use the latest known data point
\end{itemize}

Then there is another class of methods which use data from other indicators to estimate the missing point. The core idea here is that if the indicators are related to one another, it is possible to guess the missing point by using known values of other indicators. This can take the form of:

\begin{itemize}
\tightlist
\item
  Simply substituting the mean or median of the \emph{normalised} values of the other indicators
\item
  Subsituting the mean or median of \emph{normalised} values of the other indicators \emph{within the aggregation group}
\item
  Using a more formal approach, based on regression or more generally on statistical modelling
\end{itemize}

Let's explore the options available in COINr

\hypertarget{data-checks-and-screening}{%
\section{Data checks and screening}\label{data-checks-and-screening}}

A first step is to check in detail how much data missing. The function \texttt{checkData()} does this, and also has the option to screen units based on data availability.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assemble ASEM data first}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndData, }\AttributeTok{IndMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndMeta,}
                 \AttributeTok{AggMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMAggMeta)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Missing data check on the raw data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{checkData}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{MissDatSummary)}
\DocumentationTok{\#\#   UnitCode N\_missing N\_zero N\_miss\_or\_zero PrcDataAll PrcNonZero LowDataAll}
\DocumentationTok{\#\# 1      AUT         0      2              2        100   95.91837      FALSE}
\DocumentationTok{\#\# 2      BEL         0      2              2        100   95.91837      FALSE}
\DocumentationTok{\#\# 3      BGR         0      0              0        100  100.00000      FALSE}
\DocumentationTok{\#\# 4      HRV         0      1              1        100   97.95918      FALSE}
\DocumentationTok{\#\# 5      CYP         0      3              3        100   93.87755      FALSE}
\DocumentationTok{\#\# 6      CZE         0      3              3        100   93.87755      FALSE}
\DocumentationTok{\#\#   ZeroFlag LowDatOrZeroFlag Included}
\DocumentationTok{\#\# 1    FALSE            FALSE     TRUE}
\DocumentationTok{\#\# 2    FALSE            FALSE     TRUE}
\DocumentationTok{\#\# 3    FALSE            FALSE     TRUE}
\DocumentationTok{\#\# 4    FALSE            FALSE     TRUE}
\DocumentationTok{\#\# 5    FALSE            FALSE     TRUE}
\DocumentationTok{\#\# 6    FALSE            FALSE     TRUE}
\end{Highlighting}
\end{Shaded}

\emph{Note: to include updated checkData() function with screening for zeros.}

The \texttt{MissDataSummary} table shows the unit code, number of missing observations, and overall percentage data availability. Finally, the \texttt{LowDataAll} column can be used as a flag for units with data availability lower than a set amount \texttt{ind\_thresh} which is one of the input arguments to \texttt{checkData()}. ASEM indicators were already chosen to fulfill minimum data requirements, for which reason the data availability is all above the default of 2/3.

We will set the minimum data threshold a bit higher, and in doing so also demonstrate another feature of \texttt{checkData()}, which is to automatically screen units based on data availability. Setting \texttt{unit\_screen\ =\ TRUE} will generate a new data set \texttt{.\$Data\$Screened} which only includes units with data availability above the set threshold. This data set can then be passed on to subsequent operations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Missing data check on the raw data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{checkData}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{ind\_thresh =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{unit\_screen =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{RemovedUnits}
\DocumentationTok{\#\# character(0)}
\end{Highlighting}
\end{Shaded}

This generates a new data set \texttt{.\$Data\$Screened} with the removed units recorded in \texttt{.\$Analysis\$Raw\$RemovedUnits}, which in this case are Brunei, Laos and Myanmar. As a final option to mention, you can manually include or exclude units. So for example, a unit that doesn't have sufficient data coverage could be manually included anyway, and another unit could be excluded.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Countries to include and exclude}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{UnitCode =} \FunctionTok{c}\NormalTok{(}\StringTok{"AUT"}\NormalTok{,}\StringTok{"BRN"}\NormalTok{), }\AttributeTok{Status =} \FunctionTok{c}\NormalTok{(}\StringTok{"Exclude"}\NormalTok{, }\StringTok{"Include"}\NormalTok{))}

\CommentTok{\# Missing data check on the raw data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{checkData}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{ind\_thresh =} \FloatTok{0.85}\NormalTok{, }\AttributeTok{unit\_screen =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{Force =}\NormalTok{ df)}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{RemovedUnits}
\DocumentationTok{\#\# [1] "AUT"}
\end{Highlighting}
\end{Shaded}

Now AUT has been excluded, even though it had sufficient data coverage, and BRN has been manually included, even though it didn't. The intention is to give some flexibility to make exceptions for hard rules.

The other output of interest from \texttt{checkData()} is missing data by group:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{MissDatByGroup[}\DecValTok{30}\SpecialCharTok{:}\DecValTok{40}\NormalTok{,])}
\DocumentationTok{\#\#    UnitCode ConEcFin    Instit P2P Physical Political Environ    Social}
\DocumentationTok{\#\# 30      GBR      100 100.00000 100    100.0       100     100 100.00000}
\DocumentationTok{\#\# 31      AUS      100 100.00000 100    100.0       100     100 100.00000}
\DocumentationTok{\#\# 32      BGD      100  83.33333  75     87.5       100     100  88.88889}
\DocumentationTok{\#\# 33      BRN       80 100.00000  75     87.5       100     100  55.55556}
\DocumentationTok{\#\# 34      KHM       80 100.00000  75     87.5       100     100  77.77778}
\DocumentationTok{\#\# 35      CHN      100 100.00000 100    100.0       100     100 100.00000}
\DocumentationTok{\#\#    SusEcFin      Conn      Sust     Index}
\DocumentationTok{\#\# 30      100 100.00000 100.00000 100.00000}
\DocumentationTok{\#\# 31      100 100.00000 100.00000 100.00000}
\DocumentationTok{\#\# 32       80  86.66667  89.47368  87.75510}
\DocumentationTok{\#\# 33       60  86.66667  68.42105  79.59184}
\DocumentationTok{\#\# 34      100  86.66667  89.47368  87.75510}
\DocumentationTok{\#\# 35       80 100.00000  94.73684  97.95918}
\end{Highlighting}
\end{Shaded}

This gives the percentage indicator data availability inside each aggregation group.

\hypertarget{imputation-in-coinr}{%
\section{Imputation in COINr}\label{imputation-in-coinr}}

Now we turn to estimating missing data points in COINr. The function of interest is \texttt{impute()}, which gives a number of options, corresponding to the types of imputation discussed above. Briefly, COINr can impute using:

\begin{itemize}
\tightlist
\item
  The indicator mean, either across all countries or within a specified group
\item
  The indicator median, either across all countries or within a specified group
\item
  The mean or median, within the aggregation group
\item
  The expectation maximisation algorithm, via the \href{https://cran.r-project.org/web/packages/Amelia/index.html}{AMELIA} package
\end{itemize}

\hypertarget{by-column}{%
\subsection{By column}\label{by-column}}

The \texttt{impute()} function follows a similar logic to other COINr functions. We can enter a COIN or a data frame, and get a COIN or a data frame back. Let's impute on a COIN first, using the raw ASEM data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assemble ASEM data if not done already (uncomment the following)}
\CommentTok{\# ASEM \textless{}{-} assemble(IndData = COINr::ASEMIndData, IndMeta = COINr::ASEMIndMeta,}
\CommentTok{\#                 AggMeta = COINr::ASEMAggMeta)}

\CommentTok{\# check how many NAs we have}
\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Raw))}
\DocumentationTok{\#\# [1] 63}

\CommentTok{\# impute using indicator mean}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{imtype =} \StringTok{"ind\_mean"}\NormalTok{)}
\DocumentationTok{\#\# Missing data points detected = 63}
\DocumentationTok{\#\# Missing data points imputed = 63, using method = ind\_mean}

\CommentTok{\# check how many NAs after imputation}
\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Imputed))}
\DocumentationTok{\#\# [1] 0}
\end{Highlighting}
\end{Shaded}

If the input is a COIN, by default the output is an updated COIN, with a new data set \texttt{.\$Data\$Imputed}. We can also set \texttt{out2\ =\ "df"} to output the imputed data frame on its own.

The above example replaces \texttt{NA} values with the mean of the indicator, and setting \texttt{imtype\ =\ ind\_median} will instead use the indicator median. In this respect, we are using information from other units, inside the same indicator, to estimate the missing values.

Imputing by group may be more informative if you have meaningful grouping variables. For example, the ASEM data set has groupings by GDP, population, GDP per capita and whether the country is European or Asian. We might hypothesise that it is better to replace \texttt{NA} values with the median inside a group, say by GDP group. This would imply that countries with a similar GDP are likely to have similar indicator values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# impute using GDP group median}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{imtype =} \StringTok{"indgroup\_median"}\NormalTok{, }\AttributeTok{groupvar =} \StringTok{"Group\_GDP"}\NormalTok{)}
\DocumentationTok{\#\# Missing data points detected = 63}
\DocumentationTok{\#\# Missing data points imputed = 63, using method = indgroup\_median}
\end{Highlighting}
\end{Shaded}

\hypertarget{by-row}{%
\subsection{By row}\label{by-row}}

So far, the imputation has been using values from the same column (indicator). Another possibility is to use values from other indicators, i.e operate row-wise. COINr offers two basic options in this respect: either to take the mean or median of the other indicators inside the aggregation group, and this can be done by setting \texttt{imtype\ =\ "agg\_mean"} or \texttt{imtype\ =\ "agg\_median"} respectively.

Both of these imputation methods will, for a given unit, take the mean or median of the \emph{normalised} values of the other indicators in the aggregation group in the level above indicator level. The process is as follows, for each \texttt{NA} value:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find indicators in the same aggregation group
\item
  Normalise these indicators using the min-max method, so they each have a minimum of zero and a maximum of one.
\item
  Replace the \texttt{NA} with the mean or median of the normalised values, for the unit in question.
\item
  Reverse the min-max transformation so that the indicators are returned to their original scale.
\end{enumerate}

It's important to realise that this is equivalent to aggregating with the mean or median without imputing first. This is because in the aggregation step, if we take the mean of a group of indicators, and there is a \texttt{NA} present, this value is excluded from the mean calculation. Doing this is mathematically equivalent to assigning the mean to that missing value. Therefore, one reason to use this imputation method is to see which values are being implicitly assigned as a result of excluding missing values from the aggregation step. This is sometimes known as ``shadow imputation''.

\hypertarget{by-column-and-row}{%
\subsection{By column and row}\label{by-column-and-row}}

Finally, we can use a regression-based \emph{expectation maximisation} algorithm, imported via the \texttt{AMELIA} package. This estimates missing values using the values of the other indicators \emph{and} the other units. This effectively involves regressing each indicator on other indicators, and predicting missing values.

A catch of this approach is that if the number of units is small compared to the number of indicators, there will not be enough observations to estimate the parameters of the model. This is definitely the case if you have more indicators than observations. To use the EM algorithm in this case, imputation is performed on aggregation groups. So, to use the EM approach in \texttt{COINr}, you have to specify the aggregation level to group indicators by.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# impute using EM, grouping indicators inside their pillars (level 2)}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(ASEM, }\AttributeTok{imtype =} \StringTok{"EM"}\NormalTok{, }\AttributeTok{EMaglev =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# Missing data points detected = 63}
\DocumentationTok{\#\# Missing data points imputed = 63, using method = EM}
\end{Highlighting}
\end{Shaded}

This works, because the ASEM data set has 51 units and no more thann eight indicators per pillar. However, if we were to use \texttt{EMaglev\ =\ 3}, i.e.~try imputing indicators grouped into the two sub-indexes, it does not work.

Finding the right aggregation level to impute at might involve a bit of trial and error, but the advantage of imputing by groups of indicators is that aggregation groups are usually composed of indicators that are similar in some respect. This makes it more likely that they are good predictors of the other indicators in their group.

The \texttt{AMELIA} package offers many more options for imputation that are not available through the \texttt{impute()} function, such as bootstrapping, time-series imputation and more. As with plotting packages \texttt{COINr} aims to provide an easy and quick interface to standard imputation in \texttt{AMELIA}, but if you want to have full control you should use the \texttt{AMELIA} package directly. Other good imputation options are the \href{https://cran.r-project.org/web/packages/mice/index.html}{\texttt{MICE}} package (Multivariate Imputation via Chained Equations) and the \href{https://cran.rstudio.com/web/packages/missForest/index.html}{\texttt{missForest}} package (using random forests).

\hypertarget{denomination}{%
\chapter{Denomination}\label{denomination}}

The first section here gives a bit of introduction to the concept of denomination. If you just want to know how to denominate in COINr, skip to the section on \protect\hyperlink{denominating-in-coinr}{Denominating in COINr}.

\hypertarget{concept-1}{%
\section{Concept}\label{concept-1}}

\emph{Denomination} refers to the operation of dividing one indicator by another. But why should we do that?

As anyone who has ever looked at a map will know, countries come in all different sizes. The problem is that many indicators, on their own, are strongly linked to the size of the country. That means that if we compare countries using these indicators directly, we will often get a ranking that has roughly the biggest countries at the top, and the smallest countries at the bottom.

To take an example, let's examine some indicators in the ASEM data set, and introduce another plotting function in the process.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load COINr package}
\FunctionTok{library}\NormalTok{(COINr)}
\FunctionTok{library}\NormalTok{(magrittr) }\CommentTok{\# for pipe operations}
\CommentTok{\# build ASEM index}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}
\CommentTok{\# plot international trade in goods against GDP}
\FunctionTok{iplotIndDist2}\NormalTok{(ASEM, }\AttributeTok{dsets =} \FunctionTok{c}\NormalTok{(}\StringTok{"Denominators"}\NormalTok{, }\StringTok{"Raw"}\NormalTok{), }\AttributeTok{icodes =} \FunctionTok{c}\NormalTok{(}\StringTok{"Den\_GDP"}\NormalTok{, }\StringTok{"Goods"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# (note: need to fix labelling and units of denominators)}
\end{Highlighting}
\end{Shaded}

The function \texttt{iplotIndDist2()} is similar to \texttt{iplotIndDist()} but allows plotting two indicators against each other. You can pick any indicator from any data set for each, including denominators.

What are these ``denominators'' anyway? Denominators are indicators that are used to scale other indicators, in order to remove the ``size effect''. Typically, they are those related to physical or economic size, including GDP, population, land area and so on.

Anyway, looking at the plot above, it is clear that that countries with a higher GDP have a higher international trade in international goods (e.g.~Germany, China, UK, Japan, France), which is not very surprising. The problem comes when we want to include this indicator as a measure of connectivity: on its own, trade in goods simply summarises having a large economy. What is more interesting, is to measure international trade \emph{per unit GDP}, and this is done by dividing (i.e.~\textbf{denominating}) the international trade of each country by its GDP. Let's do that manually here and see what we get.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# divide trade by GDP}
\NormalTok{tradeperGDP }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{Goods}\SpecialCharTok{/}\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Input}\SpecialCharTok{$}\NormalTok{Denominators}\SpecialCharTok{$}\NormalTok{Den\_GDP}
\CommentTok{\# bar chart: add unit names first}
\FunctionTok{iplotBar}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{UnitCode =}\NormalTok{ ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{UnitCode, }\AttributeTok{TradePerGDP =}\NormalTok{ tradeperGDP))}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-31-1.pdf}

Now the picture is completely different - small countries like Slovakia, Czech Republic and Singapore have the highest values.

The rankings here are completely different because the meanings of these two measures are completely different. Denomination is in fact a nonlinear transformation, because every value is divided by a different value (each country is divided by its own unique GDP, in this case). That doesn't mean that denominated indicators are suddenly more ``right'' than the before their denomination, however. Trade per GDP is a useful measure of how much a country's economy is dependent on international trade, but in terms of economic power, it might not be meaningful to scale by GDP. In summary, it is important to consider the meaning of the indicator compared to what you want to actually measure.

More precisely, indicators can be thought of as either \emph{intensive} or \emph{extensive} variables. Intensive variables are not (or only weakly) related to the size of the country, and allow ``fair'' comparisons between countries of different sizes. Extensive variables, on the contrary, are strongly related to the size of the country.

This distinction is well known in physics, for example. Mass is related to the size of the object and is an extensive variable. If we take a block of steel, and double its size (volume), we also double its mass. Density, which is mass per unit volume, is an intensive quantity: if we double the size of the block, the density remains the same.

\begin{itemize}
\tightlist
\item
  An example of an extensive variable is population. Bigger countries tend to have bigger populations.
\item
  An example of an intensive variable is population density. This is no longer dependent on the physical size of the country.
\end{itemize}

The summary here is that an \textbf{extensive} variable becomes an \textbf{intensive} variable when we divide it by a \textbf{denominator}.

An important point to make here is about ordering. In the example above, we have simply divided two data frames by one another. To get the right result, we need to make sure that the units (rows) match properly in both data frames, as well as the columns. The example above is correct because the denominator data and indicator data originally came from the same data frame (\texttt{ASEMIndData}). However normally it would be better to use R's \texttt{match()} or \texttt{merge()} functions, or else similar tidyverse equivalents, to ensure no errors arise. In COINr, this ordering problem is automatically taken care of.

\hypertarget{denominating-in-coinr}{%
\section{Denominating in COINr}\label{denominating-in-coinr}}

Denomination is fairly simple to do in R, it's just dividing one vector by another. Nevertheless, COINr has a dedicated function for denominating which makes life easier and helps you to track what you have done.

Before we get to that, it's worth mentioning that COINr has a few built-in denominators sourced from the World Bank. It looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WorldDenoms}
\DocumentationTok{\#\# \# A tibble: 249 x 7}
\DocumentationTok{\#\#    UnitName      UnitCode    Den\_GDP Den\_Pop Den\_Area Den\_GDPpc Den\_IncomeGroup }
\DocumentationTok{\#\#    \textless{}chr\textgreater{}         \textless{}chr\textgreater{}         \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}chr\textgreater{}           }
\DocumentationTok{\#\#  1 Afghanistan   AFG         1.93e10  3.80e7   652860      507. Low income      }
\DocumentationTok{\#\#  2 Albania       ALB         1.53e10  2.85e6    27400     5353. Upper middle in\textasciitilde{}}
\DocumentationTok{\#\#  3 Algeria       DZA         1.71e11  4.31e7  2381740     3974. Lower middle in\textasciitilde{}}
\DocumentationTok{\#\#  4 American Sam\textasciitilde{} ASM         6.36e 8  5.53e4      200    11467. Upper middle in\textasciitilde{}}
\DocumentationTok{\#\#  5 Andorra       AND         3.15e 9  7.71e4      470    40886. High income     }
\DocumentationTok{\#\#  6 Angola        AGO         8.88e10  3.18e7  1246700     2791. Lower middle in\textasciitilde{}}
\DocumentationTok{\#\#  7 Anguilla      AIA        NA       NA            NA       NA  \textless{}NA\textgreater{}            }
\DocumentationTok{\#\#  8 Antarctica    ATA        NA       NA            NA       NA  \textless{}NA\textgreater{}            }
\DocumentationTok{\#\#  9 Antigua and \textasciitilde{} ATG         1.66e 9  9.71e4      440    17113. High income     }
\DocumentationTok{\#\# 10 Argentina     ARG         4.45e11  4.49e7  2736690     9912. Upper middle in\textasciitilde{}}
\DocumentationTok{\#\# \# ... with 239 more rows}
\end{Highlighting}
\end{Shaded}

and the metadata can be found by calling \texttt{?WorldDenoms}. Data here is the latest available as of February 2021 and I would recommend using these only for exploration, then updating your data yourself.

To denominate your indicators in COINr, the function to call is \texttt{denominate()}. Like other COINr functions, this can be used either independently on a data frame of indicators, or on COINs. Consider that in all cases you need three main ingredients:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Some indicator data that should be denominated
\item
  Some other indicators to use as denominators
\item
  A mapping to say which denominators (if any) to use for each indicator.
\end{enumerate}

\hypertarget{on-coins}{%
\subsection{On COINs}\label{on-coins}}

If you are working with a COIN, the indicator data will be present in the \texttt{.\$Data} folder. If you specified any denominators in \texttt{IndData} (i.e.~columns beginning with ``Den\_'') when calling \texttt{assemble()} you will also find them in \texttt{.\$Input\$Denominators}. Finally, if you specified a \texttt{Denominator} column in \texttt{IndMeta} when calling \texttt{assemble()} then the mapping of denominators to indicators will also be present.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The raw indicator data which will be denominated}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Raw}
\DocumentationTok{\#\# \# A tibble: 51 x 56}
\DocumentationTok{\#\#    UnitCode UnitName  Year Group\_GDP Group\_GDPpc Group\_Pop Group\_EurAsia  Goods}
\DocumentationTok{\#\#    \textless{}chr\textgreater{}    \textless{}chr\textgreater{}    \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}chr\textgreater{}       \textless{}chr\textgreater{}     \textless{}chr\textgreater{}          \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1 AUT      Austria   2018 L         XL          M         Europe        278.  }
\DocumentationTok{\#\#  2 BEL      Belgium   2018 L         L           L         Europe        598.  }
\DocumentationTok{\#\#  3 BGR      Bulgaria  2018 S         S           M         Europe         42.8 }
\DocumentationTok{\#\#  4 HRV      Croatia   2018 S         M           S         Europe         28.4 }
\DocumentationTok{\#\#  5 CYP      Cyprus    2018 S         L           S         Europe          8.77}
\DocumentationTok{\#\#  6 CZE      Czech R\textasciitilde{}  2018 M         L           M         Europe        274.  }
\DocumentationTok{\#\#  7 DNK      Denmark   2018 L         XL          M         Europe        147.  }
\DocumentationTok{\#\#  8 EST      Estonia   2018 S         M           S         Europe         28.2 }
\DocumentationTok{\#\#  9 FIN      Finland   2018 M         XL          M         Europe        102.  }
\DocumentationTok{\#\# 10 FRA      France    2018 XL        L           L         Europe        849.  }
\DocumentationTok{\#\# \# ... with 41 more rows, and 48 more variables: Services \textless{}dbl\textgreater{}, FDI \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   PRemit \textless{}dbl\textgreater{}, ForPort \textless{}dbl\textgreater{}, CostImpEx \textless{}dbl\textgreater{}, Tariff \textless{}dbl\textgreater{}, TBTs \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   TIRcon \textless{}dbl\textgreater{}, RTAs \textless{}dbl\textgreater{}, Visa \textless{}dbl\textgreater{}, StMob \textless{}dbl\textgreater{}, Research \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Pat \textless{}dbl\textgreater{}, CultServ \textless{}dbl\textgreater{}, CultGood \textless{}dbl\textgreater{}, Tourist \textless{}dbl\textgreater{}, MigStock \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Lang \textless{}dbl\textgreater{}, LPI \textless{}dbl\textgreater{}, Flights \textless{}dbl\textgreater{}, Ship \textless{}dbl\textgreater{}, Bord \textless{}dbl\textgreater{}, Elec \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Gas \textless{}dbl\textgreater{}, ConSpeed \textless{}dbl\textgreater{}, Cov4G \textless{}dbl\textgreater{}, Embs \textless{}dbl\textgreater{}, IGOs \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   UNVote \textless{}dbl\textgreater{}, Renew \textless{}dbl\textgreater{}, PrimEner \textless{}dbl\textgreater{}, CO2 \textless{}dbl\textgreater{}, MatCon \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Forest \textless{}dbl\textgreater{}, Poverty \textless{}dbl\textgreater{}, Palma \textless{}dbl\textgreater{}, TertGrad \textless{}dbl\textgreater{}, FreePress \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   TolMin \textless{}dbl\textgreater{}, NGOs \textless{}dbl\textgreater{}, CPI \textless{}dbl\textgreater{}, FemLab \textless{}dbl\textgreater{}, WomParl \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   PubDebt \textless{}dbl\textgreater{}, PrivDebt \textless{}dbl\textgreater{}, GDPGrow \textless{}dbl\textgreater{}, RDExp \textless{}dbl\textgreater{}, NEET \textless{}dbl\textgreater{}}
\CommentTok{\# The denominators}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Input}\SpecialCharTok{$}\NormalTok{Denominators}
\DocumentationTok{\#\# \# A tibble: 51 x 11}
\DocumentationTok{\#\#    UnitCode UnitName  Year Group\_GDP Group\_GDPpc Group\_Pop Group\_EurAsia}
\DocumentationTok{\#\#    \textless{}chr\textgreater{}    \textless{}chr\textgreater{}    \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}chr\textgreater{}       \textless{}chr\textgreater{}     \textless{}chr\textgreater{}        }
\DocumentationTok{\#\#  1 AUT      Austria   2018 L         XL          M         Europe       }
\DocumentationTok{\#\#  2 BEL      Belgium   2018 L         L           L         Europe       }
\DocumentationTok{\#\#  3 BGR      Bulgaria  2018 S         S           M         Europe       }
\DocumentationTok{\#\#  4 HRV      Croatia   2018 S         M           S         Europe       }
\DocumentationTok{\#\#  5 CYP      Cyprus    2018 S         L           S         Europe       }
\DocumentationTok{\#\#  6 CZE      Czech R\textasciitilde{}  2018 M         L           M         Europe       }
\DocumentationTok{\#\#  7 DNK      Denmark   2018 L         XL          M         Europe       }
\DocumentationTok{\#\#  8 EST      Estonia   2018 S         M           S         Europe       }
\DocumentationTok{\#\#  9 FIN      Finland   2018 M         XL          M         Europe       }
\DocumentationTok{\#\# 10 FRA      France    2018 XL        L           L         Europe       }
\DocumentationTok{\#\# \# ... with 41 more rows, and 4 more variables: Den\_Area \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Den\_Energy \textless{}dbl\textgreater{}, Den\_GDP \textless{}dbl\textgreater{}, Den\_Pop \textless{}dbl\textgreater{}}
\CommentTok{\# The mapping of denominators to indicators (see Denominator column)}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Input}\SpecialCharTok{$}\NormalTok{IndMeta}
\DocumentationTok{\#\# \# A tibble: 49 x 10}
\DocumentationTok{\#\#    IndName IndCode Direction IndWeight Denominator IndUnit  Target Agg1  Agg2 }
\DocumentationTok{\#\#    \textless{}chr\textgreater{}   \textless{}chr\textgreater{}       \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}chr\textgreater{}       \textless{}chr\textgreater{}     \textless{}dbl\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\DocumentationTok{\#\#  1 Trade \textasciitilde{} Goods           1         1 Den\_GDP     Trilli\textasciitilde{} 1.82e+3 ConE\textasciitilde{} Conn }
\DocumentationTok{\#\#  2 Trade \textasciitilde{} Servic\textasciitilde{}         1         1 Den\_GDP     Millio\textasciitilde{} 6.24e+2 ConE\textasciitilde{} Conn }
\DocumentationTok{\#\#  3 Foreig\textasciitilde{} FDI             1         1 Den\_GDP     Billio\textasciitilde{} 7.18e+1 ConE\textasciitilde{} Conn }
\DocumentationTok{\#\#  4 Person\textasciitilde{} PRemit          1         1 Den\_GDP     Millio\textasciitilde{} 2.87e+1 ConE\textasciitilde{} Conn }
\DocumentationTok{\#\#  5 Foreig\textasciitilde{} ForPort         1         1 Den\_GDP     Millio\textasciitilde{} 1.01e+4 ConE\textasciitilde{} Conn }
\DocumentationTok{\#\#  6 Cost t\textasciitilde{} CostIm\textasciitilde{}        {-}1         1 \textless{}NA\textgreater{}        Curren\textasciitilde{} 4.96e+1 Inst\textasciitilde{} Conn }
\DocumentationTok{\#\#  7 Mean t\textasciitilde{} Tariff         {-}1         1 \textless{}NA\textgreater{}        Percent 5.26e{-}1 Inst\textasciitilde{} Conn }
\DocumentationTok{\#\#  8 Techni\textasciitilde{} TBTs           {-}1         1 \textless{}NA\textgreater{}        Number\textasciitilde{} 8.86e+1 Inst\textasciitilde{} Conn }
\DocumentationTok{\#\#  9 Signat\textasciitilde{} TIRcon          1         1 \textless{}NA\textgreater{}        (1 (ye\textasciitilde{} 9.50e{-}1 Inst\textasciitilde{} Conn }
\DocumentationTok{\#\# 10 Region\textasciitilde{} RTAs            1         1 \textless{}NA\textgreater{}        Number\textasciitilde{} 4.38e+1 Inst\textasciitilde{} Conn }
\DocumentationTok{\#\# \# ... with 39 more rows, and 1 more variable: Agg3 \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

COINr's \texttt{denominate()} function knows where to look for each of these ingredients, so we can simply call:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

which will return a new data set \texttt{.Data\$Denominated}. To return the dataset directly, rather than outputting an updated COIN, you can also set \texttt{out2\ =\ "df"} (this is a common argument to many functions which can be useful if you want to examine the result directly).

You can also change which indicators are denominated, and by what.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get denominator specification from metadata}
\NormalTok{denomby\_meta }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}\SpecialCharTok{$}\NormalTok{Input}\SpecialCharTok{$}\NormalTok{IndMeta}\SpecialCharTok{$}\NormalTok{Denominator}
\CommentTok{\# Example: we want to change the denominator of flights from population to GDP}
\NormalTok{denomby\_meta[ASEM}\SpecialCharTok{$}\NormalTok{Input}\SpecialCharTok{$}\NormalTok{IndMeta}\SpecialCharTok{$}\NormalTok{IndCode }\SpecialCharTok{==} \StringTok{"Flights"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Den\_GDP"}
\CommentTok{\# Now re{-}denominate. Return data frame for inspection}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{specby =} \StringTok{"user"}\NormalTok{, }\AttributeTok{denomby =}\NormalTok{ denomby\_meta)}
\end{Highlighting}
\end{Shaded}

Here we have changed the denominator of one of the indicators, ``Flights'', to GDP. This is done by creating a character vector \texttt{denomby\_meta} (copied from the original denominator specification) which has an entry for each indicator, specifying which denominator to use, if any. We then changed the entry corresponding to ``Flights''. This overwrites any previous denomination. If you want to keep and compare alternative specifications, see the chapter on \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons}.

Let's compare the raw Flights data with the Flights per GDP data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot raw flights against denominated}
\FunctionTok{iplotIndDist2}\NormalTok{(ASEM, }\AttributeTok{dsets =} \FunctionTok{c}\NormalTok{(}\StringTok{"Raw"}\NormalTok{, }\StringTok{"Denominated"}\NormalTok{), }\AttributeTok{icodes =} \StringTok{"Flights"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-36-1.pdf}

Clearly, the denominated and raw indicators are very different from one another, reflecting the completely different meaning.

\hypertarget{on-data-frames}{%
\subsection{On data frames}\label{on-data-frames}}

If you are just working with data frames, you need to supply the three ingredients directly to the function. Here we will take some of the ASEM data for illustration (recalling that both indicator and denominator data is specified in \texttt{ASEMIndMeta}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# a small data frame of indicator data}
\NormalTok{IndData }\OtherTok{\textless{}{-}}\NormalTok{ ASEMIndData[}\FunctionTok{c}\NormalTok{(}\StringTok{"UnitCode"}\NormalTok{, }\StringTok{"Goods"}\NormalTok{, }\StringTok{"Services"}\NormalTok{, }\StringTok{"FDI"}\NormalTok{)]}
\NormalTok{IndData}
\DocumentationTok{\#\# \# A tibble: 51 x 4}
\DocumentationTok{\#\#    UnitCode  Goods Services    FDI}
\DocumentationTok{\#\#    \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1 AUT      278.      108.   5    }
\DocumentationTok{\#\#  2 BEL      598.      216.   5.71 }
\DocumentationTok{\#\#  3 BGR       42.8      13.0  1.35 }
\DocumentationTok{\#\#  4 HRV       28.4      17.4  0.387}
\DocumentationTok{\#\#  5 CYP        8.77     15.2  1.23 }
\DocumentationTok{\#\#  6 CZE      274.       43.5  3.88 }
\DocumentationTok{\#\#  7 DNK      147.      114.   9.1  }
\DocumentationTok{\#\#  8 EST       28.2      10.2  0.580}
\DocumentationTok{\#\#  9 FIN      102.       53.8  6.03 }
\DocumentationTok{\#\# 10 FRA      849.      471.  30.9  }
\DocumentationTok{\#\# \# ... with 41 more rows}
\CommentTok{\# two selected denominators}
\NormalTok{Denoms }\OtherTok{\textless{}{-}}\NormalTok{ ASEMIndData[}\FunctionTok{c}\NormalTok{(}\StringTok{"UnitCode"}\NormalTok{, }\StringTok{"Den\_Pop"}\NormalTok{, }\StringTok{"Den\_GDP"}\NormalTok{)]}

\CommentTok{\# denominate the data}
\NormalTok{IndDataDenom }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(IndData, }\AttributeTok{denomby =} \FunctionTok{c}\NormalTok{(}\StringTok{"Den\_GDP"}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\StringTok{"Den\_Pop"}\NormalTok{), }\AttributeTok{denominators =}\NormalTok{ Denoms)}
\NormalTok{IndDataDenom}
\DocumentationTok{\#\# \# A tibble: 51 x 4}
\DocumentationTok{\#\#    UnitCode Goods Services       FDI}
\DocumentationTok{\#\#    \textless{}chr\textgreater{}    \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1 AUT      0.712    108.  0.000572 }
\DocumentationTok{\#\#  2 BEL      1.28     216.  0.000500 }
\DocumentationTok{\#\#  3 BGR      0.804     13.0 0.000191 }
\DocumentationTok{\#\#  4 HRV      0.554     17.4 0.0000924}
\DocumentationTok{\#\#  5 CYP      0.437     15.2 0.00104  }
\DocumentationTok{\#\#  6 CZE      1.40      43.5 0.000365 }
\DocumentationTok{\#\#  7 DNK      0.478    114.  0.00159  }
\DocumentationTok{\#\#  8 EST      1.21      10.2 0.000443 }
\DocumentationTok{\#\#  9 FIN      0.426     53.8 0.00109  }
\DocumentationTok{\#\# 10 FRA      0.344    471.  0.000476 }
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

Since the input is recognised as a data frame, you don't need to specify any other arguments, and the output is automatically a data frame. Note how \texttt{denomby} works: here it specifies that that ``Goods'' should be denominated by ``Den\_GDP'', that ``Services'' should not be denominated, and that ``FDI'' should be denominated by ``Den\_Pop''.

\hypertarget{when-to-denominate-and-by-what}{%
\section{When to denominate, and by what?}\label{when-to-denominate-and-by-what}}

Denomination is mathematically very simple, but from a conceptual point of view it needs to be handled with care. As we have shown, denominating an indicator will usually completely change it, and will have a corresponding impact on the results.

Two ways of looking at the problem are first, from the conceptual point of view. Consider each indicator and whether it fits with the aim of your index. Some indicators are anyway intensive, such as the percentage of tertiary graduates. Others, such as trade, will be strongly linked to the size of the country. In those cases, consider whether countries with high trade values should have higher scores in your index or not? Or should it be trade as a percentage of GDP? Or trade per capita? Each of these will have different meanings.

Sometimes extensive variables will anyway be the right choice. The \href{https://power.lowyinstitute.org/}{Lowy Asia Power Index} measures the power of each country in an absolute sense: in this case, the size of the country is all-important and e.g.~trade or military capabilities per capita would not make much sense.

The second (complimentary) way to approach denomination is from a statistical point of view. We can check which indicators are strongly related to the size of a country by correlating them with some typical denominators, such as the ones used here. The \texttt{getStats()} function does just this, checking the correlations between each indicator and each denominator, and flagging any possible high correlations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get statistics on raw data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getStats}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{out2 =} \StringTok{"COIN"}\NormalTok{, }\AttributeTok{t\_denom =} \FloatTok{0.8}\NormalTok{)}
\DocumentationTok{\#\# Number of collinear indicators =  3}
\DocumentationTok{\#\# Number of signficant negative indicator correlations =  322}
\DocumentationTok{\#\# Number of indicators with high denominator correlations =  4}
\CommentTok{\# get the result}
\NormalTok{ctable }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{DenomCorrelations}
\CommentTok{\# remove first column}
\NormalTok{ctable }\OtherTok{\textless{}{-}}\NormalTok{ ctable[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\# return only columns that have entries with correlation above 0.8}
\NormalTok{ctable[,}\FunctionTok{colSums}\NormalTok{((}\FunctionTok{abs}\NormalTok{(ctable) }\SpecialCharTok{\textgreater{}} \FloatTok{0.8}\NormalTok{))}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{]}
\DocumentationTok{\#\#       Goods       FDI     StMob  CultGood}
\DocumentationTok{\#\# 1 0.2473558 0.3668625 0.5228032 0.2696675}
\DocumentationTok{\#\# 2 0.6276219 0.7405170 0.7345030 0.7120445}
\DocumentationTok{\#\# 3 0.8026770 0.8482453 0.8379575 0.8510374}
\DocumentationTok{\#\# 4 0.4158811 0.6529376 0.5523545 0.5028121}
\end{Highlighting}
\end{Shaded}

The matrix that is displayed above only includes columns where there is a correlation value (between an indicator and any denominator) of greater than 0.8. The results show some interesting patterns:

\begin{itemize}
\tightlist
\item
  Many indicators have a high positive correlation with GDP, including flights, trade, foreign direct investment (very high), personal remittances, research, stock of migrants, and so on.
\item
  Bigger countries, in terms of land area, tend to have \emph{less} trade agreements (RTAs) and a more restrictive visa policy
\item
  Larger populations are associated with higher levels of poverty
\end{itemize}

The purpose here is not to draw causal links between these quantities, although they might reveal interesting patterns. Rather, these might suggest which quantities to denominate by, if the choices also work on a conceptual level.

\hypertarget{data-treatment}{%
\chapter{Data Treatment}\label{data-treatment}}

Data treatment is the process of altering indicators to improve their statistical properties, mainly for the purposes of aggregation.

Data treatment is a delicate subject, because it essentially involves changing the values of certain observations, or transforming an entire distribution. This entails balancing two opposing considerations:

\begin{itemize}
\tightlist
\item
  On the one hand, treatment should be used as sparingly as possible, because you are altering one or more known data points.
\item
  On the other hand, \emph{this is only done for the purposes of aggregation,} (i.e.~creating a composite score), and since composite indicators are normally presented with the index scores and original data accessible underneath, the underlying data would normally be presented in its original form.
\end{itemize}

Therefore, be careful, but also realise that data treatment is not unethical, it's simply an assumption in a statistical process. Like any other step or assumption though, any data treatment should be carefully recorded.

\hypertarget{why-treat-data}{%
\section{Why treat data?}\label{why-treat-data}}

There can be many reasons to treat data, but in composite indicators the main reason is to remove outliers or adjust heavily-skewed distributions. Outliers can exist because of errors in measurement and data processing, and should always be double-checked. But often, they are simply a reflection of reality. Outliers and skewed distributions are common in economics. One has to look no further than the in-built ASEM data set, and the denominating variables of population, GDP, energy consumption and country area:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotIndDist}\NormalTok{(WorldDenoms)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/DenomOutlierplots-1.pdf}

To illustrate why this can be a problem, consider the following artificial example.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotly)}
\FunctionTok{library}\NormalTok{(magrittr)}
\CommentTok{\# normally{-}distributed data}
\NormalTok{outlierdata }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\CommentTok{\# two outliers}
\NormalTok{outlierdata }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(outlierdata, }\DecValTok{10}\NormalTok{, }\DecValTok{25}\NormalTok{)}
\CommentTok{\# plot}
\FunctionTok{plot\_ly}\NormalTok{(}\AttributeTok{x =}\NormalTok{ outlierdata, }\AttributeTok{type =} \StringTok{"histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/outlier histogram-1.pdf}

Here we have the large majority of observations with values which are crammed into the bottom fifth of the scale, and two observations that are much higher, i.e.~they are outliers. If we were to normalise this distribution using a min-max method scaled to {[}0, 100{]}, for example, this would produce an indicator where the large majority of units have a very low score.

This might not reflect the narrative that you want to convey. It may be, for example, that values above the median are ``good'', and in this case, values in the 1.5-2.5 range are ``very good''. Two units have truly exception values, but that shouldn't take away from the fact that other units have values that are considered to be good.

The problem is that by normalising to {[}0, 100{]}, these units with ``very good'' values will have normalised scores of around 20 (out of 100), which when compared to other indicators, is a ``bad'' score. And this will be reflected in the aggregated score. In summary, the outlying values are ``dominating'' the scale, which reduces the discriminatory power of the indicator.

A few things to consider here are that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The outlier problem in composite indicators is mostly linked to the fact that you are aggregating indicators. If you are not aggregating, the outlying values may not be so much of a problem.
\item
  It might be that you \emph{want} to keep the distribution as it is, and let the indicator be defined by its outliers. This will depend on the objectives of the index.
\item
  If you do wish to do something about the outlying values, there are two possibilities. One is to treat the data, and this is described in the rest of this chapter The second is to use a normalisation method that is less sensitive to outliers - this is dealt with in the \protect\hyperlink{normalisation-1}{Normalisation} chapter.
\end{enumerate}

\hypertarget{how-to-treat-data}{%
\section{How to treat data}\label{how-to-treat-data}}

If you decide to treat the data before normalising, there are a two main options.

\hypertarget{winsorisation}{%
\subsection{Winsorisation}\label{winsorisation}}

The first is to \emph{Winsorise} the data. Winsorisation involves reassigning outlying points to the next highest point, or to a percentile value. To do this manually, using the data from the previous section, it would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get position of maximum value}
\NormalTok{imax }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(outlierdata}\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(outlierdata))}
\CommentTok{\# reassign with next highest value}
\NormalTok{outlierdata[imax] }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(outlierdata[}\SpecialCharTok{{-}}\NormalTok{imax])}
\FunctionTok{plot\_ly}\NormalTok{(}\AttributeTok{x =}\NormalTok{ outlierdata, }\AttributeTok{type =} \StringTok{"histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/Manual winz-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# and let\textquotesingle{}s do it again}
\NormalTok{imax }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(outlierdata}\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(outlierdata))}
\NormalTok{outlierdata[imax] }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(outlierdata[}\SpecialCharTok{{-}}\NormalTok{imax])}
\FunctionTok{plot\_ly}\NormalTok{(}\AttributeTok{x =}\NormalTok{ outlierdata, }\AttributeTok{type =} \StringTok{"histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/Manual winz-2.pdf}

And now we have arrived back to a normal distribution with no outliers, which is well-spread over the scale. Of course, this has come at the cost of actually moving data points. However keep in mind that this is only done for the purposes of aggregation, and the original indicator data would still be retained. A helpful way of looking at Winsorisation is that it is like capping the scale: it is enough to know that certain units have the highest score, without needing to know that they are ten times higher than the other units.

Clearly, outliers could also occur from the lower end of the scale, in which case Winsorisation would use the minimum values.

Notice that in general, Winsorisation does not change the shape of the distribution, apart from the outlying values. Therefore it is suited to distributions like the example, which are well-spread except for some few outlying values.

\hypertarget{transformation}{%
\subsection{Transformation}\label{transformation}}

The second option is to transform the distribution, by applying a transformation that changes all of the data points and thus the overall shape of the distribution.

The most obvious transformation in this respect is to take the logarithm. Denoting \(x\) as the original indicator and \(x'\) as the transformed indicator, this would simply be:

\[ x' = \ln(x) \]
This is a sensible choice because skewed distributions are often roughly log-normal, and taking the log of a log-normal distrinbution results in a normal distribution. However, this will not work for negative values. In that case, an alternative is:

\[ x' = \ln(x- \text{min}(x)+a), \; \; \text{where}\; \; a = 0.01(\text{max}(x)-\text{min}(x)) \]
The logic being that by subtracting the minimum and adding something, all values will be positive, so they can be safely log-transformed. This formula is similar to that used in the COIN Tool, but with an adjustment. In the COIN Tool, \(a=1\), which, depending on the range of the indicator, can give only a gentle and sometimes near-linear transformation. By setting \(a\) to be a set percentage of the range, it is ensured that the shape of the transformation is consistent.

A general family of transformations are called the \emph{Box-Cox} transformations. These are given as:

\[ x'(\lambda) =
    \begin{cases}
      \frac{x^\lambda-1}{\lambda} & \text{if}\ \lambda  \neq 0 \\
      \ln(x) & \text{if}\ \lambda = 0
    \end{cases} \]

In other words, a log transformation or a power transformation. The Box Cox transformation is often accompanied by an optimisation which chooses the \(\lambda\) value to best approximate the normal distribution.

Comparing to Winsorisation, log transforms and Box Cox transforms perform the transformation on every data point. To see the effect of this, we can take the log of one of the denominator indicators in \texttt{WorldDenoms}, the national GDPs of over 200 countries in 2019.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make df with original indicator and log{-}transformed version}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{GDP =}\NormalTok{ WorldDenoms}\SpecialCharTok{$}\NormalTok{Den\_GDP,}
                 \AttributeTok{LogGDP =} \FunctionTok{log}\NormalTok{(WorldDenoms}\SpecialCharTok{$}\NormalTok{Den\_GDP))}
\FunctionTok{plotIndDist}\NormalTok{(df, }\AttributeTok{type =} \StringTok{"Histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/LogWorlddenoms-1.pdf}

And there we see a beautiful normal distribution as a result.

\hypertarget{data-treatment-in-coinr}{%
\section{Data treatment in COINr}\label{data-treatment-in-coinr}}

COINr data treatment has a number of options and gives the user a fairly high degree of control in terms of which kind of treatment should be applied, either for all indicators at once, or for individual indicators. We will first explore the default data treatment options that are applied to all indicators, then see how to make exceptions and specific treatment requests.

\hypertarget{global-treatment}{%
\subsection{Global treatment}\label{global-treatment}}

The ``default'' treatment in COINr is similar to that applied in the \href{https://knowledge4policy.ec.europa.eu/composite-indicators/coin-tool_en}{COIN Tool}, and widely used in composite indicator construction. It follows a basic process of trying to Winsorise up to a specified limit, followed by a transformation if the Winsorisation does not sufficiently ``correct'' the distribution. In short, for each indicator it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Checks skew and kurtosis values
\item
  If absolute skew is greater than a threshold (default 2) AND kurtosis is greater than a threshold (default 3.5):

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \tightlist
  \item
    Successively Winsorise up to a specified maximum number of points. If either skew or kurtosis goes below thresholds, stop. If after reaching the maximum number of points, both thresholds are still exceeded, then:
  \item
    Perform a transformation
  \end{enumerate}
\end{enumerate}

This is the default process, although individual specifications can be made for individual indicators - see the next section.

The data treatment function in COINr is called \texttt{treat()}. To perform a default data treatment, we can simply use the ``COIN in, COIN out'' approach as usual.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# build ASEM data set, up to denomination}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndData, }\AttributeTok{IndMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndMeta,}
                 \AttributeTok{AggMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMAggMeta)}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM)}

\CommentTok{\# treat at defaults}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{treat}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{, }\AttributeTok{winmax =} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Check which indicators were treated, and how}
\FunctionTok{library}\NormalTok{(dplyr, }\AttributeTok{quietly =}\NormalTok{ T, }\AttributeTok{verbose =}\NormalTok{ F)}
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Treated}\SpecialCharTok{$}\NormalTok{TreatSummary }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Treatment }\SpecialCharTok{!=} \StringTok{"None"}\NormalTok{)}
\DocumentationTok{\#\#       IndCode Low High           TreatSpec           Treatment}
\DocumentationTok{\#\# V2   Services   0    4 Default, winmax = 5 Winsorised 4 points}
\DocumentationTok{\#\# V3        FDI   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V5    ForPort   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V6  CostImpEx   0    1 Default, winmax = 5 Winsorised 1 points}
\DocumentationTok{\#\# V7     Tariff   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V12     StMob   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V15  CultServ   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V21   Flights   0    1 Default, winmax = 5 Winsorised 1 points}
\DocumentationTok{\#\# V23      Bord   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V25       Gas   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V35    Forest   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V36   Poverty   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V41      NGOs   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V43    FemLab   1    0 Default, winmax = 5 Winsorised 1 points}
\end{Highlighting}
\end{Shaded}

The results of \texttt{treat()} are a new data set \texttt{.Data\$Treated} with the treated data in it, and a details of the data treatment in \texttt{\$Analysis\$Treated}, including \texttt{TreatSummary} (shown above), which gives a summary of the data treatment specified and actually applied for each indicator. In this case, all indicators were successfully brought within the specified skew and kurtosis limits by Winsorisation, within the specified Winsorisation limit \texttt{winmax} of five points.

To see what happens if \texttt{winmax} is exceeded, we will lower the threshold to \texttt{winmax\ =\ 3}. Additionally, we can control what type of log transform should be applied when \texttt{winmax} is exceeded, using the \texttt{deflog} argument. We will set \texttt{deflog\ =\ "CTlog"} which ensures that negative values will not cause errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# treat at defaults}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{treat}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{, }\AttributeTok{winmax =} \DecValTok{3}\NormalTok{, }\AttributeTok{deflog =} \StringTok{"CTlog"}\NormalTok{)}

\CommentTok{\# Check which indicators were treated, and how}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Treated}\SpecialCharTok{$}\NormalTok{TreatSummary }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Treatment }\SpecialCharTok{!=} \StringTok{"None"}\NormalTok{)}
\DocumentationTok{\#\#       IndCode Low High           TreatSpec               Treatment}
\DocumentationTok{\#\# V2   Services   0    0 Default, winmax = 3 CTLog (exceeded winmax)}
\DocumentationTok{\#\# V3        FDI   0    2 Default, winmax = 3     Winsorised 2 points}
\DocumentationTok{\#\# V5    ForPort   0    3 Default, winmax = 3     Winsorised 3 points}
\DocumentationTok{\#\# V6  CostImpEx   0    1 Default, winmax = 3     Winsorised 1 points}
\DocumentationTok{\#\# V7     Tariff   0    3 Default, winmax = 3     Winsorised 3 points}
\DocumentationTok{\#\# V12     StMob   0    2 Default, winmax = 3     Winsorised 2 points}
\DocumentationTok{\#\# V15  CultServ   0    2 Default, winmax = 3     Winsorised 2 points}
\DocumentationTok{\#\# V21   Flights   0    1 Default, winmax = 3     Winsorised 1 points}
\DocumentationTok{\#\# V23      Bord   0    3 Default, winmax = 3     Winsorised 3 points}
\DocumentationTok{\#\# V25       Gas   0    3 Default, winmax = 3     Winsorised 3 points}
\DocumentationTok{\#\# V35    Forest   0    2 Default, winmax = 3     Winsorised 2 points}
\DocumentationTok{\#\# V36   Poverty   0    3 Default, winmax = 3     Winsorised 3 points}
\DocumentationTok{\#\# V41      NGOs   0    3 Default, winmax = 3     Winsorised 3 points}
\DocumentationTok{\#\# V43    FemLab   1    0 Default, winmax = 3     Winsorised 1 points}

\CommentTok{\# Compare before and after treatment}
\FunctionTok{iplotIndDist2}\NormalTok{(ASEM, }\AttributeTok{dsets =} \FunctionTok{c}\NormalTok{(}\StringTok{"Denominated"}\NormalTok{, }\StringTok{"Treated"}\NormalTok{), }\AttributeTok{icodes =} \StringTok{"Services"}\NormalTok{,}
              \AttributeTok{ptype =} \StringTok{"Histogram"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/Default treat ASEM lowered winmax-1.pdf}

Now we can see from the table that the Services indicator has had a log transformation applied, and from the histograms that the skewness has been treated. The range of the indicator is now different, and this introduces negative values, however this will be anyway fixed in the normalisation step. The important thing here is the shape of the distribution.

Other options for \texttt{deflog} are ``log'' (a standard log transform), ``GIIlog'', which is a scaled log transformation used by the \href{https://www.globalinnovationindex.org/}{Global Innovation Index}, and ``boxcox'', which uses a Box-Cox transformation with the \(\lambda\) parameter set by the \texttt{boxlam} argument to \texttt{treat()}. Optimised \(\lambda\) values are not currently available in COINr, though this may be considered for future versions, along with custom transformation functions.

A further point to note is that Winsorisation can work in two ways in COINr, specified by the \texttt{winchange} argument. If set to \texttt{FALSE}, then Winsorisation works in the following way:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform an initial check of skew and kurtosis (call them \(s_0\) and \(k_0\)). Only if \textbf{both} exceed thresholds, continue to 2. Otherwise no treatment is applied.
\item
  If \(s_0\) is positive (usually the case), begin by Winsorising the highest value (assign the value of the second-highest point).
\item
  If \(s_0\) is negative, begin by Winsorising the lowest value (assign the value of the second-lowest point).
\item
  If either skew or kurtosis thesholds are below thresholds, stop, otherwise
\item
  Go to 2. if \(s_0\) was positive, or 3. if \(s_0\) was negative.
\end{enumerate}

Notably, the \emph{direction} of the Winsorisation here is always the same, i.e.~if \(s_0\) was positive, Winsorisation will always be applied to the highest values. The only drawback here is that the sign of the skew can conceivably change during the Winsorisation process. For that reason, if you set \texttt{winchange\ =\ TRUE} (which is anyway default), the function will check after every Winsorised point to see whether to Winsorise high values or low values.

It might seem unlikely that the skew could change sign from one iteration to the next, \emph{and} still remain outside the absolute threshold. But this has been observed to happen in some cases.

Finally, you can set the skewness and kurtosis threshold values to your favourite values using the \texttt{t\_skew} and \texttt{t\_kurt} arguments to \texttt{treat()}.

\hypertarget{individual-treatment}{%
\subsection{Individual treatment}\label{individual-treatment}}

If you want more control over the treatment of individual indicators, this can be done by specifying the optional \texttt{individual} and \texttt{indiv\_only} arguments. \texttt{individual} is a data frame which specifies the treatment to apply to specific indicators. It looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example of "individual" data frame spec}

\NormalTok{individual }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{IndCode =} \FunctionTok{c}\NormalTok{(}\StringTok{"Bord"}\NormalTok{, }\StringTok{"Services"}\NormalTok{, }\StringTok{"Flights"}\NormalTok{, }\StringTok{"Tariff"}\NormalTok{),}
  \AttributeTok{Treat =} \FunctionTok{c}\NormalTok{(}\StringTok{"win"}\NormalTok{, }\StringTok{"log"}\NormalTok{, }\StringTok{"none"}\NormalTok{, }\StringTok{"boxcox"}\NormalTok{),}
  \AttributeTok{Winmax =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{Thresh =} \FunctionTok{c}\NormalTok{(}\StringTok{"thresh"}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{Boxlam =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\NormalTok{individual}
\DocumentationTok{\#\#    IndCode  Treat Winmax Thresh Boxlam}
\DocumentationTok{\#\# 1     Bord    win     10 thresh     NA}
\DocumentationTok{\#\# 2 Services    log     NA   \textless{}NA\textgreater{}     NA}
\DocumentationTok{\#\# 3  Flights   none     NA   \textless{}NA\textgreater{}     NA}
\DocumentationTok{\#\# 4   Tariff boxcox     NA   \textless{}NA\textgreater{}      2}
\end{Highlighting}
\end{Shaded}

The ``IndCode'' column is the list of indicators to apply a specific treatment. The ``Treat'' column dictates which treatment to apply: options are either Winsorise (``win''), or any of the \texttt{deflog} options described previously. The ``Winmax'' column gives the maximum number of points to Winsorise for that indicator. This value is only used if the corresponding row of ``Treat'' is set to ``win''. The ``Thresh'' column specifies whether to use skew/kurtosis thresholds or not. If ``thresh'', it uses the thresholds specified in the \texttt{thresh} argument of \texttt{treat()}. If instead it is set at \texttt{NA}, then it will ignore the thresholds and force the Winsorisation to Winsorise exactly \texttt{winmax} points. Finally, the ``Boxlam'' column allows an individual setting of the \texttt{boxlam} parameter if the corresponding entry of the ``Treat'' column is set to ``boxcox''. If \texttt{Individual} is specified, all of these columns need to be present regardless of the individual treatments specified. Where entries are not needed, you can simply put \texttt{NA}.

An important point is that if the treatment specified in the ``Treat'' column is forced upon the specified indicator, for example, if ``CTlog'' is specified, this will be applied, without any Winsorisation first. Conversely, if ``win'' is specified, this will be applied, without any subsequent log transform even if it fails to correct the distribution.

Finally, the \texttt{indiv\_only} argument specifies if \texttt{TRUE}, ONLY the indicators listed in \texttt{individual} are treated, and the rest get no treatment. Otherwise, if \texttt{FALSE} then the remaining indicators are subject to the default treatment process.

Let's now put this into practice.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{treat}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{, }\AttributeTok{individual =}\NormalTok{ individual,}
              \AttributeTok{indiv\_only =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# Check which indicators were treated, and how}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Treated}\SpecialCharTok{$}\NormalTok{TreatSummary }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Treatment }\SpecialCharTok{!=} \StringTok{"None"}\NormalTok{)}
\DocumentationTok{\#\#       IndCode Low High               TreatSpec               Treatment}
\DocumentationTok{\#\# V2   Services   0    0              Forced log                     Log}
\DocumentationTok{\#\# V3        FDI   0    2     Default, winmax = 6     Winsorised 2 points}
\DocumentationTok{\#\# V5    ForPort   0    3     Default, winmax = 6     Winsorised 3 points}
\DocumentationTok{\#\# V6  CostImpEx   0    1     Default, winmax = 6     Winsorised 1 points}
\DocumentationTok{\#\# V7     Tariff   0    0          Forced Box{-}Cox Box Cox with lambda = 2}
\DocumentationTok{\#\# V12     StMob   0    2     Default, winmax = 6     Winsorised 2 points}
\DocumentationTok{\#\# V15  CultServ   0    2     Default, winmax = 6     Winsorised 2 points}
\DocumentationTok{\#\# V23      Bord   0    3 Forced Win, winmax = 10     Winsorised 3 points}
\DocumentationTok{\#\# V25       Gas   0    3     Default, winmax = 6     Winsorised 3 points}
\DocumentationTok{\#\# V35    Forest   0    2     Default, winmax = 6     Winsorised 2 points}
\DocumentationTok{\#\# V36   Poverty   0    3     Default, winmax = 6     Winsorised 3 points}
\DocumentationTok{\#\# V41      NGOs   0    3     Default, winmax = 6     Winsorised 3 points}
\DocumentationTok{\#\# V43    FemLab   1    0     Default, winmax = 6     Winsorised 1 points}
\end{Highlighting}
\end{Shaded}

And this shows how some indicators have had certain treatments applied. Of course here, the individual treatments have been chosen arbitrarily, and the default treatment would have sufficed. In practice, you would apply individual treatment for specific indicators for good reasons, such as excessive skew, or not wanting to treat certain indicators for conceptual reasons.

\hypertarget{interactive-visualisation}{%
\section{Interactive visualisation}\label{interactive-visualisation}}

As we have seen in this chapter, it's essential to visualise your indicator distributions and check what treatment has been applied, and what the main differences are. This can be achieved with the plotting functions previously described, but could become cumbersome if you want to check many indicators.

COINr has an built-in Shiny app which lets you compare indicator distributions interactively. Shiny is an R package which allows you to build interactive apps, dashboards and gadgets that can run R code. Shiny is a powerful tool for interactively exploring, communicating and presenting results, and can be used to host apps on the web. If you would like to know more about Shiny, I would recommend the book \href{https://mastering-shiny.org/index.html}{Mastering Shiny}.

To run the indicator visualisation app, simply run \texttt{indDash(ASEM)}. This will open a window which should look like this:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/inddash_screenshot} 

}

\caption{indDash screenshot}\label{fig:unnamed-chunk-40}
\end{figure}

The \texttt{indDash()} app is purely illustrative - it does not return anything back to R, but simply displays the distributions of the existing indicators.

In the side panel, all data sets that exist in the \texttt{.\$Data} folder are accessible, as well as any denominators, if they exist. Any of the indicators from any of these data sets can be plotted against each other.

In the context of data treatment, this gives a fast way to check the effects of treating indicators. In the ASEM example, we can set the the data set of indicator 1 to ``Denominated'', and the data set of indicator 2 to ``Treated''. This will compare denominated against treated indicators, which is a sensible comparison since \texttt{treat()} was applied to \texttt{.Data\$Denominated} in this case.

When a treated data set is selected, a table of treated indicators appears in the side panel - this lists all indicators that had treatment applied to them, and gives the details of the treatments applied. You can click on column headings to sort by values, and this can give an easy way to see which indicators were treated. You can then select an indicator of interest using the dropdown menus. In the screenshot above, we are comparing ``Gas'' in the denominated and treated data sets.

This gives several comparison plots: histograms of each indicator, violin plots, and a scatter plot of one indicator against the other, as well as overlaid histograms. As with any Plotly plot, any of these can be instantly downloaded using the camera button that appears when you hover over the plot. This might be useful for generating quick reports, for example.

Below the violin plots are details of the skew and kurtosis of the indicator, and details of the treatment applied, if any.

Finally, when comparing a treated indicator against its un-treated version, it can be helpful to check or un-check the ``Plot indicators on the same axis range'' box. Checking this box gives a like-for-like comparison which shows how the data points have moved, particularly useful in the case of Winsorisation. Unchecking it may be more helpful to compare the shapes of the two distributions.

While \texttt{indDash()} has specific features for treated data, it is also useful as a general purpose tool for fast indicator visualisation and exploration. It can help, for example, to observe trends and relationships between indicators and denominators, and will work with the aggregated data, i.e.~to see how the index may depend on its indicators.

\hypertarget{normalisation}{%
\chapter{Normalisation}\label{normalisation}}

Normalisation is the operation of bringing indicators onto comparable scales so that they can be aggregated more fairly. To see why this is necessary, consider aggregating GDP values (billions or trillions of dollars) with percentage tertiary graduates (tens of percent). Average values here would make no sense because one is on a completely different scale to the other.

\hypertarget{approaches}{%
\section{Approaches}\label{approaches}}

\hypertarget{first-adjust-direction}{%
\subsection{First: adjust direction}\label{first-adjust-direction}}

Indicators can either be positively or negatively related to the concept that you are trying to measure. For example, in an index of quality of life, median income would probably be a positive indicator. Prevalance of malnourishment would be a negative indicator (higher values should give lower scores in quality of life).

Accounting for these differences is considered part of the normalisation step. Indicators loaded into COINr should usually have a ``Direction'' column in the \texttt{IndMeta} input to \texttt{assemble()}, which is 1 for positive indicators, and -1 for negative indicators. With this information, normalisation is a two step procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiply the values of each indicator by their corresponding direction value (either 1 or -1).
\item
  Apply one of the normalisation methods described below.
\end{enumerate}

It's that simple. COINr has this built in, so you don't need to do anything other than specify the directions.

\hypertarget{linear-transformations}{%
\subsection{Linear transformations}\label{linear-transformations}}

Normalisation is relatively simple but there are still a number of different approaches which have different properties.

Perhaps the most straightforward and intuitive option (and therefore probably the most widely used) is called the \emph{min-max} transformation. This is a simple linear function which rescales the indicator to have a minimum value \(l\), a maximum value \(u\), and consequently a range \(u-l\), and is as follows:

\[ \tilde{x}_{\text{min}} = \frac{ x - x_{\text{min}} }{ x_{\text{max}} - x_{\text{min}} } \times (u-l) + l\]
where \(\tilde{x}\) is the normalised indicator value. For example, if \(l=0\) and \(u=100\) this will rescale the indicator to lie exactly onto the interval \([0, 100]\). The transformation is linear because it does not change the \emph{shape} of the distribution, it simply shrinks or expands it, and moves it.

A similar transformation is to take \emph{z-scores}, which instead use the mean and standard deviation as reference points:

\[ \tilde{x}_{\text{min}} = \frac{ x - \mu_x }{ \sigma_x } \times a + b\]
where \(\mu_x\) and \(\sigma_x\) are the mean and standard deviation of \(x\). The indicator is first re-scaled to have mean zero and standard deviation of one. Then it is scaled by a factor \(a\) and moved by a distance \(b\). This is very similar to the min-max transformation in that it can be reduced to multiplying by a factor and adding a constant, which is the definition of a linear transformation. However, the two approaches have different implications. One is that Z scores will generally be less sensitive to outliers, because the standard deviation is less dependent on an outlying value than the minimum or maximum.

Following the min-max and z-score, the general linear transformation is defined as:

\[ \tilde{x} = \frac{ x - p }{ q } \times a + b\]

and it is fairly straightforward to see how z-scores and the min-max transformations are special cases of this.

\hypertarget{nonlinear-transformations}{%
\subsection{Nonlinear transformations}\label{nonlinear-transformations}}

A simple nonlinear transformation is the rank transformation.

\[ \tilde{x} = \text{rank}(x)\]
where the ranks should be defined so that the lowest indicator value has a rank of 1, the second lowest a rank of 2, and so on. The rank transformation is attractive because it automatically eliminates any outliers. Therefore there would not usually be any need to treat the data previously. However, it converts detailed indicator scores to simple ranks, which might be too reductionist for some.

It's worth pointing out that there are different ways to rank values, because of how ties (units with the same score) are handled. To read about this, just call \texttt{?rank} in R.

Similar approaches to simple ranks include Borda scores, which are simply the ranks described above but minus 1 (so the lowest score is 0 instead of 1), and percentile ranks.

\hypertarget{distances}{%
\subsection{Distances}\label{distances}}

Another approach is to use the distance of each score to some reference value. Possibilities here are the (normalised) distance to the maximum value of the indicator:

\[ \tilde{x} = 1 - \frac{\text{max}(x) - x}{\text{max}(x) - \text{min}(x)}\]

the fraction of the maximum value of the indicator:

\[ \tilde{x} = \frac{x}{\text{max}(x)}\]

the distance to a specified unit value in the indicator:

\[ \tilde{x} = 1 - \frac{x_u - x}{\text{max}(x) - \text{min}(x)}\]

where \(x_u\) is the value of unit \(u\) in the indicator. This is useful for benchmarking against a reference country, for example. Another possibility is to normalise against indicator targets. This approach is used for example in the \href{https://www.sciencedirect.com/science/article/pii/S2665972720300593}{EU2020 Index}, which used European targets on environment, education and employment issues (among others).

\[ \tilde{x} = \text{min} \left[1, \  1 - \frac{\text{targ}(x) - x}{\text{max}(x) - \text{min}(x)} \right]\]
where \(\text{targ}(x)\) is the target for the indicator. In this case, any value that exceeds the target is set to 1, i.e.~exceeding the target is counted the same as exactly meeting it. There is also an issue of what to use to scale the distance: here the range of the indicator is used, but one could also use \(\text{targ}(x) - \text{min}(x)\) or perhaps some other range.

\hypertarget{normalisation-in-coinr}{%
\section{Normalisation in COINr}\label{normalisation-in-coinr}}

The normalisation function in COINr is imaginatively named \texttt{normalise()}. It has the following main features:

\begin{itemize}
\tightlist
\item
  A wide range of normalisation methods, including custom func
\item
  Customisable parameters for normalisation
\item
  Possibility to specify detailed individual treatment for each indicator
\end{itemize}

The function looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# don\textquotesingle{}t run this chunk, just for illustration (will throw error if run)}
\NormalTok{normalise }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(COIN, }\AttributeTok{ntype=}\StringTok{"minmax"}\NormalTok{, }\AttributeTok{npara =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{icodes =} \ConstantTok{NULL}\NormalTok{,}
                      \AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{directions =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{individual =} \ConstantTok{NULL}\NormalTok{,}
                      \AttributeTok{indiv\_only =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{out2 =} \ConstantTok{NULL}\NormalTok{)\{}
\end{Highlighting}
\end{Shaded}

As an input, it takes a COIN or data frame, as usual. It outputs a new dataset to the COIN \texttt{.\$Data\$Normalised}, or a normalised data frame if \texttt{out2\ =\ "df"}. Default normalisation (min-max, scaled between 0 and 100) can be achieved by simply calling:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}
\CommentTok{\# Build ASEM index up to denomination}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndData, }\AttributeTok{IndMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndMeta, }\AttributeTok{AggMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMAggMeta)}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM)}

\CommentTok{\# Default normalisation (min max scaled between 0 and 100) on denominated data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{normalise}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{)}

\CommentTok{\# compare one of the indicators}
\FunctionTok{iplotIndDist2}\NormalTok{(ASEM, }\AttributeTok{dsets =} \FunctionTok{c}\NormalTok{(}\StringTok{"Denominated"}\NormalTok{, }\StringTok{"Normalised"}\NormalTok{),}
              \AttributeTok{icodes =} \StringTok{"TertGrad"}\NormalTok{, }\AttributeTok{ptype =} \StringTok{"Scatter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/Default normalise-1.pdf}

This plot also illustrates the linear nature of the min-max transformation.

You can select the normalisation type of the \texttt{normalise()} function using the \texttt{ntype} and accompanying \texttt{npara} arguments, where the latter is an object which specifies any parameters for the type of normalisation selected.

\begin{itemize}
\tightlist
\item
  \texttt{ntype\ =\ "minmax"} yields a min-max transformation that scales each indicator onto an interval specified by \texttt{npara}, e.g.~if \texttt{npara\$minmax\ =\ c(0,10)} the indicators will scale to {[}0, 10{]}.
\item
  \texttt{ntype\ =\ "zscore"} scales the indicator to have a mean and standard deviation specified by \texttt{npara}, e.g.~if \texttt{npara\$zscore\ =\ c(0,1)} the indicator will have mean zero and standard deviation 1.
\item
  \texttt{ntype\ =\ "scaled"} is a general linear transformation defined by \texttt{npara\$scaled\ =\ c(a,b)} which subtracts \texttt{a} and divides by \texttt{b}.
\item
  \texttt{ntype\ =\ "rank"} replaces indicator scores with their corresponding ranks, such that the highest scores have the largest rank values. Ties take average rank values. Here \texttt{npara} is not used.
\item
  \texttt{ntype\ =\ "borda"} is similar to \texttt{ntype\ =\ "rank"} but uses Borda scores, which are simply rank values minus one. \texttt{npara} is not used.
\item
  \texttt{ntype\ =\ "prank"} gives percentile ranks. \texttt{npara} is not used.
\item
  \texttt{ntype\ =\ "fracmax"} scales each indicator by dividing the value by the maximum value of the indicator. \texttt{npara} is not used.
\item
  \texttt{ntype\ =\ "dist2ref"} gives the distance to a reference unit, defined by \texttt{npara}. For example, if \texttt{npara\$dist2ref\ =\ "AUT"} then all scores will be normalised as the distance to the score of unit ``AUT'' in each indicator (divided by the range of the indicator). This is useful for benchmarking against a set unit, e.g.~a reference country. Scores
\item
  \texttt{ntype\ =\ "dist2max"} gives the normalised distance to the maximum of each indicator.
\item
  \texttt{ntype\ =\ "dist2targ"} gives the normalised distance to indicator targets. Targets are specified, in the \texttt{IndMeta} argument of \texttt{assemble()}, and will be in the COIN at \texttt{.\$Input\$IndMeta}. Any scores that exceed the target will be capped at a normalised value of one.
\item
  \texttt{ntype\ =\ "custom"} allows to pass a custom function to apply to every indicator. For example, \texttt{npara\$custom\ =\ function(x)\ \{x/max(x,\ na.rm\ =\ T)\}} would give the ``fracmax'' normalisation method described above.
\item
  \texttt{ntype\ =\ "none"} the indicator is not normalised.
\end{itemize}

The \texttt{normalise()} function also allows you to specify the directions of the indicators using the argument \texttt{directions}. If this is not specified, the directions will be taken from the indicator metadata input to \texttt{assemble()}. If they are not present here, all directions will default to positive.

\hypertarget{individual-normalisation}{%
\section{Individual normalisation}\label{individual-normalisation}}

Indicators can be normalised using individual specifications in a similar way to the \texttt{treat()} function. This is specified by the following two arguments to \texttt{normalise()}:

\begin{itemize}
\tightlist
\item
  \texttt{individual} A list of named lists specifiying individual normalisation to apply to specific indicators. Should be structured so that the name of each sublist should be the indicator code. The the list elements are:

  \begin{itemize}
  \tightlist
  \item
    \texttt{.\$ntype} is the type of normalisation to apply (any of the options mentioned above)
  \item
    \texttt{.\$npara} is a corresponding object or parameters that are used by ntype
  \end{itemize}
\item
  \texttt{indiv\_only} Logical. As with \texttt{treat()}, if this is set to \texttt{FALSE} (default), then the indicators that are \emph{not} specified in \texttt{individual} will be normalised according to the \texttt{ntype} and \texttt{npara} arguments specified in the function argument. Otherwise if \texttt{TRUE}, only the indicators in \texttt{individual} will be normalised, and the others will be unaffected.
\end{itemize}

The easiest way to clarify this is with an example. In the following, we will apply min-max, scaled to {[}0, 1{]} to all indicators, except for ``Flights'' which will be normalised using Borda scores, ``Renew'' which will remain un-normalised, and ``Ship'' which will be scaled as a distance to the value of Singapore.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{indiv }\OtherTok{=} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{Flights =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ntype =} \StringTok{"borda"}\NormalTok{),}
  \AttributeTok{Renew =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ntype =} \StringTok{"none"}\NormalTok{),}
  \AttributeTok{Ship =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ntype =} \StringTok{"dist2ref"}\NormalTok{, }\AttributeTok{npara =} \FunctionTok{list}\NormalTok{(}\AttributeTok{dist2ref =} \StringTok{"SGP"}\NormalTok{))}
\NormalTok{)}

\CommentTok{\# Minmax in [0,1] for all indicators, except custom individual normalisation}
\CommentTok{\# for those described above}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{normalise}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{, }\AttributeTok{ntype =} \StringTok{"minmax"}\NormalTok{, }\AttributeTok{npara =} \FunctionTok{list}\NormalTok{(}\AttributeTok{minmax =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                  \AttributeTok{individual =}\NormalTok{ indiv, }\AttributeTok{indiv\_only =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can visualise the new ranges of the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotIndDist}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{,}
            \AttributeTok{icodes =} \FunctionTok{c}\NormalTok{(}\StringTok{"Flights"}\NormalTok{, }\StringTok{"Renew"}\NormalTok{, }\StringTok{"Ship"}\NormalTok{, }\StringTok{"Goods"}\NormalTok{), }\AttributeTok{type =} \StringTok{"Dot"}\NormalTok{)}
\DocumentationTok{\#\# \textasciigrave{}stat\_bindot()\textasciigrave{} using \textasciigrave{}bins = 30\textasciigrave{}. Pick better value with \textasciigrave{}binwidth\textasciigrave{}.}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-42-1.pdf}

This example is meant to be illustrative of the functionality of \texttt{normalise()}, rather than being a sensible normalisation strategy, because the indicators are now on very different ranges, from the default minmax range of {[}0, 1{]} (``Goods'' in this case), to the unchanged scale of ``Renew'' and the Borda rank scale of ``Flights''. Notice also the scaling of ``Ship'', which has values above zero because the reference country, Singapore, is does not have the maximum value of the indicator.

In practice, if different normalisation strategies are selected, it is a good idea to keep the indicators on similar ranges, otherwise the effects will be very unequal in the aggregation step.

\hypertarget{aggregation}{%
\chapter{Aggregation}\label{aggregation}}

Aggregation is the operation of combining multiple indicators into one value. Many composite indicators have a hierarchical structure, so in practice this often involves multiple aggregations, for example aggregating groups of indicators into aggregate values, then aggregating those values into higher-level aggregates, and so on, until the final index value.

Aggregating should almost always be done on normalised data, unless the indicators are already on very similar scales. Otherwise the relative influence of indicators will be very uneven.

Of course you don't \emph{have} to aggregate indicators at all, and you might be content with a scoreboard, or perhaps aggregating into several aggregate values rather than a single index. However, consider that aggregation should not substitute the underlying indicator data, but complement it.

Overall, aggregating indicators is a form of information compression - you are trying to combine many indicator values into one, and inevitably information will be lost. As long as this is kept in mind, and indicator data is presented and made available along side aggregate values, then aggregate (index) values can complement indicators and be used as a useful tool for summarising the underlying data, and identifying overall trends and patterns.

\hypertarget{weighting}{%
\section{Weighting}\label{weighting}}

Many aggregation methods involve some kind of weighting, i.e.~coefficients that define the relative weight of the indicators/aggregates in the aggregation. In order to aggregate, weights need to first be specified, but to effectively adjust weights it is necessary to aggregate.

This chicken and egg conundrum is best solved by aggregating initially with a trial set of weights, perhaps equal weights, then seeing the effects of the weighting, and making any weight adjustments necessary. For this reason, weighting is dealt with in the following chapter on \protect\hyperlink{weighting-2}{Weighting}.

\hypertarget{approaches-1}{%
\section{Approaches}\label{approaches-1}}

\hypertarget{means}{%
\subsection{Means}\label{means}}

The most straightforward and widely-used approach to aggregation is the \textbf{weighted arithmetic mean}. Denoting the indicators as \(x_i \in \{x_1, x_2, ... , x_d \}\), a weighted arithmetic mean is calculated as:

\[ y = \frac{\sum_{i=1}^d x_iw_i}{\sum_{i=1}^d w_i}  \]

where the \(w_i\) are the weights corresponding to each \(x_i\). Here, if the weights are chosen to sum to 1, it will simplify to the weighted sum of the indicators. In any case, the weighted mean is scaled by the sum of the weights, so weights operate relative to each other.

Clearly, if the index has more than two levels, then there will be multiple aggregations. For example, there may be three groups of indicators which give three separate aggregate scores. These aggregate scores would then be fed back into the weighted arithmetic mean above to calculate the overall index.

The arithmetic mean has ``perfect compensability'', which means that a high score in one indicator will perfectly compensate a low score in another. In a simple example with two indicators scaled between 0 and 10 and equal weighting, a unit with scores (0, 10) would be given the same score as a unit with scores (5, 5) -- both have a score of 5.

An alternative is the \textbf{weighted geometric mean}, which uses the product of the indicators rather than the sum.

\[ y = \left( \prod_{i=1}^d x_i^{w_i} \right)^{1 / \sum_{i=1}^d w_i} \]

This is simply the product of each indicator to the power of its weight, all raised the the power of the inverse of the sum of the weights.

The geometric mean is less compensatory than the arithmetic mean -- low values in one indicator only partially substitute high values in others. For this reason, the geometric mean may sometimes be preferred when indicators represent ``essentials''. An example might be quality of life: a longer life expectancy perhaps should not compensate severe restrictions on personal freedoms.

A third type of mean, in fact the third of the so-called \href{https://en.wikipedia.org/wiki/Pythagorean_means}{Pythagorean means} is the \textbf{weighted harmonic mean}. This uses the mean of the reciprocals of the indicators:

\[ y = \frac{\sum_{i=1}^d w_i}{\sum_{i=1}^d w_i/x_i} \]

The harmonic mean is the the least compensatory of the the three means, even less so than the geometric mean. It is often used for taking the mean of rates and ratios.

\hypertarget{other-methods}{%
\subsection{Other methods}\label{other-methods}}

The \emph{weighted median} is also a simple alternative candidate. It is defined by ordering indicator values, then picking the value which has half of the assigned weight above it, and half below it. For \emph{ordered} indicators \(x_1, x_2, ..., x_d\) and corresponding weights \(w_1, w_2, ..., w_d\) the weighted median is the indicator value \(x_m\) that satisfies:

\[ \sum_{i=1}^{m-1} w_i \leq \frac{1}{2}, \: \: \text{and} \sum_{i=m+1}^{d} w_i \leq \frac{1}{2} \]

The median is known to be robust to outliers, and this may be of interest if the distribution of scores across indicators is skewed.

Another somewhat different approach to aggregation is to use the \href{https://en.wikipedia.org/wiki/Copeland\%27s_method}{Copeland method}. This approach is based pairwise comparisons between units and proceeds as follows. First, an \emph{outranking matrix} is constructed, which is a square matrix with \(N\) columns and \(N\) rows, where \(N\) is the number of units.

The element in the \(p\)th row and \(q\)th column of the matrix is calculated by summing all the indicator weights where unit \(p\) has a higher value in those indicators than unit \(q\). Similarly, the cell in the \(q\)th row and \(p\)th column (which is the cell opposite on the other side of the diagonal), is calculated as the sum of the weights unit where \(q\) has a higher value than unit \(p\). If the indicator weights sum to one over all indicators, then these two scores will also sum to 1 by definition. The outranking matrix effectively summarises to what extent each unit scores better or worse than all other units, for all unit pairs.

The Copeland score for each unit is calculated by taking the sum of the row values in the outranking matrix. This can be seen as an average measure of to what extent that unit performs above other units.

Clearly, this can be applied at any level of aggregation and used hierarchically like the other aggregation methods presented here.

In some cases, one unit may score higher than the other in all indicators. This is called a \emph{dominance pair}, and corresponds to any pair scores equal to one (equivalent to any pair scores equal to zero).

The percentage of dominance pairs is an indication of robustness. Under dominance, there is no way methodological choices (weighting, normalisation, etc.) can affect the relative standing of the pair in the ranking. One will always be ranked higher than the other. The greater the number of dominance (or robust) pairs in a classification, the less sensitive country ranks will be to methodological assumptions.

\hypertarget{aggregation-in-coinr}{%
\section{Aggregation in COINr}\label{aggregation-in-coinr}}

Many will be amazed to learn that the function to aggregate in COINr is called \texttt{aggregate()}. First, let's build the ASEM data set up to the point of normalisation, then aggregate it using the default approaches.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}

\CommentTok{\# Build ASEM data up to the normalisation step}
\CommentTok{\# Assemble data and metadata into a COIN}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndData, }\AttributeTok{IndMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMIndMeta, }\AttributeTok{AggMeta =}\NormalTok{ COINr}\SpecialCharTok{::}\NormalTok{ASEMAggMeta)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Denominate as specified in metadata}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM)}
\CommentTok{\# Normalise using default method: min{-}max scaled between 0 and 100.}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{normalise}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{)}

\CommentTok{\# Now aggregate using default method: arithmetic mean, using weights found in the COIN}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{)}

\CommentTok{\# show last few columns of aggregated data set}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Aggregated[(}\FunctionTok{ncol}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Aggregated)}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{)}\SpecialCharTok{:} \FunctionTok{ncol}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Aggregated)]}
\DocumentationTok{\#\# \# A tibble: 51 x 6}
\DocumentationTok{\#\#    Environ Social SusEcFin  Conn  Sust Index}
\DocumentationTok{\#\#      \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1    71.0   67.0     65.7  50.4  67.9  59.1}
\DocumentationTok{\#\#  2    55.9   86.3     53.5  54.8  65.3  60.0}
\DocumentationTok{\#\#  3    56.4   54.0     62.5  42.4  57.6  50.0}
\DocumentationTok{\#\#  4    76.0   54.8     55.5  45.9  62.1  54.0}
\DocumentationTok{\#\#  5    61.9   59.5     31.1  45.5  50.8  48.2}
\DocumentationTok{\#\#  6    53.7   61.1     72.0  48.9  62.3  55.6}
\DocumentationTok{\#\#  7    72.5   79.8     62.7  51.0  71.7  61.4}
\DocumentationTok{\#\#  8    35.7   68.3     70.1  49.5  58.0  53.8}
\DocumentationTok{\#\#  9    52.9   82.1     63.8  48.2  66.3  57.2}
\DocumentationTok{\#\# 10    66.5   60.8     54.2  47.0  60.5  53.7}
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

By default, the aggregation function performs the following steps:

\begin{itemize}
\tightlist
\item
  Uses the weights that were attached to \texttt{IndMeta} and \texttt{AggMeta} in \texttt{assemble()}
\item
  Aggregates hierarchically (with default method of weighted arithmetic mean), following the index structure specified in \texttt{IndMeta} and using the data specified in \texttt{dset}
\item
  Creates a new data set \texttt{.\$Data\$Aggregated}, which consists of the data in \texttt{dset}, plus extra columns with scores for each aggregation group, at each aggregation level.
\end{itemize}

Like other COINr functions \texttt{aggregate()} has arguments \texttt{dset} which specifies which data set to aggregate, and \texttt{out2} which specifies whether to output an updated COIN, or a data frame. Unlike other COINr functions however, \texttt{aggregate()} only works on COINs, because it requires a number of inputs, including data, weights, and the index structure.

The types of aggregation supported by \texttt{aggregate()} are specified by \texttt{agtype} from the following options:

\begin{itemize}
\tightlist
\item
  \texttt{agtype\ =\ arith\_mean} uses the arithmetic mean, and is the default.
\item
  \texttt{agtype\ =\ geom\_mean} uses the geometric mean. This only works if all data values are positive, otherwise it will throw an error.
\item
  \texttt{agtype\ =\ harm\_mean} uses the harmonic mean. This only works if all data values are non-zero, otherwise it will throw an error.
\item
  \texttt{agtype\ =\ median} uses the weighted median
\item
  \texttt{agtype\ =\ copeland} uses the Copeland method. This may take a few seconds to process depending on the number of units, because it involves pairwise comparisons across units.
\item
  \texttt{agtype\ =\ custom} allows you to pass a custom aggregation function.
\end{itemize}

For the last option here, if \texttt{agtype\ =\ custom} then you need to also specify the custom function via the \texttt{agfunc} argument. As an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define an aggregation function which is the weighted minimum}
\NormalTok{weightedMin }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x,w) }\FunctionTok{min}\NormalTok{(x}\SpecialCharTok{*}\NormalTok{w, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# pass to aggregate() and aggregate}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{, }\AttributeTok{agtype =} \StringTok{"custom"}\NormalTok{,}
                  \AttributeTok{agfunc =}\NormalTok{ weightedMin, }\AttributeTok{out2 =} \StringTok{"df"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Any custom function should have the form \texttt{functionName\ \textless{}-\ function(x,w)}, i.e.~it accepts x and w as vectors of indicator values and weights respectively, and returns a scalar aggregated value. Ensure that NAs are handled (e.g.~set na.rm = T) if your data has missing values, otherwise NAs will be passed through to higher levels.

A number of sophisticated aggregation approaches and linked weighting methods are available in the \href{https://cran.r-project.org/web/packages/Compind/index.html}{compind package}. These can nicely complement the features in COINr and may be of interest to those looking for more technical approaches to aggregation, such as those based on benefit of the doubt methods.

A further argument, \texttt{agtype\_bylevel}, allows specification of different normalisation types at different aggregation levels. For example, \texttt{agtype\_bylevel\ =\ c("arith\_mean",\ "geom\_mean",\ "median")} would use the arithmetic mean at the indicator level, the geometric mean at the pillar level, and the median at the sub-index level (for the ASEM data structure).

Finally, alternative weights can be used for aggregation by specifying the \texttt{agweights} argument. This can either be:

\begin{itemize}
\tightlist
\item
  \texttt{NULL}, in which case it will use the weights that were attached to IndMeta and AggMeta in GII\_assemble (if they exist), or
\item
  A character string which corresponds to a named list of weights stored in \texttt{.\$Parameters\$Weights}. You can either add these manually or through \texttt{rew8r} (see chapter on \protect\hyperlink{weighting-2}{Weighting}). E.g. entering \texttt{agweights\ =\ "Original"} will use the original weights read in on assembly. This is equivalent to \texttt{agweights\ =\ NULL}.
\item
  A list of weights to use in the aggregation, where each element of the list is a numeric vector of weights for a given level.
\end{itemize}

Now that the data has been aggregated, a natural next step is to explore the results. This is dealt with in the chapter on {[}Results visualisation{]}.

\hypertarget{weighting-1}{%
\chapter{Weighting}\label{weighting-1}}

Strictly speaking, weighting comes before aggregation. However, in order to understand the \emph{effects} of weights, we need to aggregate the index first.

Weighting in composite indicators is a thorny issue, which attracts considerable attention and is often one of the main focuses of critics. Weighting openly expresses a subjective opinion on the relative importance of each indicator relative to the others, and this opinion can easily be contested.

While this criticism definitely has some basis, weights can be viewed as a type of model parameter, and any model (e.g.~engineering models, climate models, economic models) is full of uncertain parameters. In large models, these parameters are less evident since they are inside rather complex model code. Weights in composite indicators are easy to criticise since the model of a composite indicator is quite simple, usually using simple averages of indicator values.

That said, weights do need to be carefully considered, taking into account at least:

\begin{itemize}
\tightlist
\item
  The relative conceptual importance of indicators: is indicator A more, less, or equally important to indicator B?
\item
  The statistical implications of the weights
\item
  How to communicate the weights to end users
\end{itemize}

The last point is important if the index is for advocacy/awareness. Weights may be fine tuned to account for statistical considerations, but the result may make little sense to the public or the media.

Overall, weighting is a large topic which cannot be covered in detail here. Nevertheless, this chapter gives some introduction and points to further references.

\hypertarget{approaches-to-weighting}{%
\section{Approaches to weighting}\label{approaches-to-weighting}}

Broadly, weighting approaches can be divided into those that are based on expert judgment, and those that are based on statistical considerations. Then there are approaches that combine the two.

\hypertarget{equal-weighting}{%
\subsection{Equal weighting}\label{equal-weighting}}

The most common approach to weighting is simply to make all weights equal to one another. This may seem like an unjustifiable simplification, but in practice, experts often give near-equal weights when asked, and statistical approaches may give weights that are so unequal that it may be hard to justify them to stakeholders. This is why many composite indicators use equal weights. A further consideration is that the results of a composite indicator are often less sensitive to their weights than you might think.

That said, a number of other methods exist, some of which are discussed in the following sections.

\hypertarget{budget-allocation}{%
\subsection{Budget allocation}\label{budget-allocation}}

Fundamentally, composite indicators involve breaking down a complex multidimensional concept into sub-concepts, and possibly breaking those sub-concepts down into sub-sub-concepts, and so on. Generally, you keep doing this until you arrive at the point where the (sub-)\^{}n-concepts are sufficiently specific that they can be directly measured with a small group of indicators.

Regardless of the aggregation level that you are at, however, some of the indicators (or aggregates) in the group are likely to be more or less important to the concept you are trying to measure than others.

Take the example of university rankings. One might consider the following indicators to be relevant to the notion of ``excellence'' for a university:

\begin{itemize}
\tightlist
\item
  Number of publications in top-tier journals (research output)
\item
  Teaching reputation, e.g.~through a survey (teaching quality)
\item
  Research funding from private companies (industry links)
\item
  Percentage of international students (global renown)
\item
  Mean earning of graduates (future prospects of students)
\end{itemize}

These may all be relevant for the concept, but the relative importance is definitely debatable. For example, if you are a prospective student, you might prioritise the teaching quality and graduate earnings. If you are researcher, you might priorities publications and research funding. And so on. The two points to make here are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Indicators are often not equally important to the concept, and
\item
  Different people have different opinions on what should be more or less important.
\end{enumerate}

Bringing this back to the issue of weights, it is important to make sure that the relative contributions of the indicators to the index scorers reflect the intended importance. This can be achieved by changing the weights attached to each indicator, and the weights of higher aggregation levels.

How then do we understand which indicators should be the most important to the concept? One way is to simply use our own opinion. But this does not reflect a diversity of viewpoints.

The \emph{budget allocation method} is a simple way of eliciting weights from a group of experts. The idea is to get a group of experts on the concept, stakeholders and end-users, ideally from different backgrounds. Then each member of the group is given 100 ``coins'' which they can ``spend'' on the indicators. Members allocate their budget to each indicator, so that they give more of the budget to important indicators, and less to less important ones.

This is a simple way of distributing weight to indicators, in a way that is easy for people to understand and to give their opinions. You can take the results, and take the average weight for each indicator. You can even infer a weight-distribution over each indicator which could feed into an uncertainty analysis.

\hypertarget{principle-component-analysis}{%
\subsection{Principle component analysis}\label{principle-component-analysis}}

A very different approach is to use principle component analysis (PCA). PCA is a statistical approach, which rotates your indicator data into a new set of coordinates with special properties.

One way of looking at indicator data is as data points in a multidimensional space. If we only have two indicators, then any unit can be plotted as a point on a two-dimensional plot, where the x-axis is the first indicator, and the y-axis is the second.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ind1 }\OtherTok{\textless{}{-}} \DecValTok{35}
\NormalTok{Ind2 }\OtherTok{\textless{}{-}} \DecValTok{60}

\FunctionTok{plot}\NormalTok{(Ind1, Ind2, }\AttributeTok{main =} \StringTok{"World\textquotesingle{}s most boring plot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-44-1.pdf}

Each point on this beautiful base-R plot represents a unit (here we only have one).

If we have three indicators, then we will have three axes (i.e.~a 3D plot). If we have more than three, then the point lives in a \emph{hyperspace} which we can't really visualise.

Anyway, the main point here is that indicator data can be plotted as points in a space, where each axis (dimension) is an indicator.

What PCA does, is that it rotates the data so that the axes are no longer indicators, but instead are \emph{linear combinations of indicators}. And this is done so that the first axis is the linear combination of indicators that explains the most variance. If this is not making much sense (and if this is the first time you have encountered PCA then it probably won't), I would recommend to look at one of the many visualisations of PCA on the internet, e.g.~\href{https://setosa.io/ev/principal-component-analysis/}{this one}.

PCA is useful for composite indicators, because if you use an arithmetic mean, then you are using a linear combination of indicators. And the first principle component gives the weights that maximise the variance of the units. In other words, if you use PCA weights, you will have the best weights for discriminating between units, and for capturing as much information from the underlying indicators.

This sounds perfect, but the downside is that PCA weights do not care about the relative importance of indicators. Typically, PCA assigns the highest weights to indicators with the highest correlations with other indicators, and this can result in a very unbalanced combination of indicators. Still, PCA has a number of nice properties, and has the advantage of being a purely statistical approach.

\hypertarget{weight-optimisation}{%
\subsection{Weight optimisation}\label{weight-optimisation}}

If you choose to go for the budget allocation approach, to match weights to the opinions of experts, or indeed your own opinion, there is a catch that is not very obvious. Put simply, weights do not directly translate into importance.

To understand why, we must first define what ``importance'' means. Actually there is more than one way to look at this, but one possible measure is to use the (nonlinear) correlation between each indicator and the overall index. If the correlation is high, the indicator is well-reflected in the index scores, and vice versa.

If we accept this definition of importance, then it's important to realise that this correlation is affected not only by the weights attached to each indicator, but also by the correlations \emph{between indicators}. This means that these correlations must be accounted for in choosing weights that agree with the budgets assigned by the group of experts.

In fact, it is possible to reverse-engineer the weights either \href{https://doi.org/10.1111/j.1467-985X.2012.01059.x}{analytically using a linear solution} or \href{https://doi.org/10.1016/j.ecolind.2017.03.056}{numerically using a nonlinear solution}. While the former method is far quicker than a nonlinear optimisation, it is only applicable in the case of a single level of aggregation, with an arithmetic mean, and using linear correlation as a measure. Therefore in COINr, the second method is used.

\hypertarget{weighting-tools-in-coinr}{%
\section{Weighting tools in COINr}\label{weighting-tools-in-coinr}}

As seen in the chapter on \protect\hyperlink{aggregation-1}{Aggregation}, weights are used to calculate the aggregated data set, and are one of the inputs to the \texttt{aggregate()} function. These can either be directly specified as a list, or as a character string which points to one of the sets of weights stored under \texttt{.\$Parameters\$Weights}. There is no limit to the number of sets of weights that can be stored here. So how do we create new sets of weights?

\hypertarget{manual-weighting}{%
\subsection{Manual weighting}\label{manual-weighting}}

The simplest approach is to make manual adjustments. First, make a copy of the existing weights.

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(COINr)}
\CommentTok{\# quietly build data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}

\NormalTok{NewWeights }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}\SpecialCharTok{$}\NormalTok{Parameters}\SpecialCharTok{$}\NormalTok{Weights}\SpecialCharTok{$}\NormalTok{Original}

\FunctionTok{head}\NormalTok{(NewWeights)}
\DocumentationTok{\#\#   AgLevel      Code Weight}
\DocumentationTok{\#\# 1       1     Goods      1}
\DocumentationTok{\#\# 2       1  Services      1}
\DocumentationTok{\#\# 3       1       FDI      1}
\DocumentationTok{\#\# 4       1    PRemit      1}
\DocumentationTok{\#\# 5       1   ForPort      1}
\DocumentationTok{\#\# 6       1 CostImpEx      1}
\end{Highlighting}
\end{Shaded}

The structure of the weights is a data frame, with each row being either an indicator or aggregation. The \texttt{AgLevel} column points to the aggregation level, and the \texttt{Weight} column is the (relative) weight.

We can just directly modify this. Let's say we want to set the weight of the connectivity sub-index to 0.5.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NewWeights}\SpecialCharTok{$}\NormalTok{Weight[NewWeights}\SpecialCharTok{$}\NormalTok{Code }\SpecialCharTok{==} \StringTok{"Conn"}\NormalTok{] }\OtherTok{\textless{}{-}} \FloatTok{0.5}

\FunctionTok{tail}\NormalTok{(NewWeights)}
\DocumentationTok{\#\#    AgLevel     Code Weight}
\DocumentationTok{\#\# 55       2  Environ    1.0}
\DocumentationTok{\#\# 56       2   Social    1.0}
\DocumentationTok{\#\# 57       2 SusEcFin    1.0}
\DocumentationTok{\#\# 58       3     Conn    0.5}
\DocumentationTok{\#\# 59       3     Sust    1.0}
\DocumentationTok{\#\# 60       4    Index    1.0}
\end{Highlighting}
\end{Shaded}

Remember that \emph{weights are relative}, and are re-scaled to sum to 1 during aggregation. To actually use these weights, we can either (a) input them directly in the \texttt{aggregate()} function, or first attach them to the COIN and the call \texttt{aggregate()}, pointing to the new set of weights. The latter option seems more sensible because that way, all our sets of weights are neatly stored. Also, we can access this in the reweighting app as explained in the next section.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# put new weights in the COIN}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Parameters}\SpecialCharTok{$}\NormalTok{Weights}\SpecialCharTok{$}\NormalTok{NewWeights }\OtherTok{\textless{}{-}}\NormalTok{ NewWeights}

\CommentTok{\# Aggregate again to get new results}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM, }\AttributeTok{agweights =} \StringTok{"NewWeights"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now the \texttt{ASEM} COIN contains the updated results using the new weights.

\hypertarget{interactive-re-weighting-with-rew8r}{%
\subsection{Interactive re-weighting with ReW8R}\label{interactive-re-weighting-with-rew8r}}

\emph{NOTE: at this moment, rew8r is temporarily out of action until I get it updated to the new weight format.}

Re-weighting manually, as we have just seen, is quite simple. However, depending on your objectives, weighting can be an iterative process along the lines of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Try a set of weights
\item
  Examine correlations
\item
  Check results, rankings
\item
  Adjust weights
\item
  Return to 1.
\end{enumerate}

Doing this at the command line can be time consuming, so COINr includes a ``re-weighting app'' called \texttt{rew8r()} which lets you interactively adjust weights and explore the effects of doing so.

To run \texttt{rew8r()} you must have already aggregated your data using \texttt{aggregate()}. This is because every time weights are adjusted, \texttt{rew8r()} re-aggregates to find new results, and it needs to know \emph{which} aggregation method you are using. So, with your pre-aggregated COIN at hand, simply run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{rew8r}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

At this point, a window/tab should open in your browser with the \texttt{rew8r()} app, which looks something like this:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/rew8r_screenshot} 

}

\caption{rew8r() screenshot}\label{fig:unnamed-chunk-45}
\end{figure}

This may look a little over-complicated to begin with, but let's go through it step by step.

\hypertarget{correlations}{%
\subsubsection{Correlations}\label{correlations}}

First of all, \texttt{rew8r()} is based around correlations. Why are correlations important? Because they give an idea of how each indicator is represented in the final index, and at other aggregation levels (see earlier discussion in this chapter).

The \texttt{rew8r()} app allows you to check the correlations of anything against anything else. The two dropdown menus in the top left allow you to select which aggregation level to correlate against which other. In the screenshot above, the indicators (Level 1) are being correlated against the pillars (Level 2), but this can be changed to any of the levels in the index (the first level should always be below the second level otherwise it can cause errors). You can also select which type of correlation to examine, either Pearson (i.e.~``standard'' correlation), Spearman rank correlation, or Kendall's tau (an alternative rank correlation).

The correlations themselves are shown in two plots - the first is the scatter plot in the upper right part of the window, which plots the \emph{correlation} of each indicator in Level 1 on the horizontal axis with its parent in Level 2, against its \emph{weight} (in Level 1) on the vertical axis. We will come back to this shortly.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/corr_heatmap_screenshot_rew8r} 

}

\caption{Heatmap screenshot (in rew8r)}\label{fig:unnamed-chunk-46}
\end{figure}

The second plot is a heatmap of the correlation matrix (between Level 1 and Level 2 in this case) which is shown in the lower right corner. This gives the correlation values between all the indicators/groups of each level as colours according to a discrete colour map. This colour map intends to highlight highly correlated indicators (green), as well as negative or low-correlated indicators (red). These thresholds can be adjusted by adjusting the low and high correlation threshold dropdown menus in the side panel (on the left). The aggregation groups are also shown as overlaid rectangles, and correlation values can be turned on and off using the checkbox below the heatmap. The point of this plot is to show at a glance, where there are very high or low correlations, possibly within aggregation groups. For example, indicators that are negatively correlated with their aggregation group can be problematic. Note that correlation heatmaps can also be generated independently of the app using the \texttt{plotCorr()} function - this is described more in the \protect\hyperlink{multivariate-analysis}{Multivariate analysis} section.

Highly-correlated indicators are also listed in a table at the lower middle of the window. The ``Ind-Ind'' tab flags any indicators that are above the high correlation threshold, and within the same aggregation group (the aggregation level immediately above the indicator level). Note that this is \emph{not} dependent on the weights since it regards correlations between indicators only. The ``Cross-level'' tab instead gives a table which flags any correlations across the two levels selected in side panel, which are above the threshold.

\hypertarget{weighting-2}{%
\subsubsection{Weighting}\label{weighting-2}}

Let us now turn back to the scatter plot. The scatter plot shows the correlation values on the horizontal axis, but also shows the weight of each indicator or aggregate in the first level selected in the dropdown menu. The purpose here is to show at a glance the weighting, and to make adjustments. To adjust a weight, either click on one of the points in the plot (hovering will show the names of the indicators or aggregates), or select the indicator/aggregate in the dropdown menu in the side panel under the ``Weighting'' subsection. Then adjust the weight using the slider. Doing this will automatically update all of the plots and tables, so you can see interactively how the correlations change as a result.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/corr_scat_screenshot_rew8r} 

}

\caption{Correlation scatter plot screenshots in rew8r().}\label{fig:unnamed-chunk-47-1}
\end{figure}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/corr_scat_sep_screenshot_rew8r} 

}

\caption{Correlation scatter plot screenshots in rew8r().}\label{fig:unnamed-chunk-47-2}
\end{figure}

Importantly, the scatter plot only shows correlations between each indicator/aggregate in the first level \emph{with its parent} in the second selected level. When everything is plotted on one plot, this may not be so clear, so the ``Separate groups'' checkbox in the side panel changes the plot to multiple sub-plots, one for each group in the second selected level. This helps to show how correlations are distributed within each group.

The results of the index, in terms of ranks and scores of units, can be found by switching to the ``Results'' tab in the main window.

Other than adjusting individual weights, any set of weights that is stored in \texttt{.\$Parameters\$Weights} can be selected, and this will automatically show the correlations. The button ``Equal weights'' sets weights to be equal at the current level.

Finally, if you have adjusted the weights and you wish to save the new set of weights back to the COIN, you can do this in the ``Weights output'' subsection of the side panel. The current set of weights is saved by entering a name and clicking the ``Save'' button. This saves the set of weights to a list, and when the app is closed (using the ``Close app'' button), it will return them to the updated COIN under \texttt{.\$Parameters\$Weights}, with the specified name. You can save multiple sets of weights in the same session, by making adjustments, and assigning a name, and clicking ``Save''. When you close the app, all the weight sets will be attached to the COIN. If you don't click ``Close app'', any new weight sets created in the session will be lost.

\hypertarget{remarks}{%
\subsubsection{Remarks}\label{remarks}}

The \texttt{rew8r()} app is something of a work in progress - it is difficult to strike a balance between conveying the relevant information and conveying too much information. In future versions, it may be updated to become more streamlined.

\hypertarget{automatic-re-weighting}{%
\subsection{Automatic re-weighting}\label{automatic-re-weighting}}

As discussed earlier, weighting can also be performed statistically, by optimising or targeting some statistical criteria of interest. COINr includes a few possibilities in this respect.

\hypertarget{with-pca}{%
\subsubsection{With PCA}\label{with-pca}}

The \texttt{getPCA()} function is used as a quick way to perform PCA on COINs. This does two things: first, to output PCA results for any aggregation level; and second, to provide PCA weights corresponding to the ``loadings'' of the first principle component. As discussed above, this is the linear combination that results in the most explained variance.

The other outputs of \texttt{getPCA()} are left to the \protect\hyperlink{multivariate-analysis}{Multivariate analysis} section. Here, we will focus on weighting. To obtain PCA weights, simply run something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getPCA}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This calls the aggregated data set, and performs PCA on aggregation level 2, i.e.~the pillars in the case of ASEM. The result is an updated COIN, with a new set of weights called \texttt{.Parameters\$Weights\$PCA\_AggregatedL2}. This is automatically named according to the data set used and the aggregation level.

An important point here is that this weighting represents optimal properties if the components (pillars here) are aggregated directly into a single value. In this example, it is not strictly correct because above the pillar level we have another aggregation (sub-index) before reaching the final index. This means that these weights wouldn't be strictly correct, because they don't account for the intermediate level. However, depending on the aggregation structure and weighting (i.e.~if it is fairly even) it may give a fairly good approximation.

\emph{Note: PCA weights have not been fully tested at the time of writing, treat with skepticism.}

\hypertarget{weight-optimisation-1}{%
\subsubsection{Weight optimisation}\label{weight-optimisation-1}}

An alternative approach is to optimise weight iteratively. This is in a way less elegant (and slower) because it uses numerical optimisation, but allows much more flexibility.

The \texttt{weightOpt()} function gives several options in this respect. In short, it allows weights to be optimised to agree with either (a) some pre-specified ``importances'' (perhaps obtained by the budget allocation method, or perhaps simply equal importance), or (b) to maximise the information conveyed by the composite indicator. Starting with the first option, this is how it can be applied to our example data:

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{weightOpt}\NormalTok{(ASEM, }\AttributeTok{itarg =} \StringTok{"equal"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{3}\NormalTok{, }\AttributeTok{out2 =} \StringTok{"COIN"}\NormalTok{)}
\DocumentationTok{\#\# iterating... squared difference = 0.000814548399872482}
\DocumentationTok{\#\# iterating... squared difference = 0.00116234608542398}
\DocumentationTok{\#\# iterating... squared difference = 0.00052279688155717}
\DocumentationTok{\#\# iterating... squared difference = 0.000271428850497901}
\DocumentationTok{\#\# iterating... squared difference = 4.19003122660241e{-}05}
\DocumentationTok{\#\# iterating... squared difference = 1.66726715407502e{-}06}
\DocumentationTok{\#\# iterating... squared difference = 0.000166401854209433}
\DocumentationTok{\#\# iterating... squared difference = 0.000357199714112592}
\DocumentationTok{\#\# iterating... squared difference = 4.91262049731255e{-}05}
\DocumentationTok{\#\# iterating... squared difference = 0.000189074238231704}
\DocumentationTok{\#\# iterating... squared difference = 2.15217457991648e{-}06}
\DocumentationTok{\#\# iterating... squared difference = 4.02429806062667e{-}05}
\DocumentationTok{\#\# iterating... squared difference = 1.04404840785835e{-}05}
\DocumentationTok{\#\# iterating... squared difference = 1.00833908145386e{-}05}
\DocumentationTok{\#\# iterating... squared difference = 2.40420217478906e{-}06}
\DocumentationTok{\#\# Optimisation successful!}
\end{Highlighting}
\end{Shaded}

This targets the aggregation level 3 (sub-index), and the \texttt{itarg} argument here is set to ``equal'', which means that the objective is to make the correlations between each sub-index and the index equal to one another. More details of the results of this optimisation are found in the \texttt{.\$Analysis\$Weights} folder of the object. In particular, we can see the final correlations and the optimised weights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Weights}\SpecialCharTok{$}\NormalTok{OptimsedLev3}\SpecialCharTok{$}\NormalTok{CorrResults}
\DocumentationTok{\#\#   Desired  Obtained OptWeight}
\DocumentationTok{\#\# 1     0.5 0.8971336     0.400}
\DocumentationTok{\#\# 2     0.5 0.8925119     0.625}
\end{Highlighting}
\end{Shaded}

In this case, the weights are slightly uneven but seem reasonable. The correlations are also almost exactly equal. The full set of weights is found as a new weight set under \texttt{.\$Parameters\$Weights}.

Other possibilities with the \texttt{weightOpt()} function include:

\begin{itemize}
\tightlist
\item
  Specifying an unequal vector of target importances using the \texttt{itarg} argument. E.g. taking the sub-pillar level, specifying \texttt{itarg\ =\ c(0.5,\ 1)} would optimise the weights so that the first sub-pillar has half the correlation of the second sub-pillar with the index.
\item
  Optimising the weights at other aggregation levels using the \texttt{aglev} argument
\item
  Optimising over other correlation types, e.g.~rank correlations, using the \texttt{cortype} argument
\item
  Aiming to maximise overall correlations by setting \texttt{optype\ =\ infomax}
\end{itemize}

It's important to point out that only one level can be optimised at a time, and the optimisation is ``conditional'' on the weights at other levels. For example, optmising at the sub-index level produces optimal correlations for the sub-indexes, but not for any other level. If we then optimise at lower levels, this will change the correlations of the sub-indexes! So probably the most sensible strategy is to optimise for the last aggregation level (before the index).

That said, an advantage of \texttt{weightOpt()} is that since it is numerical, it can be used for any type of aggregation method, any correlation type, and with any index structure. It calls the aggregation method that you last used to aggregate the index, so it rebuilds the index according to your specifications.

Other correlation types may be of interest when linear correlation (i.e.~Pearson correlation) is not a sufficient measure. This is the case for example, when distributions are highly skewed. Rank correlations are robust to outliers and can handle some nonlinearity (but not non-monotonicity). Fully nonlinear correlation methods \href{https://www.sciencedirect.com/science/article/pii/S1470160X17301759}{also exist} but most of the time, relationships between indicators are fairly linear. Nonlinear correlation may be included in a future version of COINr.

Setting \texttt{optype\ =\ infomax} changes the optimisation criterion, to instead \emph{maximise} the sum of the correlations, which is equivalent to maximising the information transferred to the index, and is similar to PCA. Like PCA, this can however result in fairly unequal weights, and not infrequently in negative weights.

The \texttt{weightOpt()} function uses a numerical optimisation algorithm which does not always succeed in finding a good solution. Optimisation is a large field in its own right, and is more difficult the more variables you optimise over. If \texttt{weightOpt()} fails to find a good solution, try:

\begin{itemize}
\tightlist
\item
  Decreasing the \texttt{toler} argument - this decreases the acceptable error between the target importance and the importance with the final set of weights.
\item
  Increasing the \texttt{maxiter} argument - this is the number of iterations allowed to find a solution within the specified tolerance. By default it is set at 500.
\end{itemize}

Even then, the algorithm may not always converge, depending on the problem.

\hypertarget{effective-weights}{%
\section{Effective weights}\label{effective-weights}}

By now, you may know or have realised that the weight of an indicator is not only affected by its own weight, but also the weights of any aggregate groups to which it belongs. If the aggregation method is the arithmetic mean, the ``effective weight'', i.e.~the final weight of each indicator, including its parent weights, can be be easily obtained by COINr's \texttt{effectiveWeight()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get effective weight info}
\NormalTok{EffWeights }\OtherTok{\textless{}{-}} \FunctionTok{effectiveWeight}\NormalTok{(ASEM)}

\NormalTok{EffWeights}\SpecialCharTok{$}\NormalTok{EffectiveWeightsList}
\DocumentationTok{\#\#    AgLevel      Code EffectiveWeight}
\DocumentationTok{\#\# 1        1     Goods      0.02000000}
\DocumentationTok{\#\# 2        1  Services      0.02000000}
\DocumentationTok{\#\# 3        1       FDI      0.02000000}
\DocumentationTok{\#\# 4        1    PRemit      0.02000000}
\DocumentationTok{\#\# 5        1   ForPort      0.02000000}
\DocumentationTok{\#\# 6        1 CostImpEx      0.01666667}
\DocumentationTok{\#\# 7        1    Tariff      0.01666667}
\DocumentationTok{\#\# 8        1      TBTs      0.01666667}
\DocumentationTok{\#\# 9        1    TIRcon      0.01666667}
\DocumentationTok{\#\# 10       1      RTAs      0.01666667}
\DocumentationTok{\#\# 11       1      Visa      0.01666667}
\DocumentationTok{\#\# 12       1     StMob      0.01250000}
\DocumentationTok{\#\# 13       1  Research      0.01250000}
\DocumentationTok{\#\# 14       1       Pat      0.01250000}
\DocumentationTok{\#\# 15       1  CultServ      0.01250000}
\DocumentationTok{\#\# 16       1  CultGood      0.01250000}
\DocumentationTok{\#\# 17       1   Tourist      0.01250000}
\DocumentationTok{\#\# 18       1  MigStock      0.01250000}
\DocumentationTok{\#\# 19       1      Lang      0.01250000}
\DocumentationTok{\#\# 20       1       LPI      0.01250000}
\DocumentationTok{\#\# 21       1   Flights      0.01250000}
\DocumentationTok{\#\# 22       1      Ship      0.01250000}
\DocumentationTok{\#\# 23       1      Bord      0.01250000}
\DocumentationTok{\#\# 24       1      Elec      0.01250000}
\DocumentationTok{\#\# 25       1       Gas      0.01250000}
\DocumentationTok{\#\# 26       1  ConSpeed      0.01250000}
\DocumentationTok{\#\# 27       1     Cov4G      0.01250000}
\DocumentationTok{\#\# 28       1      Embs      0.03333333}
\DocumentationTok{\#\# 29       1      IGOs      0.03333333}
\DocumentationTok{\#\# 30       1    UNVote      0.03333333}
\DocumentationTok{\#\# 31       1     Renew      0.03333333}
\DocumentationTok{\#\# 32       1  PrimEner      0.03333333}
\DocumentationTok{\#\# 33       1       CO2      0.03333333}
\DocumentationTok{\#\# 34       1    MatCon      0.03333333}
\DocumentationTok{\#\# 35       1    Forest      0.03333333}
\DocumentationTok{\#\# 36       1   Poverty      0.01851852}
\DocumentationTok{\#\# 37       1     Palma      0.01851852}
\DocumentationTok{\#\# 38       1  TertGrad      0.01851852}
\DocumentationTok{\#\# 39       1 FreePress      0.01851852}
\DocumentationTok{\#\# 40       1    TolMin      0.01851852}
\DocumentationTok{\#\# 41       1      NGOs      0.01851852}
\DocumentationTok{\#\# 42       1       CPI      0.01851852}
\DocumentationTok{\#\# 43       1    FemLab      0.01851852}
\DocumentationTok{\#\# 44       1   WomParl      0.01851852}
\DocumentationTok{\#\# 45       1   PubDebt      0.03333333}
\DocumentationTok{\#\# 46       1  PrivDebt      0.03333333}
\DocumentationTok{\#\# 47       1   GDPGrow      0.03333333}
\DocumentationTok{\#\# 48       1     RDExp      0.03333333}
\DocumentationTok{\#\# 49       1      NEET      0.03333333}
\DocumentationTok{\#\# 50       2  Physical      0.10000000}
\DocumentationTok{\#\# 51       2  ConEcFin      0.10000000}
\DocumentationTok{\#\# 52       2 Political      0.10000000}
\DocumentationTok{\#\# 53       2    Instit      0.10000000}
\DocumentationTok{\#\# 54       2       P2P      0.10000000}
\DocumentationTok{\#\# 55       2   Environ      0.16666667}
\DocumentationTok{\#\# 56       2    Social      0.16666667}
\DocumentationTok{\#\# 57       2  SusEcFin      0.16666667}
\DocumentationTok{\#\# 58       3      Conn      0.50000000}
\DocumentationTok{\#\# 59       3      Sust      0.50000000}
\DocumentationTok{\#\# 60       4     Index      1.00000000}
\end{Highlighting}
\end{Shaded}

A nice way to visualise this is to call \texttt{plotframework()} (see \protect\hyperlink{initial-visualisation-and-analysis}{Initial visualisation and analysis}). \texttt{effectiveWeight()} also gives other info on the parents of each indicator/aggregate which may be useful for some purposes (e.g.~some types of plots external to COINr).

\hypertarget{final-points}{%
\section{Final points}\label{final-points}}

Weighting and correlations is a complex topic, which is important to explore and address. On the other hand, weighting and correlations are just one part of a composite indicator, and balancing and optimising weights probably not be pursued at the expense of building a confusing composite indicator that makes no sense to most people (depending on the context and application).

\hypertarget{visualising-results}{%
\chapter{Visualising results}\label{visualising-results}}

The main results of a composite indicator are the aggregated scores and ranks. These can be explored and visualised in different ways, and the way you do this will depend on the context. For example, in building the index, it is useful to check the results at an initial stage, again after methodological adjustments, and so on through the construction process. Short static plots and tables are useful for concisely communicating results in reports or briefs.

When the index is complete, it can be presented using a dedicated web platform, which allows users to explore the results in depth. Some interesting examples of this include:

\begin{itemize}
\tightlist
\item
  The \href{https://power.lowyinstitute.org/}{Lowy Asia Power Index}
\item
  The \href{https://composite-indicators.jrc.ec.europa.eu/cultural-creative-cities-monitor/performance-map}{Cultural and Creative Cities Monitor}
\item
  The \href{https://dashboards.sdgindex.org/map}{SDG Index and Dashboards}
\end{itemize}

These latter platforms are not usually built in R, but in Javascript. However, COINr does have a simple results viewer that can be used for prototyping.

\hypertarget{tables}{%
\section{Tables}\label{tables}}

To see the aggregated results, one way is to look in \texttt{.\$Data\$Aggregated}, which contains the aggregated scores, along with any grouping variables that were attached with the original data. However, this is not always convenient because the table can be quite large, and also the index itself will be the last column. You can of course manually select columns, but to make life easier COINr has a dedicated function, \texttt{getResults()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}
\FunctionTok{library}\NormalTok{(magrittr)}

\CommentTok{\# build example data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{suppressMessages}\NormalTok{()}

\CommentTok{\# get results table, top 10}
\FunctionTok{getResults}\NormalTok{(ASEM) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\DocumentationTok{\#\#    UnitCode    UnitName Index Rank}
\DocumentationTok{\#\# 1       CHE Switzerland 68.38    1}
\DocumentationTok{\#\# 2       NLD Netherlands 64.82    2}
\DocumentationTok{\#\# 3       DNK     Denmark 64.80    3}
\DocumentationTok{\#\# 4       NOR      Norway 64.47    4}
\DocumentationTok{\#\# 5       BEL     Belgium 63.54    5}
\DocumentationTok{\#\# 6       SWE      Sweden 63.00    6}
\DocumentationTok{\#\# 7       AUT     Austria 61.90    7}
\DocumentationTok{\#\# 8       LUX  Luxembourg 61.58    8}
\DocumentationTok{\#\# 9       DEU     Germany 60.75    9}
\DocumentationTok{\#\# 10      MLT       Malta 60.36   10}
\end{Highlighting}
\end{Shaded}

By default, \texttt{getResults()} gives a table of the highest level of aggregation (i.e.~typically the index), along with the scores, and the ranks. Scores are rounded to two decimal places, and this can be controlled by the \texttt{nround} argument. To see all aggregate scores, we can change the \texttt{tab\_type} argument:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getResults}\NormalTok{(ASEM, }\AttributeTok{tab\_type =} \StringTok{"Aggregates"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  reactable}\SpecialCharTok{::}\FunctionTok{reactable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/resultsAgg-1.pdf}

Here, we have used the reactable package to display the full table interactively. Notice that the aggregation levels are ordered from the highest level downwards, which is a more intuitive way to read the table. Other options for \texttt{tab\_type} are ``Full'', which shows all columns in \texttt{.\$Data\$Aggregated} (but rearranged to make them easier to read), and ``FullWithDenoms'', which also attaches the denominators. These latter two are probably mostly useful for further data processing, or passing the results on to someone else.

Often it may be more interesting to look at ranks rather than scores We can do this by setting \texttt{use\ =\ "ranks"}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getResults}\NormalTok{(ASEM, }\AttributeTok{tab\_type =} \StringTok{"Aggregates"}\NormalTok{, }\AttributeTok{use =} \StringTok{"ranks"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  reactable}\SpecialCharTok{::}\FunctionTok{reactable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/resultsAgg2-1.pdf}

Additionally to the \texttt{getResults()} function, the \texttt{iplotTable()} function also plots tables, but interactively (and indeed using the reactable package underneath). Like many other COINr functions it takes the \texttt{isel} and \texttt{aglev} arguments to select subsets of the indicator data. Moreover, it colours table cells, similar to conditional formatting in Excel.

As an example, we can request to see all the pillar scores inside the Connectivity sub-index. The function sorts the units by the first column by default. This function is useful for generating quick tables inside HTML reports.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotTable}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Conn"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/iplotTable_ASEM-1.pdf}

Recall that since the output of this function is a reactable table, we can also edit it further by using reactable functions.

If an individual unit is of interest, you can also call \texttt{getUnitSummary()}, which is a function which gives a summary of the scores for an individual unit:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getUnitSummary}\NormalTok{(ASEM, }\AttributeTok{usel =} \StringTok{"GBR"}\NormalTok{, }\AttributeTok{aglevs =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
Indicator & Score & Rank\\
\hline
Sustainable Connectivity & 57.76 & 15\\
\hline
Connectivity & 52.81 & 14\\
\hline
Sustainability & 62.72 & 14\\
\hline
Physical & 51.75 & 10\\
\hline
Economic and Financial (Con) & 21.17 & 32\\
\hline
Political & 77.08 & 13\\
\hline
Institutional & 76.61 & 15\\
\hline
People to People & 37.44 & 20\\
\hline
Environmental & 64.88 & 20\\
\hline
Social & 73.10 & 10\\
\hline
Economic and Financial (Sus) & 50.17 & 43\\
\hline
\end{tabular}

Finally, the strengths and weaknesses of an individual unit can be found by calling \texttt{getStrengthNWeak()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SAW }\OtherTok{\textless{}{-}} \FunctionTok{getStrengthNWeak}\NormalTok{(ASEM, }\AttributeTok{usel =} \StringTok{"GBR"}\NormalTok{)}
\NormalTok{SAW}\SpecialCharTok{$}\NormalTok{Strengths  }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r|r}
\hline
Code & Name & Rank & Value\\
\hline
TIRcon & Signatory of TIR Convention & 1 & 1.0\\
\hline
Embs & Embassies network & 1 & 100.0\\
\hline
Ship & Liner Shipping Connectivity Index & 2 & 20.8\\
\hline
Lang & Common languages users & 3 & 19.4\\
\hline
RTAs & Regional trade agreements & 4 & 30.0\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SAW}\SpecialCharTok{$}\NormalTok{Weaknesses  }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r|r}
\hline
Code & Name & Rank & Value\\
\hline
TBTs & Technical barriers to trade & 39 & 1190.00\\
\hline
Goods & Trade in goods & 41 & 779.00\\
\hline
Renew & Renewable energy in total final energy consumption & 41 & 8.71\\
\hline
PubDebt & Public debt as a percentage of GDP & 42 & 89.00\\
\hline
GDPGrow & GDP per capita growth & 42 & 1.13\\
\hline
\end{tabular}

These are selected as the top-ranked and bottom-ranked indicators for each unit, respectively, where by default the top and bottom five are selected (but this can be changed using the \texttt{topN} and \texttt{bottomN} arguments).

\hypertarget{plots}{%
\section{Plots}\label{plots}}

Tables are useful but can be a bit dry, so you can spice this up using the bar chart and map options already mentioned in \protect\hyperlink{initial-visualisation-and-analysis}{Initial visualisation and analysis}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotBar}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Index"}\NormalTok{, }\AttributeTok{usel =} \StringTok{"GBR"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/ASEMindexbar-1.pdf}

And here's a map:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotMap}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Index"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/ASEMindexmap-1.pdf}

The map and bar chart options here are very useful for quickly presenting results. However, if you want to make a really beautiful map, you should consider one of the many mapping packages in R, such as \href{https://rstudio.github.io/leaflet/}{leaflet}.

Apart from plotting individual indicators, we can inspect and compare unit scores using a radar chart. The function \texttt{iplotRadar()} allows the scores of one or more units, in a set of indicators, to be plotted on the same chart. It follows a similar syntax to \texttt{iplotTable()} and others, since it calls groups of indicators, so the \texttt{isel} and \texttt{aglev} arguments should be specified. Additionally, it requires a set of units to plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotRadar}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{usel =} \FunctionTok{c}\NormalTok{(}\StringTok{"CHN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{), }\AttributeTok{isel =} \StringTok{"Conn"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/radarASEM-1.pdf}

Radar charts, which look exciting to newbies, but have are often grumbled about by data-vis veterans, should be used in the right context. In the first place, they are mainly good for \emph{comparisons} between units - if you simply want to see the scores of one unit, a bar chart may be a better choice because it is easier to see the scores and to compare scores between one indicator and another. Second, they are useful for a smallish number of indicators. A radar chart with two or three indicators looks silly, and with ten indicators or more is hard to read. The chart above, with five indicators, is a clear comparison between two countries (in my opinion), showing that one country scores higher in all dimensions of connectivity than another, and the relative differences.

The \texttt{iplotRadar()} function is powered by Plotly, therefore it can be edited using Plotly commands. It is an interactive graphic, so you can add and remove units by clicking on the legend.

A useful feature of this function is the possibility to easily add (group) means or medians. For example, we can add the a trace representing the overall median values for each indicator:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotRadar}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{usel =} \FunctionTok{c}\NormalTok{(}\StringTok{"CHN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{), }\AttributeTok{isel =} \StringTok{"Conn"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{2}\NormalTok{, }\AttributeTok{addstat =} \StringTok{"median"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/radarASEM_median-1.pdf}

We can also add the mean or median of a grouping variable that is present in the selected data set. Grouping variables are those that start with ``Group\_''. Here we can add the GDP group mean (both the selected countries are in the ``XL'' group).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotRadar}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{usel =} \FunctionTok{c}\NormalTok{(}\StringTok{"CHN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{), }\AttributeTok{isel =} \StringTok{"Conn"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{2}\NormalTok{, }\AttributeTok{addstat =} \StringTok{"groupmean"}\NormalTok{,}
           \AttributeTok{statgroup =} \StringTok{"Group\_GDP"}\NormalTok{, }\AttributeTok{statgroup\_name =} \StringTok{"GDP"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/radarASEM_groupmean-1.pdf}

\hypertarget{interactive-exploration}{%
\section{Interactive exploration}\label{interactive-exploration}}

COINr also offers a Shiny app called \texttt{resultsDash()} for quickly and interactively exploring the results. It features the plots described above, but in an interactive app format. The point of this is to give a tool which can be used to explore the results during development, and to demonstrate the results of the index in validation sessions with experts and stakeholders. It is not meant to replace a dedicated interactive platform. It could however be a starting point for a Shiny-based web platform.

To call the app, simply run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{resultsDash}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

This will open a browser window or tab with the results dashboard app. The app allows you to select any of the data sets present in \texttt{.\$Data}, and to plot any of the indicators inside either on a bar chart or map (see the toggle map/bar box in the bottom right corner). Units (i.e.~countries here) can be compared by clicking on the bar chart or the map to select one or more countries - this plots them on the radar chart in the bottom left. The indicator groups plotted in the radar chart are selected by the ``Aggregation level'' and ``Aggregation group'' dropdowns. Finally, the ``Table'' tab switches to a table view, which uses the \texttt{iplotTable()} function mentioned above.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/resultsDash_screenshot} 

}

\caption{resultsDash screenshot}\label{fig:unnamed-chunk-51}
\end{figure}

\hypertarget{unit-reports}{%
\section{Unit reports}\label{unit-reports}}

Many composite indicators produce reports for each unit, for example the ``economy profiles'' in the Global Innovation Index. Typically this is a summary of the scores and ranks of the unit, possibly some figures, and maybe strengths and weaknesses.

COINr allows country reports to be generated automatically. This is done via a \emph{parameterised R markdown document}, which includes several of the tables and figures shown in this section\footnote{See \href{https://bookdown.org/yihui/rmarkdown/parameterized-reports.html}{here} for more information on how parameterised documents work.}. The COINr function \texttt{getUnitReport()} generates a report for any specified unit using an example scorecard which is found in the \texttt{/inst} directory where COINr is installed on your computer. To easily find this, call:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{system.file}\NormalTok{(}\StringTok{"UnitReport"}\NormalTok{, }\StringTok{"unit\_report\_source.Rmd"}\NormalTok{, }\AttributeTok{package =} \StringTok{"COINr"}\NormalTok{)}
\DocumentationTok{\#\# [1] "C:/R/RLib/COINr/UnitReport/unit\_report\_source.Rmd"}
\end{Highlighting}
\end{Shaded}

where the directory will different on your computer.

Opening this document in R will give you an idea of how the parameterised document works. In short, the COIN is passed into the R Markdown document, so that anything that is inside the COIN can be accessed and used for generating tables, figures and text. The example included is just that, an example. To see how it looks, we can run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getUnitReport}\NormalTok{(COIN)}
\end{Highlighting}
\end{Shaded}

Using R Markdown it is possible to generate a unit report using parameterised documents, which means you can easily generate a large number of similar reports for different units

\hypertarget{adjustments-and-comparisons}{%
\chapter{Adjustments and comparisons}\label{adjustments-and-comparisons}}

It's fairly common to make adjustments to the index, perhaps in terms of alternative data sets, indicators, methodological decisions, and so on. COINr allows you to (a) make fast adjustments, and (b) to compare alternative versions of the index relatively easily.

\hypertarget{regeneration}{%
\section{Regeneration}\label{regeneration}}

One of the key advantages of working within the COINrverse is that (nearly) all the methodology that is applied when building a composite indicator (COIN) is stored automatically in a folder of the COIN called \texttt{.\$Method}. To see what this looks like:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load COINr if not loaded}
\FunctionTok{library}\NormalTok{(COINr)}

\CommentTok{\# build example COIN}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Missing data points detected = 65}
\DocumentationTok{\#\# Missing data points imputed = 65, using method = indgroup\_mean}

\CommentTok{\# look in ASEM$Method folder in R Studio...}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/method_folder} 

}

\caption{Method folder in ASEM example}\label{fig:unnamed-chunk-54}
\end{figure}

The easiest way to view it is by looking in the viewer of R Studio as in the screenshot above. Essentially, the \texttt{.\$Method} folder has one entry for each COINr function that was used to build the COIN, and inside each of these folders are the arguments to the function that were input. Notice that the names inside \texttt{.\$Method\$denominate} correspond exactly to arguments to \texttt{denominate()}, for example.

Every time a COINr ``construction function'' is run, the inputs to the function are automatically recorded inside the COIN. Construction functions are any of the following seven:

\begin{longtable}[]{@{}ll@{}}
\toprule
Function & Description \\
\midrule
\endhead
\texttt{assemble()} & Assembles indicator data/metadata into a COIN \\
\texttt{checkData()} & Data availability check and unit screening \\
\texttt{denominate()} & Denominate (divide) indicators by other indicators \\
\texttt{impute()} & Impute missing data using various methods \\
\texttt{treat()} & Treat outliers with Winsorisation and transformations \\
\texttt{normalise()} & Normalise data using various methods \\
\texttt{aggregate()} & Aggregate indicators into hierarchical levels, up to index \\
\bottomrule
\end{longtable}

These are the core functions that are used to build a composite indicator, from assembling from the original data, up to aggregation.

One reason to do this is simply to have a record of what you did to arrive at the results. However, this is not the main reason (and in fact, it would be anyway good practice to make your own record by creating a script or markdown doc which records the steps). The real advantage is that results can be automatically regenerated with a handy function called \texttt{regen()}.

To regenerate the results, simply run e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM2 }\OtherTok{\textless{}{-}} \FunctionTok{regen}\NormalTok{(ASEM)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Missing data points detected = 65}
\DocumentationTok{\#\# Missing data points imputed = 65, using method = indgroup\_mean}
\end{Highlighting}
\end{Shaded}

The \texttt{regen()} function reruns all functions that are recorded in the \texttt{.\$Method} folder \emph{in the order they appear in the folder}. In this example, it runs, in order, \texttt{assemble()}, \texttt{denominate()}, \texttt{impute()}, \texttt{treat()}, \texttt{normalise()}and \texttt{aggregate()}. This replicates exactly the results. But what is the point of that? Well, it means that we can make changes to the index, generally by altering parameters in the \texttt{.\$Method} folder, and then rerun everything very quickly with a single command. This will be demonstrated in the following section. After that, we will also see how to compare between alternative versions of the index.

Before that, there is one extra feature of \texttt{regen()} which is worth mentioning. The \texttt{regen()} function only runs the seven construction functions as listed above. These functions are very flexible and should encompass most needs for constructing a composite indicator. But what happens if you want to build in an extra operation, or operations, that are outside of these seven functions?

It is possible to also include ``custom'' chunks of code inside the COIN. Custom chunks should be written manually using the \texttt{quote()} function, to a special folder \texttt{.\$Method\$Custom}. These chunks can be any type of code, but the important thing is to also know \emph{when} to run the code, i.e.~at what point in the construction process.

More specifically, custom code chunks are written as a named list. The \textbf{name} of the item in the list specifies \emph{when} to perform the operation. For example, ``after\_treat'' means to perform this immediately after the treatment step. Other options are e.g.~``after\_normalise'' or ``after\_impute'' -- in general it should be ``after\_'' followed by a name of one of the names of the construction functions.

The corresponding \textbf{value} in the list should be an operation which \emph{must} be enclosed in the \texttt{quote()} function. Clearly, the list can feature multiple operations at different points in construction.

This is slightly complicated, but can be clarified a bit with an example. Let's imagine that after Winsorisation, we would like to ``reset'' one of the Winsorised points to its original value. This is currently not possible inside the \texttt{treat()} function so has to be done manually. But we would like this operation to be kept when we regenerate the index with variations in methodology (e.g.~trying different aggregation functions, weights, etc.).

We create a list with one operation: resetting the point after the treatment step.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create list. }\AlertTok{NOTE}\CommentTok{ the use of the quote() function!}
\NormalTok{custlist }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{after\_treat =} \FunctionTok{quote}\NormalTok{(ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Treated}\SpecialCharTok{$}\NormalTok{Bord[ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Treated}\SpecialCharTok{$}\NormalTok{UnitCode}\SpecialCharTok{==}\StringTok{"BEL"}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ ASEM2}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Imputed}\SpecialCharTok{$}\NormalTok{Bord[ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Imputed}\SpecialCharTok{$}\NormalTok{UnitCode}\SpecialCharTok{==}\StringTok{"BEL"}\NormalTok{]))}

\CommentTok{\# Add the list to the $Method$Custom folder}
\NormalTok{ASEM2}\SpecialCharTok{$}\NormalTok{Method}\SpecialCharTok{$}\NormalTok{Custom }\OtherTok{\textless{}{-}}\NormalTok{ custlist}
\end{Highlighting}
\end{Shaded}

Specifically, we have replaced the Winsorised value of Belgium, for the ``Bord'' indicator, with its imputed value (i.e.~the value it had before it was treated).

Now, when we regenerate the COIN using \texttt{regen()}, it will insert this extra line of code immediately after the treatment step.

Using custom code may seem a bit confusing but it adds an extra layer of flexibility. It is however intended for small snippets of custom code, rather than large blocks of custom operations. If you are doing a lot of operations outside COINr, it may be better to do this in your own dedicated script or function, rather than trying to encode absolutely everything inside the COIN.

\hypertarget{adjustments}{%
\section{Adjustments}\label{adjustments}}

Let us now explore how the COIN can be adjusted and regenerated. This will (hopefully) clarify why regenerating is a useful thing.

The general steps for adjustments are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Copy the object
\item
  Adjust the index methodology or data by editing the \texttt{.\$Method} folder and/or the underlying data
\item
  Regenerate the results
\item
  Compare alternatives
\end{enumerate}

Copying the object is straightforward, and regeneration has been dealt with in the previous section. Comparison is also addressed in the following section. Here we will focus on adjustments and changing methodology.

\hypertarget{addingremoving-indicators}{%
\subsection{Adding/removing indicators}\label{addingremoving-indicators}}

First of all, let's consider an alternative index where we decide to add or remove one or more indicators. There are different ways that we could consider doing this.

A first possibility would be to manually create a new data frame of indicator data and indicator metadata, i.e.~the \texttt{IndData} and \texttt{IndMeta} inputs for \texttt{assemble()}. This is fine, but we would have to start the index again from scratch and rebuild it.

A better idea is that when we first supply the set of indicators to \texttt{assemble()}, we include all indicators that we might possibly want to include in the index, including e.g.~alternative indicators with alternative data sources. Then we build different versions of the index using subsets of these indicators. This allows us to use \texttt{regen()} and therefore to make fast copies of the index.

To illustrate, consider again the ASEM example. We can rebuild the ASEM index using a subset of the indicators by using the \texttt{include} or \texttt{exclude} arguments of the \texttt{assemble()} function. Because these are stored in \texttt{.\$Method}, we can easily regenerate the new results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make a copy}
\NormalTok{ASEM\_NoLPIShip }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}

\CommentTok{\# Edit method: exclude two indicators}
\NormalTok{ASEM\_NoLPIShip}\SpecialCharTok{$}\NormalTok{Method}\SpecialCharTok{$}\NormalTok{assemble}\SpecialCharTok{$}\NormalTok{exclude }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"LPI"}\NormalTok{, }\StringTok{"Ship"}\NormalTok{)}

\CommentTok{\# Regenerate results (suppress any messages)}
\NormalTok{ASEM\_NoLPIShip }\OtherTok{\textless{}{-}} \FunctionTok{regen}\NormalTok{(ASEM\_NoLPIShip, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that, in the ASEM example, by default, \texttt{ASEM\$Method\$assemble} doesn't exist because the \texttt{include} or \texttt{exclude} arguments of \texttt{assemble()} are empty, and these are the only arguments to \texttt{assemble()} that are recorded in \texttt{.\$Method\$assemble}.

In summary, we have removed two indicators, then regenerated the results using exactly the same methodology as used before. Importantly, \texttt{include} and \texttt{exclude} operate \emph{relative to the original data} input to \texttt{assemble()}, i.e.~the data found in \texttt{.\$Input\$Original}. This means that if we now were to make a copy of the version excluding the two indicators above, and exclude another different indicator:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make a copy}
\NormalTok{ASEM\_NoBord }\OtherTok{\textless{}{-}}\NormalTok{ ASEM\_NoLPIShip}

\CommentTok{\# Edit method: exclude two indicators}
\NormalTok{ASEM\_NoBord}\SpecialCharTok{$}\NormalTok{Method}\SpecialCharTok{$}\NormalTok{assemble}\SpecialCharTok{$}\NormalTok{exclude }\OtherTok{\textless{}{-}} \StringTok{"Bord"}

\CommentTok{\# Regenerate results}
\NormalTok{ASEM\_NoBord }\OtherTok{\textless{}{-}} \FunctionTok{regen}\NormalTok{(ASEM\_NoBord, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{ASEM\_NoBord}\SpecialCharTok{$}\NormalTok{Parameters}\SpecialCharTok{$}\NormalTok{IndCodes}
\DocumentationTok{\#\#  [1] "Goods"     "Services"  "FDI"       "PRemit"    "ForPort"   "CostImpEx"}
\DocumentationTok{\#\#  [7] "Tariff"    "TBTs"      "TIRcon"    "RTAs"      "Visa"      "StMob"    }
\DocumentationTok{\#\# [13] "Research"  "Pat"       "CultServ"  "CultGood"  "Tourist"   "MigStock" }
\DocumentationTok{\#\# [19] "Lang"      "LPI"       "Flights"   "Ship"      "Elec"      "Gas"      }
\DocumentationTok{\#\# [25] "ConSpeed"  "Cov4G"     "Embs"      "IGOs"      "UNVote"    "Renew"    }
\DocumentationTok{\#\# [31] "PrimEner"  "CO2"       "MatCon"    "Forest"    "Poverty"   "Palma"    }
\DocumentationTok{\#\# [37] "TertGrad"  "FreePress" "TolMin"    "NGOs"      "CPI"       "FemLab"   }
\DocumentationTok{\#\# [43] "WomParl"   "PubDebt"   "PrivDebt"  "GDPGrow"   "RDExp"     "NEET"}
\end{Highlighting}
\end{Shaded}

\ldots we see that ``LPI'' and ``Ship'' are once again present.

In fact, COINr has an even quicker way to add and remove indicators, which is a short cut function called \texttt{indChange()}. In one command you can add or remove indicators and regenerate the results. Unlike the method above, \texttt{indChange()} also adds and removes relative to the existing index, which may be more convenient in some circumstances. To demonstrate this, we can use the same example as above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make a copy}
\NormalTok{ASEM\_NoBord2 }\OtherTok{\textless{}{-}} \FunctionTok{indChange}\NormalTok{(ASEM\_NoLPIShip, }\AttributeTok{drop =} \StringTok{"Bord"}\NormalTok{, }\AttributeTok{regen =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\# COIN has been regenerated using new specs.}

\NormalTok{ASEM\_NoBord2}\SpecialCharTok{$}\NormalTok{Parameters}\SpecialCharTok{$}\NormalTok{IndCodes}
\DocumentationTok{\#\#  [1] "Goods"     "Services"  "FDI"       "PRemit"    "ForPort"   "CostImpEx"}
\DocumentationTok{\#\#  [7] "Tariff"    "TBTs"      "TIRcon"    "RTAs"      "Visa"      "StMob"    }
\DocumentationTok{\#\# [13] "Research"  "Pat"       "CultServ"  "CultGood"  "Tourist"   "MigStock" }
\DocumentationTok{\#\# [19] "Lang"      "Flights"   "Elec"      "Gas"       "ConSpeed"  "Cov4G"    }
\DocumentationTok{\#\# [25] "Embs"      "IGOs"      "UNVote"    "Renew"     "PrimEner"  "CO2"      }
\DocumentationTok{\#\# [31] "MatCon"    "Forest"    "Poverty"   "Palma"     "TertGrad"  "FreePress"}
\DocumentationTok{\#\# [37] "TolMin"    "NGOs"      "CPI"       "FemLab"    "WomParl"   "PubDebt"  }
\DocumentationTok{\#\# [43] "PrivDebt"  "GDPGrow"   "RDExp"     "NEET"}
\end{Highlighting}
\end{Shaded}

And here we see that now, ``Bord'' has been excluded \emph{as well as} ``LPI'' and ``Ship''.

\hypertarget{other-adjustments}{%
\subsection{Other adjustments}\label{other-adjustments}}

We can make any methodological adjustments we want by editing any parameters in \texttt{.\$Method} and then running \texttt{regen()}. For example, we can change the imputation method:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make a copy}
\NormalTok{ASEMAltImpute }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}

\CommentTok{\# Edit .$Method}
\NormalTok{ASEMAltImpute}\SpecialCharTok{$}\NormalTok{Method}\SpecialCharTok{$}\NormalTok{impute}\SpecialCharTok{$}\NormalTok{imtype }\OtherTok{\textless{}{-}} \StringTok{"indgroup\_median"}

\CommentTok{\# Regenerate}
\NormalTok{ASEMAltImpute }\OtherTok{\textless{}{-}} \FunctionTok{regen}\NormalTok{(ASEMAltImpute, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We could also change the normalisation method, e.g.~to use Borda scores:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make a copy}
\NormalTok{ASEMAltNorm }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}

\CommentTok{\# Edit .$Method}
\NormalTok{ASEMAltNorm}\SpecialCharTok{$}\NormalTok{Method}\SpecialCharTok{$}\NormalTok{normalise}\SpecialCharTok{$}\NormalTok{ntype }\OtherTok{\textless{}{-}} \StringTok{"borda"}

\CommentTok{\# Regenerate}
\NormalTok{ASEMAltNorm }\OtherTok{\textless{}{-}} \FunctionTok{regen}\NormalTok{(ASEMAltNorm, }\AttributeTok{quietly =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and of course this extends to any parameters of any of the ``construction'' functions. We can even alter the underlying data directly if we want, e.g.~by altering values in \texttt{.\$Input\$Original\$Data}. In short, anything inside the COIN can be edited and then the results regenerated. This allows a fast way to make different alternative indexes and explore the effects of different methodology very quickly.

\hypertarget{comparisons}{%
\section{Comparisons}\label{comparisons}}

A logical follow up to making alternative indexes is to try to understand the differences between these indexes. This can of course be done manually. But to make this quicker, COINr includes a few tools to quickly inspect the differences between different COINs.

In using these tools it should be fairly evident that comparisons are made between different versions of the same index. So the two indexes must have at least some units in common. The tools are intended for methodological variations of the type shown previously in this chapter - different aggregation, weighting, adding and removing indicators and so on. That said, since comparisons are made on units, it would also be possible to compare two totally different COINs, as long as they have units in common.

To begin with, we can make a simple bilateral comparison between two COINs. Taking two of the index versions created previously:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{compTable}\NormalTok{(ASEM, ASEMAltNorm, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Index"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r|r|r}
\hline
  & UnitCode & UnitName & RankCOIN1 & RankCOIN2 & RankChange & AbsRankChange\\
\hline
43 & PRT & Portugal & 27 & 16 & 11 & 11\\
\hline
29 & LAO & Lao PDR & 48 & 39 & 9 & 9\\
\hline
33 & MLT & Malta & 10 & 19 & -9 & 9\\
\hline
14 & EST & Estonia & 22 & 15 & 7 & 7\\
\hline
21 & IDN & Indonesia & 43 & 49 & -6 & 6\\
\hline
13 & ESP & Spain & 19 & 24 & -5 & 5\\
\hline
19 & HRV & Croatia & 18 & 23 & -5 & 5\\
\hline
17 & GBR & United Kingdom & 15 & 11 & 4 & 4\\
\hline
30 & LTU & Lithuania & 16 & 12 & 4 & 4\\
\hline
35 & MNG & Mongolia & 44 & 48 & -4 & 4\\
\hline
\end{tabular}

The \texttt{compTable()} function allows a rank comparison of a single indicator or aggregate, between two COINs. The COINs must both share the indicator code which is assigned to \texttt{isel}. By default the output table (data frame) is sorted by the highest absolute rank change downwards, but it can be sorted by the other columns using the \texttt{sort\_by} argument:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{compTable}\NormalTok{(ASEM, ASEMAltNorm, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Index"}\NormalTok{, }\AttributeTok{sort\_by =} \StringTok{"RankCOIN1"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|r|r|r|r}
\hline
  & UnitCode & UnitName & RankCOIN1 & RankCOIN2 & RankChange & AbsRankChange\\
\hline
7 & CHE & Switzerland & 1 & 1 & 0 & 0\\
\hline
37 & NLD & Netherlands & 2 & 4 & -2 & 2\\
\hline
12 & DNK & Denmark & 3 & 2 & 1 & 1\\
\hline
38 & NOR & Norway & 4 & 3 & 1 & 1\\
\hline
3 & BEL & Belgium & 5 & 6 & -1 & 1\\
\hline
49 & SWE & Sweden & 6 & 5 & 1 & 1\\
\hline
2 & AUT & Austria & 7 & 7 & 0 & 0\\
\hline
31 & LUX & Luxembourg & 8 & 10 & -2 & 2\\
\hline
11 & DEU & Germany & 9 & 8 & 1 & 1\\
\hline
33 & MLT & Malta & 10 & 19 & -9 & 9\\
\hline
\end{tabular}

Why use ranks as a comparison and not scores? As explained elsewhere in this documentation, a different normalisation method or aggregation method can result in very different scores for the same unit. Since scores have no units, the scale is in many ways arbitrary, but ranks are a consistent way of comparing different versions.

For comparisons between more than two COINs, the \texttt{compTableMulti()} function can be used.

\hypertarget{sensitivity-analysis}{%
\chapter{Sensitivity analysis}\label{sensitivity-analysis}}

Composite indicators, like any model, have many associated uncertainties.

\emph{Sensitivity analysis} can help to quantify the uncertainty in the scores and rankings of the composite indicator, and to identify which assumptions are driving this uncertainty, and which are less important.

\hypertarget{about}{%
\section{About}\label{about}}

Sensitivity analysis is often confused with \emph{uncertainty analysis}. Uncertainty analysis involves estimating the uncertainty in the ouputs of a system (here, the scores and ranks of the composite indicator), given the uncertainties in the inputs (here, methodological decisions, weights, etc.). The results of an uncertainty include for example confidence intervals over the ranks, median ranks, and so on.

Sensitivity analysis is an extra step after uncertainty analysis, and estimates which of the input uncertainties are driving the output uncertainty, and by how much. A rule of thumb, known as the \href{https://en.wikipedia.org/wiki/Pareto_principle}{Pareto Principle} (or the 80/20 Rule) suggests that often, only a small proportion of the input uncertainties are causing the majority of the output uncertainty. Sensitivity analysis allows us to find which input uncertainties are significant (and therefore perhaps worthy of extra attention), and which are not important.

In reality, sensitivity analysis and uncertainty analysis can be performed simultaneously. However in both cases, the main technique is to use Monte Carlo methods. This essentially involves re-calculating the composite indicator many times, each time randomly varying the uncertain variables (assumptions, parameters), in order to estimate the output distributions.

At first glance, one might think that sensitivity analysis can be performed by switching one assumption at a time, using the tools outlined in \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons}. However, uncertainties interact with one another, and to properly understand the impact of uncertainties, one must vary uncertain parameters and assumptions \emph{simultaneously}.

Sensitivity analysis and uncertainty analysis are large topics and are in fact research fields in their own right. To better understand them, a good starting point is \href{https://onlinelibrary.wiley.com/doi/book/10.1002/9780470725184}{Global Sensitivity Analysis: The Primer}, and a recent summary of sensitivity analysis research can be found \href{https://www.sciencedirect.com/science/article/pii/S1364815220310112?via\%3Dihub}{here}.

\hypertarget{five-steps}{%
\section{Five steps}\label{five-steps}}

To perform an uncertainty or sensitivity analysis, one must define several things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The system or model (in this case it is a composite indicator, represented as a COIN)
\item
  Which assumptions to treat as uncertain
\item
  The alternative values or distributions assigned to each uncertain assumption
\item
  Which output or outputs to target (i.e.~to calculate confidence intervals for)
\item
  Methodological specifications for the sensitivity analysis itself, for example the method and the number of replications to run.
\end{enumerate}

This should dispel the common idea that one can simply ``run a sensitivity analysis''. In fact, all of these steps require some thought and attention, and the results of the sensitivity analysis will be themselves dependent on these choices. Let's go through them one by one.

\hypertarget{specifying-the-model}{%
\subsection{Specifying the model}\label{specifying-the-model}}

First, the \textbf{system or model}. This should be clear: you need to have already built your (nominal) composite indicator in order to check the uncertainties. Usually this would involve calculating the results up to and including the aggregated index. The choices in this model should represent your ``best'' choices for each methodological step, for example your preferred aggregration method, preferred set of weights, and so on.

\hypertarget{which-assumptions-to-vary}{%
\subsection{Which assumptions to vary}\label{which-assumptions-to-vary}}

Specifying \textbf{which assumptions to vary} is more complicated. It is impossible to fully quantify the uncertainty in a composite indicator (or any model, for that matter) because there are simply so many sources of uncertainty, ranging from the input data, the choice of indicators, the structure of the index, and all the methodological steps along the way (imputation, treatment, normalisation, etc.). A reasonable approach is to identify specific assumptions and parameters that could have plausible alternatives, and can be practically varied.

The construction of composite indicators in COINr (deliberately) lends itself well to uncertainty analysis, because as we have seen in the \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons} chapter, all the methodological choices used to build the composite indicator are recorded inside the COIN, and changes can be made by simply altering these parameters and calling the \texttt{regen()} function. Sensitivity and uncertainty analysis is simply an extension of this concept - where we create a large number of alternative COINs, each with methodological variations following the distributions assigned to each uncertain assumption.

The upshot here is that (at least in theory) any parameter from any of the construction functions (see again the table in \protect\hyperlink{adjustments-and-comparisons}{Adjustments and comparisons}) can be varied as part of a sensitivity analysis. This includes, for example:

\begin{itemize}
\tightlist
\item
  Inclusion and exclusion of indicators
\item
  Data availability thresholds for screening units
\item
  Alternative choices of denominators
\item
  Alternative imputation methods
\item
  Alternative data treatment, Winsorisation thresholds, skew and kurtosis values, transformations
\item
  Alternative normalisation methods and parameters
\item
  Alternative aggregration methods and weight sets
\end{itemize}

On top of this, it is possible to randomly perturb weights at each level by a specified noise factor. This is explained in more detail later in this chapter.

The reason that these can be varied \emph{in theory} is because there may arise conflicts between methodological choices. For example, if a normalisation method results in negative values, we cannot use a default geometric mean aggregation method. For this reason, it is recommended to focus on key uncertainties and start modestly with a sensitivity analysis, working up to a more complex version if required.

\hypertarget{alternative-values}{%
\subsection{Alternative values}\label{alternative-values}}

Having selected some key uncertain assumptions, we must assign plausible alternative values. For example, let's say that an uncertain assumption is the normalisation method. By default, we have used min-max, but perhaps other methods could be reasonable alternatives. The question is then, which alternatives to test?

The answer here should not be to blindly apply all possible alternatives available, but rather to select some alternatives that represent \textbf{plausible} alternatives, ruling out any that do not fit the requirements of the index. For example, using rank normalisation is a robust method that neatly deals with outliers but by doing so, also ignores whether a unit is an exceptional performer. This may be good or bad depending on what you want to capture. If it is ``good'' then it could be considered as an alternative. If it does not fit the objectives, it is not a plausible alternative, so should not be included in the sensitivity analysis.

Finding plausible alternatives is not necessarily an easy task, and we must recall that in any case we will end up with a lower bound on the uncertainty, since we cannot fully test all uncertainties, as discussed previously (we recall that this is the same in any model, not just in composite indicators). Yet, we can still do a lot better than no uncertainty analysis at all.

In the end, for each uncertain parameter, we should end up with a list of alternative plausible values. Note that at the moment, COINr assumes equal probability for all alternatives, i.e.~uniform distributions. This may be extended to other distributions in future releases.

\hypertarget{selecting-the-output}{%
\subsection{Selecting the output}\label{selecting-the-output}}

Composite indicators have multidimensional outputs - one value for each unit and for each aggregate or normalised indicator. Typically, the most interesting outputs to look at in the context of a sensitivity analysis are the final index values, and possibly some of the underlying aggregate scores (sub-indexes, pillars, etc.).

COINr allows us to select which inputs to target, and this is explained more below.

\hypertarget{sa-methodology}{%
\subsection{SA methodology}\label{sa-methodology}}

Finally, we have to specify what kind of analysis to perform, and how many model runs. Currently, COINr offers either an uncertainty analysis (resulting in distributions over ranks), or a sensitivity analysis (additionally showing which input uncertainties cause the most output uncertainty). As mentioned, sensitivity analysis methodology is a rabbit hole in itself, and interested readers could refer to the references at the beginning of this chapter to find out more.

Sensitivity analysis usually requires more model runs (replications of the composite indicator). Still, composite indicators are fairly cheap to evaluate, depending on the number of indicators and the complexity of the construction. Regardless of whether you run an uncertainty or sensitivity analysis, more model runs is always better because it increases the accuracy of the estimations of the distributions over ranks. If you are prepared to wait some minutes or possibly hour(s), normally this is enough to perform a fairly solid sensitivity analysis.

\hypertarget{variance-based-sensitivity-analysis}{%
\section{Variance-based sensitivity analysis}\label{variance-based-sensitivity-analysis}}

COINr is almost certainly the only package in any language which allows a full variance-based (global) sensitivity analysis on a composite indicator. However, this means that variance-based sensitivity analysis needs to be briefly explained.

Variance-based sensitivity analysis is largely considered as the ``gold standard'' in testing the effects of uncertainties in modelling and more generally in systems. Briefly, the central idea is that the uncertainty in a single output \(y\) of a model can be encapsulated as its variance \(V(y)\) - the greater the variance is, the more uncertain the output.

In a \href{https://doi.org/10.1016/S0378-4754(00)00270-6}{seminal paper}, Russian mathematician Ilya Sobol' showed that this output variance can be decomposed into chunks which are attributable to each uncertain input and interactions between inputs. Letting \(x_i\) denote the \(i\)th assumption to be varied, and \(k\) the number of uncertain assumptions:

\[ V(y)=\sum_i V_i+\sum_i\sum_{j>i}V_{i,j}+...+V_{1,2,...,k}, \]

where

\[ V_i= V[E(y|x_i)], \\ V_{i,j}=V[E(y|x_i,x_j)] - V[E(y|x_i)]-V[E(y|x_j)] \]

and so on for higher terms. Here, \(V(\cdot)\) denotes the variance operator, \(E(\cdot)\) the expected value, and these terms are used directly as \emph{sensitivity indices}, e.g.~the \emph{first order sensitivity index} \(S_{i}=V_i/V(y)\) measures the contribution of the input \(x_i\) to \(V(y)\), without including interactions with other inputs. Since they are standardised by \(V(y)\), each sensitivity index measures the \emph{fraction of the variance} caused by each input (or interactions between inputs), and therefore the \emph{fraction of the uncertainty}.

In the same paper, Sobol' also showed how these sensitivity indices can be estimated using a Monte Carlo design. This Monte Carlo design (running the composite indicator many times with a particular combination of input values) is implemented in COINr. This follows the methodology described in \href{https://doi.org/10.1111/j.1467-985X.2005.00350.x}{this paper}, although due to the difficulty of implementing it, it has not been used very extensively in practice.

In any case, the exact details of variance-based sensitivity analysis are better described elsewhere. Here, the important points are how to interpret the sensitivity indices. COINr produces two indices for each uncertain assumption. The first one is the \emph{first order sensitivity index}, which is the fraction of the output variance caused by each uncertain input assumption alone and is defined as:

\[ S_i = \frac{V[E(y|x_i)]}{V(y)} \]

Importantly, this is \emph{averaged over variations in other input assumptions}. In other words, it is not the same as simply varying that assumption alone and seeing what happens (this result is dependent on the values of the other assumptions).

The second measure is the \emph{total order sensitivity index}, sometimes called the \emph{total effect index}, and is:

\[ S_{Ti} = 1 - \frac {V[E\left(y \mid \textbf{x}_{-i} \right)]}{V(y)} = \frac {E[V\left(y \mid
    \textbf{x}_{-i} \right)]}{V(y)}  \]

where \(\textbf{x}_{-i}\) is the set of all uncertain inputs except the \(i\)th. The quantity \(S_{Ti}\) measures the fraction of the output variance caused by \(x_i\) \emph{and any interactions with other assumptions}.

Depending on your background, this may or may not seem a little confusing. Either way, see the examples in the rest of this chapter, which show that once calculated, the sensitivity indices are fairly intuitive.

\hypertarget{sensitivity-in-coinr}{%
\section{Sensitivity in COINr}\label{sensitivity-in-coinr}}

To run a sensitivity analysis in COINr, the function of interest is \texttt{sensitivity()}. We must follow the steps outlined above, so first, you should have a nominal composite indicator constructed up to and including the aggregation step. Here, we use our old friend the ASEM index.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}
\CommentTok{\# build ASEM index}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Missing data points detected = 65}
\DocumentationTok{\#\# Missing data points imputed = 65, using method = indgroup\_mean}
\end{Highlighting}
\end{Shaded}

\hypertarget{general-specifications}{%
\subsection{General specifications}\label{general-specifications}}

Next, we have to decide which parameters to treat as uncertain. Here, we will consider three things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The imputation method
\item
  The normalisation method
\item
  The weights
\end{enumerate}

These are chosen as a limited set to keep things relatively simple, and because imputation is always a significant uncertainty (we are basically guessing data points). We will also consider a couple of alternative normalisation methods. Finally, we will randomly perturb the weights.

For the distributions (i.e.~the plausible alternatives), we will consider the following:

\begin{itemize}
\tightlist
\item
  Imputation using either indicator group mean, indicator mean, or no imputation
\item
  Normalisation using either min-max, rank, or distance to maximum
\item
  Perturb pillar and sub index weights by +/-25\%
\end{itemize}

Finally, we will consider the index as the target.

Let's enter these specifications. The most complex part is specifying the \texttt{SA\_specs} argument of the \texttt{sensitivity()} function, which specifies which parameters to vary and which alternative values to use. To explain how this works, the easiest way is to construct the specifications for our example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define noise to be applied to weights}
\NormalTok{nspecs }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{AgLevel =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{NoiseFactor =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{,}\FloatTok{0.25}\NormalTok{))}

\CommentTok{\# create list specifying assumptions to vary and alternatives}
\NormalTok{SAspecs }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{impute =} \FunctionTok{list}\NormalTok{(}\AttributeTok{imtype =} \FunctionTok{c}\NormalTok{(}\StringTok{"indgroup\_mean"}\NormalTok{, }\StringTok{"ind\_mean"}\NormalTok{, }\StringTok{"none"}\NormalTok{)),}
  \AttributeTok{normalise =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ntype =} \FunctionTok{c}\NormalTok{(}\StringTok{"minmax"}\NormalTok{, }\StringTok{"rank"}\NormalTok{, }\StringTok{"dist2max"}\NormalTok{)),}
  \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(}\AttributeTok{NoiseSpecs =}\NormalTok{ nspecs, }\AttributeTok{Nominal =} \StringTok{"Original"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Leaving aside the \texttt{nspecs} line for now (but see below for details), we focus on the list \texttt{SAspecs}, which will be used as the \texttt{SA\_specs} argument of \texttt{sensitivity()}. The \emph{names} of the first level of this list should be any of the seven construction functions. Each named element of the list is itself a list, which specifies the parameters of that function to vary, and the alternative values. In summary, the general format is:

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{\textless{}}\NormalTok{function\_name}\SpecialCharTok{\textgreater{}} \ErrorTok{=} \FunctionTok{list}\NormalTok{(}\SpecialCharTok{\textless{}}\NormalTok{parameter name}\SpecialCharTok{\textgreater{}} \ErrorTok{=} \ErrorTok{\textless{}}\NormalTok{vector or list of alternatives}\SpecialCharTok{\textgreater{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There is no restriction on the number of parameters that can be varied, or the number of alternatives. You can have multiple parameters from the same function, for example.

\hypertarget{varying-weights}{%
\subsection{Varying weights}\label{varying-weights}}

The exception to the above format is regarding weights. Weights are input as an argument of the \texttt{aggregate()} function, but since this is a single argument, it only behaves as a single parameter. That means that we could test alternative weight sets as part of the sensitivity analysis:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# example entry in SA\_specs list of sensitivity() [not used here]}
\NormalTok{aggregate }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{agweights =} \FunctionTok{c}\NormalTok{(}\StringTok{"Weights1"}\NormalTok{, }\StringTok{"Weights2"}\NormalTok{, }\StringTok{"Weights3"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

where ``Weights1'' and friends are alternative sets of weights that we have stored in \texttt{.\$Parameters\$Weights}. This is a useful test but only gives a limited picture of uncertainty because we test between a small set of alternatives. It may be the best approach if we have two or three fairly clear alternatives for weighting (note: this can also be a way to exclude indicators or even entire aggregation groups, by setting certain weights to zero).

If the uncertainty is more general, i.e.~we have elicited weights but we feel that there is a reasonable degree of uncertainty surrounding all weight values (which usually there is), COINr also includes the option to apply random ``noise'' to the weights. With this approach, for each replication of the composite indicator, a random value is added to each weight, of the form:

\[ w'_i = w_i + \epsilon_i, \; \; \epsilon_i \sim U[-\phi w_i, \phi w_i] \]

where \(w_i\) is a weight, \(\epsilon_i\) is the added noise, and \(\phi\) is a ``noise factor''. This means that if we set \(\phi = 0.25\), for example, it would let \(w_i\) vary between +/-25\% of its nominal value, following a uniform distribution.

The noise factor can be different from one aggregation level to another in COINr. That means we can choose which aggregation level weights to apply noise to, and how much for each level. To specify this in the \texttt{sensitivity()} function, we add it as a ``special'' \texttt{weights} entry in the \texttt{SAspecs} list described above. It is special in the sense that this is the only entry that is allowed that is not a construction function name.

The weights entry was already defined above, but let's look at it again here:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define noise to be applied to weights}
\NormalTok{nspecs }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{AgLevel =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\AttributeTok{NoiseFactor =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{,}\FloatTok{0.25}\NormalTok{))}

\NormalTok{nspecs}

\CommentTok{\# create list specifying assumptions to vary and alternatives}
\NormalTok{weights }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{NoiseSpecs =}\NormalTok{ nspecs,}
               \AttributeTok{Nominal =} \StringTok{"Original"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The weights list has two entries. The first is a data frame where each row is the specifications for noise to apply to the weights of an aggregation level. It has two columns: \texttt{AgLev} which is the aggregation level, and \texttt{NoiseFactor}, which is the noise factor described above. In the example here, we have specified that at aggregation levels 2 and 3 (pillars and sub-indexes), there should be noise factors of 0.25 (+/-25\% of nominal weight values). Indicator weights remain always at nominal values since they are not specified in this table.

The second entry, \texttt{Nominal}, is the name of the weight set to use as the nominal values (i.e.~the baseline to apply the noise to). This should correspond to a weight set present in \texttt{.\$Parameters\$Weights}. Here, we have set it to \texttt{Original}, which is the original weights that were input to \texttt{assemble()}.

\hypertarget{running-an-uncertainty-analysis}{%
\subsection{Running an uncertainty analysis}\label{running-an-uncertainty-analysis}}

To actually run the analysis, we call the \texttt{sensitivity()} function, using the specifications in the \texttt{SAspecs} list, and setting \texttt{v\_targ\ =\ "Index"}, meaning that the index will be the target of the sensitivity analysis. Further, we specify that there should be 100 replications (this number is kept fairly low to limit the time taken to compile this book), and that the type of analysis should be an uncertainty analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# run uncertainty analysis}
\NormalTok{SAresults }\OtherTok{\textless{}{-}} \FunctionTok{sensitivity}\NormalTok{(ASEM, }\AttributeTok{v\_targ =} \StringTok{"Index"}\NormalTok{,}
                         \AttributeTok{SA\_specs =}\NormalTok{ SAspecs,}
                         \AttributeTok{N =} \DecValTok{100}\NormalTok{,}
                         \AttributeTok{SA\_type =} \StringTok{"UA"}\NormalTok{)}
\DocumentationTok{\#\# Iteration 1 of 100 ... 1\% complete}
\DocumentationTok{\#\# Iteration 2 of 100 ... 2\% complete}
\DocumentationTok{\#\# Iteration 3 of 100 ... 3\% complete}
\DocumentationTok{\#\# Iteration 4 of 100 ... 4\% complete}
\DocumentationTok{\#\# Iteration 5 of 100 ... 5\% complete}
\DocumentationTok{\#\# Iteration 6 of 100 ... 6\% complete}
\DocumentationTok{\#\# Iteration 7 of 100 ... 7\% complete}
\DocumentationTok{\#\# Iteration 8 of 100 ... 8\% complete}
\DocumentationTok{\#\# Iteration 9 of 100 ... 9\% complete}
\DocumentationTok{\#\# Iteration 10 of 100 ... 10\% complete}
\DocumentationTok{\#\# Iteration 11 of 100 ... 11\% complete}
\DocumentationTok{\#\# Iteration 12 of 100 ... 12\% complete}
\DocumentationTok{\#\# Iteration 13 of 100 ... 13\% complete}
\DocumentationTok{\#\# Iteration 14 of 100 ... 14\% complete}
\DocumentationTok{\#\# Iteration 15 of 100 ... 15\% complete}
\DocumentationTok{\#\# Iteration 16 of 100 ... 16\% complete}
\DocumentationTok{\#\# Iteration 17 of 100 ... 17\% complete}
\DocumentationTok{\#\# Iteration 18 of 100 ... 18\% complete}
\DocumentationTok{\#\# Iteration 19 of 100 ... 19\% complete}
\DocumentationTok{\#\# Iteration 20 of 100 ... 20\% complete}
\DocumentationTok{\#\# Iteration 21 of 100 ... 21\% complete}
\DocumentationTok{\#\# Iteration 22 of 100 ... 22\% complete}
\DocumentationTok{\#\# Iteration 23 of 100 ... 23\% complete}
\DocumentationTok{\#\# Iteration 24 of 100 ... 24\% complete}
\DocumentationTok{\#\# Iteration 25 of 100 ... 25\% complete}
\DocumentationTok{\#\# Iteration 26 of 100 ... 26\% complete}
\DocumentationTok{\#\# Iteration 27 of 100 ... 27\% complete}
\DocumentationTok{\#\# Iteration 28 of 100 ... 28\% complete}
\DocumentationTok{\#\# Iteration 29 of 100 ... 29\% complete}
\DocumentationTok{\#\# Iteration 30 of 100 ... 30\% complete}
\DocumentationTok{\#\# Iteration 31 of 100 ... 31\% complete}
\DocumentationTok{\#\# Iteration 32 of 100 ... 32\% complete}
\DocumentationTok{\#\# Iteration 33 of 100 ... 33\% complete}
\DocumentationTok{\#\# Iteration 34 of 100 ... 34\% complete}
\DocumentationTok{\#\# Iteration 35 of 100 ... 35\% complete}
\DocumentationTok{\#\# Iteration 36 of 100 ... 36\% complete}
\DocumentationTok{\#\# Iteration 37 of 100 ... 37\% complete}
\DocumentationTok{\#\# Iteration 38 of 100 ... 38\% complete}
\DocumentationTok{\#\# Iteration 39 of 100 ... 39\% complete}
\DocumentationTok{\#\# Iteration 40 of 100 ... 40\% complete}
\DocumentationTok{\#\# Iteration 41 of 100 ... 41\% complete}
\DocumentationTok{\#\# Iteration 42 of 100 ... 42\% complete}
\DocumentationTok{\#\# Iteration 43 of 100 ... 43\% complete}
\DocumentationTok{\#\# Iteration 44 of 100 ... 44\% complete}
\DocumentationTok{\#\# Iteration 45 of 100 ... 45\% complete}
\DocumentationTok{\#\# Iteration 46 of 100 ... 46\% complete}
\DocumentationTok{\#\# Iteration 47 of 100 ... 47\% complete}
\DocumentationTok{\#\# Iteration 48 of 100 ... 48\% complete}
\DocumentationTok{\#\# Iteration 49 of 100 ... 49\% complete}
\DocumentationTok{\#\# Iteration 50 of 100 ... 50\% complete}
\DocumentationTok{\#\# Iteration 51 of 100 ... 51\% complete}
\DocumentationTok{\#\# Iteration 52 of 100 ... 52\% complete}
\DocumentationTok{\#\# Iteration 53 of 100 ... 53\% complete}
\DocumentationTok{\#\# Iteration 54 of 100 ... 54\% complete}
\DocumentationTok{\#\# Iteration 55 of 100 ... 55\% complete}
\DocumentationTok{\#\# Iteration 56 of 100 ... 56\% complete}
\DocumentationTok{\#\# Iteration 57 of 100 ... 57\% complete}
\DocumentationTok{\#\# Iteration 58 of 100 ... 58\% complete}
\DocumentationTok{\#\# Iteration 59 of 100 ... 59\% complete}
\DocumentationTok{\#\# Iteration 60 of 100 ... 60\% complete}
\DocumentationTok{\#\# Iteration 61 of 100 ... 61\% complete}
\DocumentationTok{\#\# Iteration 62 of 100 ... 62\% complete}
\DocumentationTok{\#\# Iteration 63 of 100 ... 63\% complete}
\DocumentationTok{\#\# Iteration 64 of 100 ... 64\% complete}
\DocumentationTok{\#\# Iteration 65 of 100 ... 65\% complete}
\DocumentationTok{\#\# Iteration 66 of 100 ... 66\% complete}
\DocumentationTok{\#\# Iteration 67 of 100 ... 67\% complete}
\DocumentationTok{\#\# Iteration 68 of 100 ... 68\% complete}
\DocumentationTok{\#\# Iteration 69 of 100 ... 69\% complete}
\DocumentationTok{\#\# Iteration 70 of 100 ... 70\% complete}
\DocumentationTok{\#\# Iteration 71 of 100 ... 71\% complete}
\DocumentationTok{\#\# Iteration 72 of 100 ... 72\% complete}
\DocumentationTok{\#\# Iteration 73 of 100 ... 73\% complete}
\DocumentationTok{\#\# Iteration 74 of 100 ... 74\% complete}
\DocumentationTok{\#\# Iteration 75 of 100 ... 75\% complete}
\DocumentationTok{\#\# Iteration 76 of 100 ... 76\% complete}
\DocumentationTok{\#\# Iteration 77 of 100 ... 77\% complete}
\DocumentationTok{\#\# Iteration 78 of 100 ... 78\% complete}
\DocumentationTok{\#\# Iteration 79 of 100 ... 79\% complete}
\DocumentationTok{\#\# Iteration 80 of 100 ... 80\% complete}
\DocumentationTok{\#\# Iteration 81 of 100 ... 81\% complete}
\DocumentationTok{\#\# Iteration 82 of 100 ... 82\% complete}
\DocumentationTok{\#\# Iteration 83 of 100 ... 83\% complete}
\DocumentationTok{\#\# Iteration 84 of 100 ... 84\% complete}
\DocumentationTok{\#\# Iteration 85 of 100 ... 85\% complete}
\DocumentationTok{\#\# Iteration 86 of 100 ... 86\% complete}
\DocumentationTok{\#\# Iteration 87 of 100 ... 87\% complete}
\DocumentationTok{\#\# Iteration 88 of 100 ... 88\% complete}
\DocumentationTok{\#\# Iteration 89 of 100 ... 89\% complete}
\DocumentationTok{\#\# Iteration 90 of 100 ... 90\% complete}
\DocumentationTok{\#\# Iteration 91 of 100 ... 91\% complete}
\DocumentationTok{\#\# Iteration 92 of 100 ... 92\% complete}
\DocumentationTok{\#\# Iteration 93 of 100 ... 93\% complete}
\DocumentationTok{\#\# Iteration 94 of 100 ... 94\% complete}
\DocumentationTok{\#\# Iteration 95 of 100 ... 95\% complete}
\DocumentationTok{\#\# Iteration 96 of 100 ... 96\% complete}
\DocumentationTok{\#\# Iteration 97 of 100 ... 97\% complete}
\DocumentationTok{\#\# Iteration 98 of 100 ... 98\% complete}
\DocumentationTok{\#\# Iteration 99 of 100 ... 99\% complete}
\DocumentationTok{\#\# Iteration 100 of 100 ... 100\% complete}
\DocumentationTok{\#\# Time elapsed = 14.71s, average 0.15s/rep.}
\end{Highlighting}
\end{Shaded}

Running a sensitivity/uncertainty analysis can be time consuming and depends on the complexity of the model and the number of runs specified. This particular analysis took less than a minute for me, but could take more or less time depending on the speed of your computer.

The output of the analysis is a list with several entries:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# see summary of analysis}
\FunctionTok{summary}\NormalTok{(SAresults)}
\DocumentationTok{\#\#            Length Class      Mode     }
\DocumentationTok{\#\# Scores     101    data.frame list     }
\DocumentationTok{\#\# Parameters   2    data.frame list     }
\DocumentationTok{\#\# Ranks      101    data.frame list     }
\DocumentationTok{\#\# RankStats    6    data.frame list     }
\DocumentationTok{\#\# Nominal      3    data.frame list     }
\DocumentationTok{\#\# t\_elapse     1    {-}none{-}     numeric  }
\DocumentationTok{\#\# t\_average    1    {-}none{-}     numeric  }
\DocumentationTok{\#\# ParaNames    3    {-}none{-}     character}
\end{Highlighting}
\end{Shaded}

We will go into more detail in a minute, but first we can plot the results of the uncertainty analysis using a dedicated function \texttt{plotSARanks()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotSARanks}\NormalTok{(SAresults)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/plotUAranks-1.pdf}

This plot orders the units by their nominal ranks, and plots the median rank across all the replications of the uncertainty analysis, as well as the 5th and 95th percentile rank values. The ranks are the focus of an uncertainty analysis because scores can change drastically depending on the method. Ranks are the only comparable metric across different composite indicator methodologies\footnote{This may give a more general hint that focusing on scores in composite indicators is usually not a good idea, because the scores can be extremely different depending on the methodology, while the ranks are much more stable}.

Let us now look at the elements in \texttt{SAresults}. The data frames \texttt{SAresults\$Scores} and \texttt{SAresults\$Ranks} give the scores and ranks respectively for each replication of the composite indicator. The \texttt{SAresults\$Parameters} data frame gives the uncertain parameter values used for each iteration:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(SAresults}\SpecialCharTok{$}\NormalTok{Parameters)}
\DocumentationTok{\#\#          imtype    ntype}
\DocumentationTok{\#\# 1 indgroup\_mean   minmax}
\DocumentationTok{\#\# 2      ind\_mean dist2max}
\DocumentationTok{\#\# 3 indgroup\_mean dist2max}
\DocumentationTok{\#\# 4          none   minmax}
\DocumentationTok{\#\# 5          none     rank}
\DocumentationTok{\#\# 6      ind\_mean     rank}
\end{Highlighting}
\end{Shaded}

Here, each column is a parameter that was varied in the analysis. Note that this does not include the weights used for each iteration since these are themselves data frames.

The \texttt{SAresults\$RankStats} table gives a summary of the main statistics of the ranks of each unit, including mean, median, and percentiles:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SAresults}\SpecialCharTok{$}\NormalTok{RankStats}
\DocumentationTok{\#\#    UnitCode Nominal  Mean Median    Q5   Q95}
\DocumentationTok{\#\# 1       AUT       7  7.09    7.0  5.00  9.00}
\DocumentationTok{\#\# 2       BEL       5  5.46    5.0  3.00  8.05}
\DocumentationTok{\#\# 3       BGR      30 29.55   30.0 29.00 31.00}
\DocumentationTok{\#\# 4       HRV      18 20.67   21.0 17.00 26.00}
\DocumentationTok{\#\# 5       CYP      29 29.85   30.0 28.00 32.00}
\DocumentationTok{\#\# 6       CZE      17 16.81   17.0 15.00 19.00}
\DocumentationTok{\#\# 7       DNK       3  2.61    2.0  2.00  4.00}
\DocumentationTok{\#\# 8       EST      22 18.96   18.0 13.00 25.05}
\DocumentationTok{\#\# 9       FIN      13 13.44   14.0 11.00 15.00}
\DocumentationTok{\#\# 10      FRA      21 20.77   21.0 18.00 24.00}
\DocumentationTok{\#\# 11      DEU       9  9.07    9.0  7.00 13.00}
\DocumentationTok{\#\# 12      GRC      32 31.99   32.0 30.00 34.00}
\DocumentationTok{\#\# 13      HUN      20 20.90   21.0 19.00 24.00}
\DocumentationTok{\#\# 14      IRL      12 12.10   12.0 10.00 16.00}
\DocumentationTok{\#\# 15      ITA      28 28.16   28.0 27.00 30.00}
\DocumentationTok{\#\# 16      LVA      23 21.96   22.0 19.00 24.00}
\DocumentationTok{\#\# 17      LTU      16 15.00   16.0 11.00 17.05}
\DocumentationTok{\#\# 18      LUX       8  8.58    9.0  5.00 11.05}
\DocumentationTok{\#\# 19      MLT      10 13.17   12.0  9.00 19.00}
\DocumentationTok{\#\# 20      NLD       2  3.00    3.0  2.00  4.00}
\DocumentationTok{\#\# 21      NOR       4  3.72    4.0  2.00  6.00}
\DocumentationTok{\#\# 22      POL      26 26.16   26.0 25.00 28.00}
\DocumentationTok{\#\# 23      PRT      27 23.57   27.0 15.00 27.00}
\DocumentationTok{\#\# 24      ROU      25 24.09   24.5 19.00 27.05}
\DocumentationTok{\#\# 25      SVK      24 23.45   23.0 21.00 26.00}
\DocumentationTok{\#\# 26      SVN      11  9.90   10.0  8.00 12.05}
\DocumentationTok{\#\# 27      ESP      19 22.57   23.0 19.00 26.00}
\DocumentationTok{\#\# 28      SWE       6  6.04    6.0  5.00  7.00}
\DocumentationTok{\#\# 29      CHE       1  1.00    1.0  1.00  1.00}
\DocumentationTok{\#\# 30      GBR      15 14.21   15.0 11.00 17.00}
\DocumentationTok{\#\# 31      AUS      35 35.52   35.0 34.00 38.05}
\DocumentationTok{\#\# 32      BGD      46 45.21   45.5 40.00 49.00}
\DocumentationTok{\#\# 33      BRN      40 38.99   39.0 34.00 45.05}
\DocumentationTok{\#\# 34      KHM      37 36.43   37.0 34.00 39.00}
\DocumentationTok{\#\# 35      CHN      49 49.51   50.0 47.00 51.00}
\DocumentationTok{\#\# 36      IND      45 45.49   46.0 42.00 48.00}
\DocumentationTok{\#\# 37      IDN      43 45.71   45.0 42.00 49.00}
\DocumentationTok{\#\# 38      JPN      34 33.57   34.0 31.00 35.00}
\DocumentationTok{\#\# 39      KAZ      47 45.49   45.0 41.95 49.00}
\DocumentationTok{\#\# 40      KOR      31 31.23   31.0 28.95 34.00}
\DocumentationTok{\#\# 41      LAO      48 42.40   41.0 36.00 49.00}
\DocumentationTok{\#\# 42      MYS      39 39.97   40.0 36.00 44.00}
\DocumentationTok{\#\# 43      MNG      44 45.29   46.0 40.95 49.00}
\DocumentationTok{\#\# 44      MMR      41 41.47   41.0 38.00 48.00}
\DocumentationTok{\#\# 45      NZL      33 32.83   33.0 31.00 34.05}
\DocumentationTok{\#\# 46      PAK      50 48.41   48.0 46.00 51.00}
\DocumentationTok{\#\# 47      PHL      38 41.02   41.0 38.00 45.00}
\DocumentationTok{\#\# 48      RUS      51 50.61   51.0 50.00 51.00}
\DocumentationTok{\#\# 49      SGP      14 13.93   14.0  8.95 20.05}
\DocumentationTok{\#\# 50      THA      42 41.78   42.0 40.00 44.00}
\DocumentationTok{\#\# 51      VNM      36 37.29   37.0 36.00 39.00}
\end{Highlighting}
\end{Shaded}

It also includes the nominal ranks for comparison. Finally, \texttt{SAresults\$Nominal} gives a summary of nominal ranks and scores.

If this information is not enough, the \texttt{store\_results} argument to \texttt{sensitivity()} gives options of what information to store for each iteration. For example, \texttt{store\_results\ =\ "results+method"} returns the information described so far, plus the full .\$Method list of each replication (i.e.~the full specifications used to build the composite indicator). Setting \texttt{store\_results\ =\ "results+COIN"} stores all results and the complete COIN of each replication. Of course, this latter option in particular will take up more memory.

\hypertarget{running-a-sensitivity-analysis}{%
\subsection{Running a sensitivity analysis}\label{running-a-sensitivity-analysis}}

Running a sensitivity analysis, i.e.~understanding the individual contributions of input uncertainties, is similar to an uncertainty analysis, but there are some extra considerations to keep in mind.

The first is that the target of the sensitivity analysis will be different. Whereas when you run an uncertainty analysis, you can calculate confidence intervals for each unit, with a sensitivity analysis a different approach is taken because otherwise, you would have a set of sensitivity indices for each single unit, and this would be hard to interpret. Instead, COINr calculates sensitivity with respect to the \emph{average absolute rank change}, between nominal and perturbed values.

The second consideration is that the number of replications required to a sensitivity analysis is quite a bit higher than the number required for an uncertainty analysis. In fact, when you specify \texttt{N} in the \texttt{sensitivity()} function, and set to \texttt{SAtype\ =\ "SA"}, the actual number of replications is \(N_T = N(d +2)\), where \(d\) is the number of uncertain input parameters/assumptions.

To run the sensitivity analysis, the format is very similar to the uncertainty analysis. We simply run \texttt{sensitivity()} with \texttt{SA\_specs} in exactly the same format as previously (we can use the same list), and set \texttt{SA\_type\ =\ "SA"}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Not actually run here. If you run this, it will take a few minutes}
\NormalTok{SAresults }\OtherTok{\textless{}{-}} \FunctionTok{sensitivity}\NormalTok{(ASEM, }\AttributeTok{v\_targ =} \StringTok{"Index"}\NormalTok{,}
                         \AttributeTok{SA\_specs =}\NormalTok{ SAspecs,}
                         \AttributeTok{N =} \DecValTok{500}\NormalTok{,}
                         \AttributeTok{SA\_type =} \StringTok{"SA"}\NormalTok{, }\AttributeTok{Nboot =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The output of the sensitivity analysis is a list. This is in fact an extended version of the list when \texttt{SA\_type\ =\ "UA"}, in that it has the recorded scores for each iteration, ranks for each iteration, as well as confidence intervals for ranks, and all the other outputs associated with \texttt{SA\_type\ =\ "SA"}.

Additionally it outputs a data frame \texttt{.\$Sensitivity}, which gives first and total order sensitivity indices for each input variable. Moreover, if \texttt{Nboot} is specified in \texttt{sensitivity()}, it provides estimated confidence intervals for each sensitivity index using bootstrapping.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{roundDF}\NormalTok{(SAresults}\SpecialCharTok{$}\NormalTok{Sensitivity)}
\DocumentationTok{\#\#   Variable   Si  STi Si\_q5 Si\_q95 STi\_q5 STi\_q95}
\DocumentationTok{\#\# 1   imtype 0.00 0.04 {-}0.04   0.05   0.03    0.04}
\DocumentationTok{\#\# 2    ntype 0.54 0.80  0.30   0.77   0.72    0.88}
\DocumentationTok{\#\# 3  weights 0.23 0.17  0.12   0.34   0.15    0.19}
\end{Highlighting}
\end{Shaded}

Recall that the target output in the sensitivity analysis is the mean absolute rank change (let's call it the MARC from here). How do we interpret these results? The first order sensitivity index \(S_i\) can be interpreted as the uncertainty caused by the effect of the \(i\)th uncertain parameter/assumption \emph{on its own}. The total order sensitivity index is the uncertainty caused by the effect of the \(i\)th uncertain parameter/assumption, \emph{including its interactions with other inputs}.

To help understand this, we will use COINr's plotting function for sensitivity analysis, \texttt{plotSA}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot bar chart}
\FunctionTok{plotSA}\NormalTok{(SAresults, }\AttributeTok{ptype =} \StringTok{"bar"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/plotSAbar-1.pdf}

This plot shows the three uncertainties that we introduced on the x-axis: the imputation type, the normalisation type, and the weights. The y-axis is the sensitivity index, and the total height of each bar is the total effect index \(S_{Ti}\), i.e.~the uncertainty caused by the variable on its own (the \emph{main effect}) as well as its interactions. Then each bar is divided into the interaction effects and the main effects.

What this shows is that the normalisation type is the most important uncertainty, followed by the weights, and last by the imputation type. In fact, the choice of imputation method (between the ones specified) is effectively insignificant. We can also see that the nomalisation has a significant interaction effect, probably with the imputation method. Whereas the weights don't seem to interact with the other inputs.

Another way of looking at this is in a pie chart:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot bar chart}
\FunctionTok{plotSA}\NormalTok{(SAresults, }\AttributeTok{ptype =} \StringTok{"pie"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/plotSApie-1.pdf}

Here we can see that more than half of the uncertainty is caused by the normalisation method choice alone, while a bit less than a quarter is caused by the weights, and the remainder by interactions.

It is likely that the normalisation method is important because one of the choices was rank normalisation, which radically alters the distribution of each indicator.

Finally, to see the uncertainty on the estimates of each sensitivity index we can use a box plot (or error bars):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot bar chart}
\FunctionTok{plotSA}\NormalTok{(SAresults, }\AttributeTok{ptype =} \StringTok{"box"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/plotSAbox-1.pdf}

This will not work unless \texttt{Nboot} was specified. Bootstrapping allows confidence intervals to be estimated, and this shows that the estimates of total effect indices (\(S_{Ti}\)) are quite reliable. Whereas the confidence intervals are much wider on the first order indices (\(S_{i}\)). Still, with the number of runs applied, it is possible to draw robust conclusions. If the confidence intervals were still too wide here, you could consider increasing \texttt{N} - this should lead to narrower intervals, although you might have to wait for a bit.

\hypertarget{helper-functions}{%
\chapter{Helper functions}\label{helper-functions}}

\hypertarget{r-interfaces}{%
\section{R Interfaces}\label{r-interfaces}}

\hypertarget{data-import}{%
\subsection{Data import}\label{data-import}}

COINr has a couple of useful functions to help import and export data and metadata. You might have heard of the \href{https://knowledge4policy.ec.europa.eu/composite-indicators/coin-tool_en}{COIN Tool} which is an Excel-based tool for building and analysing composite indicators, similar in fact to COINr\footnote{Full disclosure, I was also involved in the development of the COIN Tool}. With the \texttt{coinToolIn()} function you can import data directly from the COIN Tool to cross check or extend your analysis in COINr.

To demonstrate, we can take the example version of the COIN Tool, which you can download \href{https://composite-indicators.jrc.ec.europa.eu/sites/default/files/COIN_Tool_v1_LITE_exampledata.xlsm}{here}. Then it's as simple as running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}

\CommentTok{\# This is the file path and name where the COIN Tool is downloaded to}
\CommentTok{\# You could also just put it in your project directory.}
\NormalTok{fname }\OtherTok{\textless{}{-}} \StringTok{"C:/Users/becke/Downloads/COIN\_Tool\_v1\_LITE\_exampledata.xlsm"}

\NormalTok{dflist }\OtherTok{\textless{}{-}} \FunctionTok{COINToolIn}\NormalTok{(fname)}
\DocumentationTok{\#\# Imported 15 indicators and 28 units.}
\end{Highlighting}
\end{Shaded}

The output of this function is a list with the three data frame inputs to \texttt{assemble()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dflist}
\DocumentationTok{\#\# $IndData}
\DocumentationTok{\#\# \# A tibble: 28 x 17}
\DocumentationTok{\#\#    UnitName UnitCode ind.01 ind.02 ind.03 ind.04 ind.05 ind.06 ind.07 ind.08}
\DocumentationTok{\#\#    \textless{}chr\textgreater{}    \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1 Austria  AT         13.3   80.4   492.   14.9   68.8     34    3.7   87.6}
\DocumentationTok{\#\#  2 Belgium  BE         15.1   71.8   503.    7     59.6     24    5.2   81.2}
\DocumentationTok{\#\#  3 Bulgaria BG         12.3   78.1   440.    2.2   51.3     15   10.6   72  }
\DocumentationTok{\#\#  4 Cyprus   CY         14     76     438.    6.9   16.7     23    3.6   73.4}
\DocumentationTok{\#\#  5 Czech R\textasciitilde{} CZ         13.5   87.6   491.    8.8   73.2     27    3.9   86.7}
\DocumentationTok{\#\#  6 Germany  DE          9.7   80.2   508.    8.5   46.3     30    5.6   90.1}
\DocumentationTok{\#\#  7 Denmark  DK          9.7   73     504.   27.7   40.6     39    3.8   83.9}
\DocumentationTok{\#\#  8 Estonia  EE          8.6   83.3   524.   15.7   35.7     37    4.1   77.1}
\DocumentationTok{\#\#  9 Greece   EL         11.8   70     458.    4     31.5     30    3.9   49.2}
\DocumentationTok{\#\# 10 Spain    ES         14.9   57.4   491.    9.4   34.8     33   11.4   68  }
\DocumentationTok{\#\# \# ... with 18 more rows, and 7 more variables: ind.09 \textless{}dbl\textgreater{}, ind.10 \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   ind.11 \textless{}dbl\textgreater{}, ind.12 \textless{}dbl\textgreater{}, ind.13 \textless{}dbl\textgreater{}, ind.14 \textless{}dbl\textgreater{}, ind.15 \textless{}dbl\textgreater{}}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $IndMeta}
\DocumentationTok{\#\# \# A tibble: 15 x 9}
\DocumentationTok{\#\#    IndCode IndName         GPupper GPlower Direction IndWeight Agg1  Agg2  Agg3 }
\DocumentationTok{\#\#    \textless{}chr\textgreater{}   \textless{}chr\textgreater{}             \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\DocumentationTok{\#\#  1 ind.01  Pre{-}primary pu\textasciitilde{}      NA      NA        {-}1      0.4  sp.01 p.01  Index}
\DocumentationTok{\#\#  2 ind.02  Share of popul\textasciitilde{}      NA      NA         1      0.3  sp.01 p.01  Index}
\DocumentationTok{\#\#  3 ind.03  Reading, maths\textasciitilde{}      NA      NA         1      0.3  sp.01 p.01  Index}
\DocumentationTok{\#\#  4 ind.04  Recent training      NA      NA         1      0.3  sp.02 p.01  Index}
\DocumentationTok{\#\#  5 ind.05  VET students         NA      NA         1      0.35 sp.02 p.01  Index}
\DocumentationTok{\#\#  6 ind.06  High computer \textasciitilde{}      NA      NA         1      0.35 sp.02 p.01  Index}
\DocumentationTok{\#\#  7 ind.07  Early leavers \textasciitilde{}      NA      NA        {-}1      0.7  sp.03 p.02  Index}
\DocumentationTok{\#\#  8 ind.08  Recent graduat\textasciitilde{}      NA      NA         1      0.3  sp.03 p.02  Index}
\DocumentationTok{\#\#  9 ind.09  Activity rate \textasciitilde{}      NA      NA         1      0.5  sp.04 p.02  Index}
\DocumentationTok{\#\# 10 ind.10  Activity rate \textasciitilde{}      NA      NA         1      0.5  sp.04 p.02  Index}
\DocumentationTok{\#\# 11 ind.11  Long{-}term unem\textasciitilde{}      NA      NA        {-}1      0.4  sp.05 p.03  Index}
\DocumentationTok{\#\# 12 ind.12  Underemployed \textasciitilde{}      NA      NA        {-}1      0.6  sp.05 p.03  Index}
\DocumentationTok{\#\# 13 ind.13  Higher educati\textasciitilde{}      NA      NA        {-}1      0.4  sp.06 p.03  Index}
\DocumentationTok{\#\# 14 ind.14  ISCED 5{-}8 prop\textasciitilde{}      NA      NA        {-}1      0.1  sp.06 p.03  Index}
\DocumentationTok{\#\# 15 ind.15  Qualification \textasciitilde{}      NA      NA        {-}1      0.5  sp.06 p.03  Index}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $AggMeta}
\DocumentationTok{\#\# \# A tibble: 10 x 4}
\DocumentationTok{\#\#    AgLevel Code  Name                            Weight}
\DocumentationTok{\#\#      \textless{}dbl\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}                            \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1       4 Index European Skills Index              1  }
\DocumentationTok{\#\#  2       3 p.01  Skills Development                 0.3}
\DocumentationTok{\#\#  3       3 p.02  Skills Activation                  0.3}
\DocumentationTok{\#\#  4       3 p.03  Skills Matching                    0.4}
\DocumentationTok{\#\#  5       2 sp.01 Compulsory education               0.5}
\DocumentationTok{\#\#  6       2 sp.02 Training and tertiary education    0.5}
\DocumentationTok{\#\#  7       2 sp.03 Transition to work                 0.5}
\DocumentationTok{\#\#  8       2 sp.04 Activity rates                     0.5}
\DocumentationTok{\#\#  9       2 sp.05 Unemployment                       0.4}
\DocumentationTok{\#\# 10       2 sp.06 Skills mismatch                    0.6}
\end{Highlighting}
\end{Shaded}

Because the COIN Tool uses numeric codes for indicators such as \texttt{ind.01}, you might want slightly more informative codes. The best way to do this is to name the codes yourself, but a quick solution is to set \texttt{makecodes\ =\ TRUE} in \texttt{COINToolIn()}. This generates short codes based on the indicator names. It will not yield perfect results, but for a quick analysis it might be sufficient. At least, you could use this and then modify the results by hand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dflist }\OtherTok{\textless{}{-}} \FunctionTok{COINToolIn}\NormalTok{(fname, }\AttributeTok{makecodes =} \ConstantTok{TRUE}\NormalTok{)}
\DocumentationTok{\#\# Imported 15 indicators and 28 units.}
\NormalTok{dflist}\SpecialCharTok{$}\NormalTok{IndMeta}\SpecialCharTok{$}\NormalTok{IndCode}
\DocumentationTok{\#\#  [1] "Pre{-}Pupi"   "SharPopu"   "ReadMath"   "ReceTrai"   "Stud"      }
\DocumentationTok{\#\#  [6] "HighComp"   "EarlLeav"   "ReceGrad"   "ActiRate"   "ActiRate\_1"}
\DocumentationTok{\#\# [11] "LongUnem"   "UndePart"   "HighEduc"   "Isce5{-}Pr"   "QualMism"}
\end{Highlighting}
\end{Shaded}

While the codes could certainly be improved, it's a lot better than uninformative numbers. Finally, we can assemble the output into a COIN and begin the construction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ESI }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(dflist[[}\DecValTok{1}\NormalTok{]],dflist[[}\DecValTok{2}\NormalTok{]],dflist[[}\DecValTok{3}\NormalTok{]],)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# No denominators detected.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 15}
\DocumentationTok{\#\# Number of units = 28}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 6 aggregate groups: sp.01, sp.02, sp.03, sp.04, sp.05, sp.06}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 3 aggregate groups: p.01, p.02, p.03}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\hypertarget{export-to-excel}{%
\subsection{Export to Excel}\label{export-to-excel}}

Trigger warning for R purists! Sometimes it's easier to look at your data in Excel. There, I said it. R is great for doing all kinds of complicated tasks, but if you just want to look at big tables of numbers and play around with them, maybe make a few quick graphs, then Excel is a great tool.

Actually Excel is kind of underrated by many people who are used to programming in R or Python, Matlab or even Stata. It has a lot of clever tools that not many people know about. But more importantly, Excel is a \emph{lingua franca} between all kinds of professions - you can pass an Excel spreadsheet to almost anyone and they will be able to take a look at it and use the data. Try doing that with an R or Python script.

It just boils down to using the right tool for the right job. Anyway, with that aside, let's look at COINr's \texttt{coin2Excel()} function. You put in your COIN, and it will write a spreadsheet.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Build ASEM index}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}
\CommentTok{\# Get some statistics}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getStats}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{)}
\CommentTok{\# write to Excel}
\FunctionTok{coin2Excel}\NormalTok{(ASEM, }\AttributeTok{fname =} \StringTok{"ASEMresults.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The spreadsheet will contain a number of tabs, including:

\begin{itemize}
\tightlist
\item
  The indicator data, metadata and aggregation metadata that was input to COINr
\item
  All data sets in the \texttt{.\$Data} folder, e.g.~raw, treated, normalised, aggregated, etc.
\item
  (almost) All data frames found in the \texttt{.\$Analysis} folder, i.e.~statistics tables, outlier flags, correlation tables.
\end{itemize}

\hypertarget{selecting-data-sets-and-indicators}{%
\section{Selecting data sets and indicators}\label{selecting-data-sets-and-indicators}}

The \texttt{getIn()} function is widely used by many \texttt{COINr} functions. It is used for selecting specific data sets, and returning subsets of indicators. While some of this can be achieved fairly easily with base R, or \texttt{dplyr::select()}, subsetting in a hierarchical context can be more awkward. That's where \texttt{getIn()} steps in to help.

Although it was made to be used internally, it might also help in other contexts. Note that this can work on COINs or data frames, but is most useful with COINs.

Let's take some examples. First, we can get a whole data set. \texttt{getIn()} will retrieve any of the data sets in the \texttt{.\$Data} folder, as well as the denominators.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Build data set first, if not already done}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Missing data points detected = 65}
\DocumentationTok{\#\# Missing data points imputed = 65, using method = indgroup\_mean}

\CommentTok{\# Get raw data set}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{)}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 49}
\DocumentationTok{\#\#     Goods Services    FDI PRemit ForPort CostImpEx Tariff  TBTs TIRcon  RTAs}
\DocumentationTok{\#\#     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1 278.      108.   5      5.07    809.          0    1.6  1144      1    30}
\DocumentationTok{\#\#  2 598.      216.   5.71  13.4    1574.          0    1.6  1348      1    30}
\DocumentationTok{\#\#  3  42.8      13.0  1.35   1.04     15.5        52    1.6  1140      1    30}
\DocumentationTok{\#\#  4  28.4      17.4  0.387  1.56     16.9         0    1.6  1179      1    30}
\DocumentationTok{\#\#  5   8.77     15.2  1.23   0.477    40.8       100    1.6  1141      1    30}
\DocumentationTok{\#\#  6 274.       43.5  3.88   4.69    108.          0    1.6  1456      1    30}
\DocumentationTok{\#\#  7 147.      114.   9.1    2.19   1021.          0    1.6  1393      1    30}
\DocumentationTok{\#\#  8  28.2      10.2  0.580  0.589    17.4         0    1.6  1153      1    30}
\DocumentationTok{\#\#  9 102.       53.8  6.03   1.51    748.         70    1.6  1215      1    30}
\DocumentationTok{\#\# 10 849.      471.  30.9   30.2    6745.          0    1.6  1385      1    30}
\DocumentationTok{\#\# \# ... with 41 more rows, and 39 more variables: Visa \textless{}dbl\textgreater{}, StMob \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Research \textless{}dbl\textgreater{}, Pat \textless{}dbl\textgreater{}, CultServ \textless{}dbl\textgreater{}, CultGood \textless{}dbl\textgreater{}, Tourist \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   MigStock \textless{}dbl\textgreater{}, Lang \textless{}dbl\textgreater{}, LPI \textless{}dbl\textgreater{}, Flights \textless{}dbl\textgreater{}, Ship \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Bord \textless{}dbl\textgreater{}, Elec \textless{}dbl\textgreater{}, Gas \textless{}dbl\textgreater{}, ConSpeed \textless{}dbl\textgreater{}, Cov4G \textless{}dbl\textgreater{}, Embs \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   IGOs \textless{}dbl\textgreater{}, UNVote \textless{}dbl\textgreater{}, Renew \textless{}dbl\textgreater{}, PrimEner \textless{}dbl\textgreater{}, CO2 \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   MatCon \textless{}dbl\textgreater{}, Forest \textless{}dbl\textgreater{}, Poverty \textless{}dbl\textgreater{}, Palma \textless{}dbl\textgreater{}, TertGrad \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   FreePress \textless{}dbl\textgreater{}, TolMin \textless{}dbl\textgreater{}, NGOs \textless{}dbl\textgreater{}, CPI \textless{}dbl\textgreater{}, FemLab \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   WomParl \textless{}dbl\textgreater{}, PubDebt \textless{}dbl\textgreater{}, PrivDebt \textless{}dbl\textgreater{}, GDPGrow \textless{}dbl\textgreater{}, RDExp \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   NEET \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

The output, here \texttt{datalist} is a list containing the full data set \texttt{.\$ind\_data}, the data set \texttt{.\$ind\_data\_only} only including numerical (indicator) columns, as well as unit codes, indicator codes and names, and the object type.

More usefully, we can get specific indicators:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get raw data set}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{icodes =} \FunctionTok{c}\NormalTok{(}\StringTok{"Flights"}\NormalTok{, }\StringTok{"LPI"}\NormalTok{))}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 2}
\DocumentationTok{\#\#    Flights   LPI}
\DocumentationTok{\#\#      \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1   29.0   4.10}
\DocumentationTok{\#\#  2   31.9   4.11}
\DocumentationTok{\#\#  3    9.24  2.81}
\DocumentationTok{\#\#  4    9.25  3.16}
\DocumentationTok{\#\#  5    8.75  3.00}
\DocumentationTok{\#\#  6   15.3   3.67}
\DocumentationTok{\#\#  7   32.8   3.82}
\DocumentationTok{\#\#  8    3.13  3.36}
\DocumentationTok{\#\#  9   18.9   3.92}
\DocumentationTok{\#\# 10   97.6   3.90}
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

More usefully still, we can get groups of indicators based on their groupings. For example, we can ask for indicators that belong to the ``Political'' group:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get raw data set}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Political"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{1}\NormalTok{)}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 3}
\DocumentationTok{\#\#     Embs  IGOs UNVote}
\DocumentationTok{\#\#    \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1    88   227   42.6}
\DocumentationTok{\#\#  2    84   248   43.0}
\DocumentationTok{\#\#  3    67   209   43.0}
\DocumentationTok{\#\#  4    62   197   42.7}
\DocumentationTok{\#\#  5    43   172   42.3}
\DocumentationTok{\#\#  6    84   201   42.2}
\DocumentationTok{\#\#  7    77   259   42.8}
\DocumentationTok{\#\#  8    46   194   42.9}
\DocumentationTok{\#\#  9    74   269   42.7}
\DocumentationTok{\#\# 10   100   329   40.4}
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

To do this, you have to specify the \texttt{aglev} argument, which specifies which level to retrieve the indicators from. Before the data set is aggregated, you can anyway only select the indicators, but for the aggregated data set, the situation is more complex. To illustrate, we can call the Connectivity sub-index, first asking for all indicators:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get raw data set}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Conn"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{1}\NormalTok{)}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 30}
\DocumentationTok{\#\#    Goods Services   FDI PRemit ForPort CostImpEx Tariff  TBTs TIRcon  RTAs  Visa}
\DocumentationTok{\#\#    \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1  40.7     33.0  6.24  16.8    34.2      100     79.2  34.8    100  64.4  86.8}
\DocumentationTok{\#\#  2  80.1     58.7  5.79  40.9    55.7      100     79.2  23.2    100  64.4  86.8}
\DocumentationTok{\#\#  3  47.1     28.4 15.8   26.9     4.82      91.3   79.2  35.0    100  64.4  85.7}
\DocumentationTok{\#\#  4  29.7     41.6  2.26  43.9     5.45     100     79.2  32.8    100  64.4  84.6}
\DocumentationTok{\#\#  5  21.6    100   43.1   33.6    33.6       83.2   79.2  35.0    100  64.4  85.7}
\DocumentationTok{\#\#  6  88.9     25.5 11.6   33.9     9.12     100     79.2  17.0    100  64.4  85.7}
\DocumentationTok{\#\#  7  24.4     46.0 19.0    7.73   55.1      100     79.2  20.6    100  64.4  87.9}
\DocumentationTok{\#\#  8  75.4     55.4 15.4   35.8    12.3      100     79.2  34.3    100  64.4  85.7}
\DocumentationTok{\#\#  9  20.8     25.9 15.7    6.50   51.9       88.2   79.2  30.7    100  64.4  87.9}
\DocumentationTok{\#\# 10  15.1     21.1  6.04  15.7    45.3      100     79.2  21.0    100  64.4  87.9}
\DocumentationTok{\#\# \# ... with 41 more rows, and 19 more variables: StMob \textless{}dbl\textgreater{}, Research \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Pat \textless{}dbl\textgreater{}, CultServ \textless{}dbl\textgreater{}, CultGood \textless{}dbl\textgreater{}, Tourist \textless{}dbl\textgreater{}, MigStock \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Lang \textless{}dbl\textgreater{}, LPI \textless{}dbl\textgreater{}, Flights \textless{}dbl\textgreater{}, Ship \textless{}dbl\textgreater{}, Bord \textless{}dbl\textgreater{}, Elec \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Gas \textless{}dbl\textgreater{}, ConSpeed \textless{}dbl\textgreater{}, Cov4G \textless{}dbl\textgreater{}, Embs \textless{}dbl\textgreater{}, IGOs \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   UNVote \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

Or we can call all the pillars belonging to ``Connectivity'', i.e.~the level below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get raw data set}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Conn"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{2}\NormalTok{)}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 5}
\DocumentationTok{\#\#    ConEcFin Instit   P2P Physical Political}
\DocumentationTok{\#\#       \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1     26.2   77.5  54.1     41.1      78.2}
\DocumentationTok{\#\#  2     48.2   75.6  43.3     72.0      80.8}
\DocumentationTok{\#\#  3     24.6   75.9  27.1     28.4      67.5}
\DocumentationTok{\#\#  4     24.6   76.8  39.1     47.6      62.6}
\DocumentationTok{\#\#  5     46.4   74.6  59.8     29.5      48.7}
\DocumentationTok{\#\#  6     33.8   74.4  39.6     41.5      71.0}
\DocumentationTok{\#\#  7     30.4   75.4  51.0     50.5      78.1}
\DocumentationTok{\#\#  8     38.9   77.3  53.5     42.5      55.4}
\DocumentationTok{\#\#  9     24.1   75.1  39.1     44.5      77.9}
\DocumentationTok{\#\# 10     20.6   75.4  28.4     42.4      87.6}
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

Finally, if we want ``Conn'' itself, we can just call it directly with no \texttt{aglev} specified.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get raw data set}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Conn"}\NormalTok{)}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 1}
\DocumentationTok{\#\#     Conn}
\DocumentationTok{\#\#    \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1  55.4}
\DocumentationTok{\#\#  2  64.0}
\DocumentationTok{\#\#  3  44.7}
\DocumentationTok{\#\#  4  50.1}
\DocumentationTok{\#\#  5  51.8}
\DocumentationTok{\#\#  6  52.1}
\DocumentationTok{\#\#  7  57.1}
\DocumentationTok{\#\#  8  53.5}
\DocumentationTok{\#\#  9  52.1}
\DocumentationTok{\#\# 10  50.9}
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

We can also use \texttt{getIn()} with data frames, and it will behave in more or less the same way, except a data frame has no information about the structure of the index. Here, \texttt{getIn()} returns what it can, and arguments like \texttt{dset} and \texttt{aglev} are ignored.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use the ASEM indicator data frame directly}
\NormalTok{datalist }\OtherTok{\textless{}{-}} \FunctionTok{getIn}\NormalTok{(ASEMIndData, }\AttributeTok{icodes =} \FunctionTok{c}\NormalTok{(}\StringTok{"LPI"}\NormalTok{, }\StringTok{"Goods"}\NormalTok{))}
\NormalTok{datalist}\SpecialCharTok{$}\NormalTok{ind\_data\_only}
\DocumentationTok{\#\# \# A tibble: 51 x 2}
\DocumentationTok{\#\#      LPI  Goods}
\DocumentationTok{\#\#    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1  4.10 278.  }
\DocumentationTok{\#\#  2  4.11 598.  }
\DocumentationTok{\#\#  3  2.81  42.8 }
\DocumentationTok{\#\#  4  3.16  28.4 }
\DocumentationTok{\#\#  5  3.00   8.77}
\DocumentationTok{\#\#  6  3.67 274.  }
\DocumentationTok{\#\#  7  3.82 147.  }
\DocumentationTok{\#\#  8  3.36  28.2 }
\DocumentationTok{\#\#  9  3.92 102.  }
\DocumentationTok{\#\# 10  3.90 849.  }
\DocumentationTok{\#\# \# ... with 41 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{rounding-data-frames}{%
\section{Rounding data frames}\label{rounding-data-frames}}

The \texttt{roundDF} function is a small helper function for rounding data frames that contain a mix of numeric and non-numeric columns. This is very handy for presenting tables generated by COINr in documents.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use the ASEM indicator data frame directly}
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{ASEMIndData }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{roundDF}\NormalTok{(}\AttributeTok{decimals =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  reactable}\SpecialCharTok{::}\FunctionTok{reactable}\NormalTok{(}\AttributeTok{defaultPageSize =} \DecValTok{5}\NormalTok{, }\AttributeTok{highlight =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{wrap =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-71-1.pdf}

By default, numbers are rounded to two decimal places.

\hypertarget{appendix-r-resources}{%
\chapter{Appendix: R Resources}\label{appendix-r-resources}}

One of the great things about R is the sheer number of freely-available resources that are out there. Not just the software and packages, but also online books and materials to learn everything you need to know about pretty much anything.

Here are a list of resources for users who (a) are interested in R and want to get started, and (b) are proficient R users but want to learn more. I am focusing here on resources that have really helped me in the work that I have done. Of course there is far more out there and you only have to look.

\hypertarget{introduction-to-r}{%
\section{Introduction to R}\label{introduction-to-r}}

If you're just starting out, these are good places to start.

\textbf{R for Data Science} is a modern classic that starts from the beginning and leads you into the world of R, from a data science perspective. It uses the ``tidyverse'' approach which is developed by R Guru Hadley Wickham. Even advanced users can probably learn something here.

\url{https://r4ds.had.co.nz/index.html}

\textbf{Swirl} is an R package which lets you learn R at the command line. Also very good for beginners.

\url{https://swirlstats.com/}

\hypertarget{advanced-resources}{%
\section{Advanced resources}\label{advanced-resources}}

\hypertarget{programming}{%
\subsection{Programming}\label{programming}}

If you really want to sharpen your programming skills in R, Hadley Wickham has another book: this one digs around in the roots of R and teaches you all kinds of tricks and quirks.

\url{https://adv-r.hadley.nz/}

Want to build your own R package? Hadley come to the rescue, again.

\url{https://r-pkgs.org/}

\hypertarget{visualisation}{%
\subsection{Visualisation}\label{visualisation}}

Plotly is a big R package which generates interactive graphics using Javascript (there are many others, by the way). This book tells you all you need to know about that.

\url{https://plotly-r.com/}

Shiny is another R package which lets you build interactive web apps based on R code. It's tricky to get your head around at first, but this book really helps.

\url{https://mastering-shiny.org/index.html}

\hypertarget{other}{%
\subsection{Other}\label{other}}

If you're not using GitHub, ask yourself, why not? GitHub is the best way to collaborate and share code, and you can also host documentation and websites. This book is hosted on GitHub, to take a random example. The Happy Git with R Book gives an easy introduction to hooking up R Studio to work seamlessly with Git and Github.

\url{https://happygitwithr.com/}

You wouldn't think that R would be a good tool for writing books, but actually it turns out that\ldots{} it's a pretty good tool for writing books. The Bookdown package lets you build nifty online books with a simple and neat layout. You can include equations and importantly, R code, outputs and HTML widgets, etc.

\url{https://bookdown.org/yihui/bookdown/}

Finally, you wouldn't think that R would be a good tool for building website, but actually it turns out\ldots{} it's a pretty good tool for building websites. OK, yes, if you want to build something really complicated and/or highly customised, then it's not the way forward. But for building fairly simple sites and blogs, personal pages etc (especially if you want to stick in some R code), then the \emph{Blogdown} package gives a great way to do this. And guess what, you can link it to Github and it automatically updates your website when you push any changes from R Studio. And it's all free. An example of this is my very humble website which you can find at \url{http://www.bluefoxdata.eu}.

What's that you say, if only there were a book to teach me how to do all this? Well you're in luck - here it is.

\url{https://bookdown.org/yihui/blogdown/}

\hypertarget{faq}{%
\chapter{FAQ}\label{faq}}

\begin{quote}
What is COINr?
\end{quote}

COINr is an R package for building and analysing composite indicators. It aims to make many (sometimes complex) operations in building and auditing composite indicators fast and easy to reproduce.

\begin{quote}
What is a composite indicator?
\end{quote}

A composite indicator is an aggregation of indicators which aims to measure a particular concept. Composite indicators are typically used to measure complex and multidimensional concepts which are difficult to define, and cannot be measured directly. Examples include innovation, human development, environmental performance, and so on. Composite indicators are closely related to scoreboards, which are also groups of indicators aiming to capture a concept. However, scoreboards do not aggregate indicator values. Composite indicators also usually use a hierarchical structure which breaks the concept down into elements, sometimes known as sub-pillars, pillars, sub-indexes, dimensions, and so on.

If you want to know more about composite indicators, see the European Commission's \href{https://knowledge4policy.ec.europa.eu/composite-indicators/about_en}{Competence Centre on Composite Indicators and Scoreboards} and in particular the Handbook which can be found \href{https://knowledge4policy.ec.europa.eu/composite-indicators/toolkit_en}{here} with a few other useful things.

\begin{quote}
Do I need to be ``good at R'' to use COINr?
\end{quote}

COINr is designed to have a simple interface which mostly involves passing COINs from one function to another. This means that you can build a composite indicator in a few simple commands. You can also export easily to Excel and prepare graphs and tables there if you prefer. Of course, composite indicator construction often involves iterative steps and it is likely that some things will have to be done outside of COINr. So the answer is no, but it helps to have a basic level of competency in R. See \protect\hyperlink{appendix-r-resources}{Appendix: R Resources} for some suggested ways to get more into R.

\begin{quote}
Thanks but is there an equivalent tool in Excel?
\end{quote}

Yes - the \href{https://knowledge4policy.ec.europa.eu/composite-indicators/coin-tool_en}{COIN Tool} is an Excel tool which does similar things to COINr, although the options are more limited. It is however a very useful tool and if you prefer to do everything in Excel this should do the job.

\begin{quote}
What are the steps for building a composite indicator in COINr?
\end{quote}

See \protect\hyperlink{appendix-building-a-composite-indicator-example}{Appendix: Building a Composite Indicator Example}.

\begin{quote}
What are the steps for analysing a composite indicator in COINr?
\end{quote}

See \protect\hyperlink{appendix-analysing-a-composite-indicator-example}{Appendix: Analysing a Composite Indicator Example}.

\begin{quote}
Can I construct a composite indicator over multiple years?
\end{quote}

At the moment, COINr does not have a dedicated support for panel data, i.e.~data that has different values for different years. This can also be complicated because from one year to another, indicators may change, there may be a different set of units, methodology may change, and so on. A suggested workaround, for the moment, is to build a separate COIN for each year of data. This allows methodological variations if needed, and the results can be compared using COINr's \texttt{compTable()} and \texttt{compTableMulti()} functions. You would probably also need to extract results using your own code as well.

Eventually, time permitting, I would build this into a package. This would probably be done in the same way suggested here, i.e.~having multiple COINs for each year, but would be slightly automated. The COINs would also be stored in a single list or possibly a data frame, following something along the lines of the \href{https://r4ds.had.co.nz/many-models.html}{Many Models} chapter in the R For Data Science book.

\begin{quote}
Do I \emph{have} to aggregate indicators? Can I just make a scoreboard or only aggregate up to a certain level?
\end{quote}

No of course, you don't have to aggregate at all, and in some cases it might not even be a good idea. However, you can always see what happens when you aggregate. You can also aggregate up to an index, but choose to only present pillar or dimension scores. This might make sense, for instance, when pillars represent very different things and are poorly correlated or divergent between one another. Another thing to keep in mind is that an aggregated score and a scoreboard are not mutually exclusive - you can easily present the two together.

\begin{quote}
I found a bug - what do?
\end{quote}

You can \href{https://github.com/bluefoxr/COINr/issues}{open an issue} on the COINr repo. Otherwise, you can also just \href{mailto:william.becker@bluefoxdata.eu}{email me}.

\begin{quote}
I need help. Who can I ask?
\end{quote}

Please search this manual first and also the help documentation by typing e.g.~\texttt{?normalise} (depending on the function you are interested in). If you need general assistance with composite indicators and indicators, \href{mailto:william.becker@bluefoxdata.eu}{email me} and I may be able to help.

\begin{quote}
Composite indicators are full of subjective choices, so doesn't that make them pseudo-science?
\end{quote}

Composite indicators are indeed full of subjective choices, and this introduces uncertainties in their results (scores and rankings). For example, weights, aggregation methods, which indicators to include, are some of the many choices to make along the way. However, if you have worked in any modelling exercise, you will know that any model has similar problems -- which processes to include, which modelling approach to adopt, what values to assign to parameters, and so on.

This means that, like any modelling exercise, composite indicators should be interpreted carefully. Which is to say that you should be aware that small differences in scores and ranks may not be significant. Uncertainty analysis can help with this.

It's also worth pointing out that composite indicators usually aim to measure things that cannot really be measured any other way, like innovation or quality of life. And also that even if we build an index, it should always be presented alongside its underlying data.

If you want to read in more detail on this, you could see my \href{https://www.bluefoxdata.eu/blog/2020-11-17-whycompositeindicators/}{blog post} on the topic.

\hypertarget{appendix-building-a-composite-indicator-example}{%
\chapter{Appendix: Building a Composite Indicator Example}\label{appendix-building-a-composite-indicator-example}}

Here we go through the main steps to building a composite indicator in COINr. This is effectively a condensed version of some earlier chapters in this book.

\hypertarget{before-coinr-bc}{%
\section{Before COINr (BC)}\label{before-coinr-bc}}

Before even getting to COINr you will have to take a number of steps. The steps can be summarised by e.g.~the European Commission's \href{https://knowledge4policy.ec.europa.eu/publication/your-10-step-pocket-guide-composite-indicators-scoreboards_en}{Ten Step Guide}. In short, before getting to COINr you should have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tried to summarise all elements of the concept you are trying to capture, including by e.g.

  \begin{itemize}
  \tightlist
  \item
    A literature review
  \item
    Talking to experts and stakeholders if possible
  \item
    Reviewing any other indicator frameworks on the topic
  \end{itemize}
\item
  Constructed a ``conceptual framework'' using the information in 1. For a composite indicator, this should be a hierarchical structure.
\item
  Gathered indicator data (these need not be the final set of indicators)
\end{enumerate}

Depending on how thorough you are, and how accessible your data is, this process can take a surprisingly long time. On these topics, there is a lot of good training material available through the European Commission's Competence Centre for Composite Indicators and Scoreboards \href{https://knowledge4policy.ec.europa.eu/composite-indicators/2019-jrc-week-composite-indicators-scoreboards_en}{here}.

\hypertarget{load-data-and-assemble}{%
\section{Load data and assemble}\label{load-data-and-assemble}}

Having got a set of preliminary indicator data, the next step is to load data into R in some way. Remember that to build a composite indicator with COINr you need to first build a COIN. To build a COIN you need three data frames, as explained in detail in \protect\hyperlink{coins-the-currency-of-coinr}{COINs: the currency of COINr}:

\begin{itemize}
\tightlist
\item
  Indicator data
\item
  Indicator metadata
\item
  Aggregation metadata
\end{itemize}

Where these data frames come from is up to you. You might want to load data into R and then assemble them using your own script. Alternatively, you may wish to assemble these in Excel, for example, then read the sheets of the spreadsheet into R.

Let's assume now that you have got these three data frames in the correct formats. Now you simply put them into \texttt{assemble()} to build your COIN. Here I will use the built-in data frames in COINr:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}

\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ ASEMIndData,}
                 \AttributeTok{IndMeta =}\NormalTok{ ASEMIndMeta,}
                 \AttributeTok{AggMeta =}\NormalTok{ ASEMAggMeta)}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Denominators detected {-} stored in .$Input$Denominators}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Indicator codes cross{-}checked and OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Number of indicators = 49}
\DocumentationTok{\#\# Number of units = 51}
\DocumentationTok{\#\# Number of aggregation levels = 3 above indicator level.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\# Aggregation level 1 with 8 aggregate groups: Physical, ConEcFin, Political, Instit, P2P, Environ, Social, SusEcFin}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 2 with 2 aggregate groups: Conn, Sust}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# Aggregation level 3 with 1 aggregate groups: Index}
\DocumentationTok{\#\# Cross{-}check between metadata and framework = OK.}
\DocumentationTok{\#\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\hypertarget{check-the-data}{%
\section{Check the data}\label{check-the-data}}

At this point, it's worth checking the COIN to make sure that everything is as you would expect. One thing is to check the framework:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotframework}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-73-1.pdf}

You can also explore the indicator data using COINr's \texttt{indDash()} app:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# not run here because requires an active R session}
\FunctionTok{indDash}\NormalTok{(ASEM)}
\end{Highlighting}
\end{Shaded}

I would also recommend to check the statistics of each indicator using \texttt{getStats()}. Look for any indicators that are highly skewed, have few unique values, have low data availability, or strong correlations with other indicators or with denominators.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load reactable for viewing table (in R you can do this instead with R Studio)}
\FunctionTok{library}\NormalTok{(reactable)}

\CommentTok{\# get stats}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getStats}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{)}
\DocumentationTok{\#\# Number of collinear indicators =  3}
\DocumentationTok{\#\# Number of signficant negative indicator correlations =  322}
\DocumentationTok{\#\# Number of indicators with high denominator correlations =  7}

\CommentTok{\# view stats table}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{StatTable }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roundDF}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{reactable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-75-1.pdf}

At this point you may decide to check individual indicators, and some may be added or excluded.

\hypertarget{denomination-1}{%
\section{Denomination}\label{denomination-1}}

If you want to divide indicators by e.g.~GDP or population to be able to compare small countries with larger ones, you can use \texttt{denominate()}. The specifications are either made initially in \texttt{IndMeta}, or as arguments to \texttt{denominate()}. In the case of the ASEM data set, these are included in \texttt{IndMeta} so the command is very simple (run \texttt{View(ASEMIndMeta)} to see). We will afterwards check the new stats to see what has changed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create denominated data set}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{denominate}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Raw"}\NormalTok{)}

\CommentTok{\# get stats of denominated data}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{getStats}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{)}
\DocumentationTok{\#\# Number of collinear indicators =  0}
\DocumentationTok{\#\# Number of signficant negative indicator correlations =  440}
\DocumentationTok{\#\# Number of indicators with high denominator correlations =  0}

\CommentTok{\# view stats table}
\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Raw}\SpecialCharTok{$}\NormalTok{StatTable }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roundDF}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{reactable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-76-1.pdf}

According to the new table, there are now no high correlations with denominators, which indicates some kind of success.

\hypertarget{imputation}{%
\section{Imputation}\label{imputation}}

If you have missing data (and if you don't, you are fortunate), you can choose to impute the data. If you \emph{don't} impute missing data, you can still build a composite indicator though.

We can use one of COINr's options for imputing data. Like other commands, we operate on the COIN and the output is an updated COIN. Actually at this point I will make two alternatives - one which imputes missing data using a group median, and another which performs no imputation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make a copy}
\NormalTok{ASEM\_noImpute }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}

\CommentTok{\# impute one}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{, }\AttributeTok{imtype =} \StringTok{"indgroup\_mean"}\NormalTok{, }\AttributeTok{groupvar =} \StringTok{"Group\_GDP"}\NormalTok{)}
\DocumentationTok{\#\# Missing data points detected = 65}
\DocumentationTok{\#\# Missing data points imputed = 65, using method = indgroup\_mean}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-treatment-1}{%
\section{Data treatment}\label{data-treatment-1}}

Next we may wish to treat the data for outliers. Here we apply a standard approach which Winsorises each indicator up to a specified limit of points, in order to bring skew and kurtosis below specified thresholds. If Winsorisation fails, it applies a log transformation or similar.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{treat}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Imputed"}\NormalTok{, }\AttributeTok{winmax =} \DecValTok{5}\NormalTok{)}
\NormalTok{ASEM\_noImpute }\OtherTok{\textless{}{-}} \FunctionTok{treat}\NormalTok{(ASEM\_noImpute, }\AttributeTok{dset =} \StringTok{"Denominated"}\NormalTok{, }\AttributeTok{winmax =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Following treatment, it is a good idea to check which indicators were treated and how:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Analysis}\SpecialCharTok{$}\NormalTok{Treated}\SpecialCharTok{$}\NormalTok{TreatSummary }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Treatment }\SpecialCharTok{!=} \StringTok{"None"}\NormalTok{)}
\DocumentationTok{\#\#       IndCode Low High           TreatSpec           Treatment}
\DocumentationTok{\#\# V2   Services   0    4 Default, winmax = 5 Winsorised 4 points}
\DocumentationTok{\#\# V3        FDI   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V5    ForPort   0    5 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V6  CostImpEx   0    1 Default, winmax = 5 Winsorised 1 points}
\DocumentationTok{\#\# V7     Tariff   0    4 Default, winmax = 5 Winsorised 4 points}
\DocumentationTok{\#\# V12     StMob   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V15  CultServ   0    4 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V21   Flights   0    1 Default, winmax = 5 Winsorised 1 points}
\DocumentationTok{\#\# V23      Bord   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V25       Gas   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V35    Forest   0    2 Default, winmax = 5 Winsorised 2 points}
\DocumentationTok{\#\# V36   Poverty   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V41      NGOs   0    3 Default, winmax = 5 Winsorised 3 points}
\DocumentationTok{\#\# V43    FemLab   1    0 Default, winmax = 5 Winsorised 1 points}
\DocumentationTok{\#\# V45   PubDebt   0    1 Default, winmax = 5 Winsorised 1 points}
\end{Highlighting}
\end{Shaded}

It is also a good idea to visualise and compare the treated data against the untreated data. The best way to do this interactively is to call \texttt{indDash()} again, which allows comparison of treated and untreated indicators side by side. We can also do this manually (or for presentation) for specific indicators:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotIndDist2}\NormalTok{(ASEM, }\AttributeTok{dsets =} \FunctionTok{c}\NormalTok{(}\StringTok{"Imputed"}\NormalTok{, }\StringTok{"Treated"}\NormalTok{), }\AttributeTok{icodes =} \StringTok{"Services"}\NormalTok{, }\AttributeTok{ptype =} \StringTok{"Scatter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-80-1.pdf}

This shows the Winsorisation of four points for the ``Services'' indicator. This could also be plotted in different ways using box plots or violin plots.

\hypertarget{normalisation-1}{%
\section{Normalisation}\label{normalisation-1}}

The next step would be to normalise the data. In the ASEM index we will use a simple min-max normalisation in the \([0, 100]\) interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{normalise}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Treated"}\NormalTok{, }\AttributeTok{ntype =} \StringTok{"minmax"}\NormalTok{, }\AttributeTok{npara =} \FunctionTok{list}\NormalTok{(}\AttributeTok{minmax =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)))}
\NormalTok{ASEM\_noImpute }\OtherTok{\textless{}{-}} \FunctionTok{normalise}\NormalTok{(ASEM\_noImpute, }\AttributeTok{dset =} \StringTok{"Treated"}\NormalTok{, }\AttributeTok{ntype =} \StringTok{"minmax"}\NormalTok{, }\AttributeTok{npara =} \FunctionTok{list}\NormalTok{(}\AttributeTok{minmax =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Again, we could visualise and check stats here but to keep things shorter we'll skip that for now.

\hypertarget{aggregation-1}{%
\section{Aggregation}\label{aggregation-1}}

The last construction step (apart from iterative changes) is to aggregate. Again we use a simple arithmetic mean. The structure of the index is stored in the \texttt{IndMeta} data frame inside the COIN, so we only need to specify which data set to aggregate, and which method to use.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM, }\AttributeTok{agtype =} \StringTok{"arith\_mean"}\NormalTok{, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{)}
\NormalTok{ASEM\_noImpute }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM\_noImpute, }\AttributeTok{agtype =} \StringTok{"arith\_mean"}\NormalTok{, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualisation-1}{%
\section{Visualisation}\label{visualisation-1}}

We can now visualise our results. A good way at the index level is a stacked bar chart.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotBar}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Index"}\NormalTok{, }\AttributeTok{aglev =} \DecValTok{4}\NormalTok{, }\AttributeTok{stack\_children =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-83-1.pdf}

This can look a bit strange because in fact the ASEM index was only aggregated up to the sustainability and connectivity sub-indexes. We can also plot that:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotIndDist2}\NormalTok{(ASEM, }\AttributeTok{dsets =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{icodes =} \StringTok{"Index"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-84-1.pdf}

We may also plot the results on a map. Here we'll only plot connectivity:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplotMap}\NormalTok{(ASEM, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Conn"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{COINr-documentation_files/figure-latex/unnamed-chunk-85-1.pdf}

For a bit more detail we can also generate a table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getResults}\NormalTok{(ASEM, }\AttributeTok{tab\_type =} \StringTok{"Summary"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r|r}
\hline
UnitCode & UnitName & Index & Rank\\
\hline
CHE & Switzerland & 67.70 & 1\\
\hline
DNK & Denmark & 64.15 & 2\\
\hline
NLD & Netherlands & 63.70 & 3\\
\hline
NOR & Norway & 63.66 & 4\\
\hline
BEL & Belgium & 62.88 & 5\\
\hline
SWE & Sweden & 62.43 & 6\\
\hline
LUX & Luxembourg & 61.51 & 7\\
\hline
AUT & Austria & 61.47 & 8\\
\hline
DEU & Germany & 60.36 & 9\\
\hline
MLT & Malta & 60.14 & 10\\
\hline
SVN & Slovenia & 60.04 & 11\\
\hline
IRL & Ireland & 60.02 & 12\\
\hline
SGP & Singapore & 59.00 & 13\\
\hline
FIN & Finland & 58.17 & 14\\
\hline
GBR & United Kingdom & 57.15 & 15\\
\hline
LTU & Lithuania & 56.87 & 16\\
\hline
CZE & Czech Republic & 56.47 & 17\\
\hline
HRV & Croatia & 55.51 & 18\\
\hline
HUN & Hungary & 55.22 & 19\\
\hline
EST & Estonia & 55.11 & 20\\
\hline
FRA & France & 54.82 & 21\\
\hline
LVA & Latvia & 54.72 & 22\\
\hline
SVK & Slovakia & 54.67 & 23\\
\hline
ROU & Romania & 54.46 & 24\\
\hline
ESP & Spain & 54.04 & 25\\
\hline
POL & Poland & 53.87 & 26\\
\hline
PRT & Portugal & 53.03 & 27\\
\hline
ITA & Italy & 51.38 & 28\\
\hline
BGR & Bulgaria & 50.68 & 29\\
\hline
CYP & Cyprus & 50.63 & 30\\
\hline
KOR & Korea & 50.20 & 31\\
\hline
NZL & New Zealand & 48.94 & 32\\
\hline
GRC & Greece & 48.64 & 33\\
\hline
JPN & Japan & 47.24 & 34\\
\hline
KHM & Cambodia & 45.63 & 35\\
\hline
BRN & Brunei Darussalam & 45.56 & 36\\
\hline
AUS & Australia & 45.42 & 37\\
\hline
VNM & Vietnam & 43.55 & 38\\
\hline
LAO & Lao PDR & 42.52 & 39\\
\hline
MYS & Malaysia & 42.25 & 40\\
\hline
MMR & Myanmar & 41.95 & 41\\
\hline
PHL & Philippines & 41.33 & 42\\
\hline
THA & Thailand & 41.27 & 43\\
\hline
MNG & Mongolia & 40.46 & 44\\
\hline
IDN & Indonesia & 39.93 & 45\\
\hline
IND & India & 39.48 & 46\\
\hline
KAZ & Kazakhstan & 38.95 & 47\\
\hline
BGD & Bangladesh & 38.92 & 48\\
\hline
CHN & China & 37.83 & 49\\
\hline
PAK & Pakistan & 37.69 & 50\\
\hline
RUS & Russian Federation & 35.45 & 51\\
\hline
\end{tabular}

\hypertarget{comparison}{%
\section{Comparison}\label{comparison}}

Since we built two slightly different indexes, it makes sense also to check the difference. How do the ranks change if we do or do not impute?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{compTable}\NormalTok{(ASEM, ASEM\_noImpute, }\AttributeTok{dset =} \StringTok{"Aggregated"}\NormalTok{, }\AttributeTok{isel =} \StringTok{"Index"}\NormalTok{)}
\DocumentationTok{\#\#    UnitCode           UnitName RankCOIN1 RankCOIN2 RankChange AbsRankChange}
\DocumentationTok{\#\# 29      LAO            Lao PDR        39        45         {-}6             6}
\DocumentationTok{\#\# 22      IND              India        46        42          4             4}
\DocumentationTok{\#\# 41      PHL        Philippines        42        39          3             3}
\DocumentationTok{\#\# 1       AUS          Australia        37        35          2             2}
\DocumentationTok{\#\# 6       BRN  Brunei Darussalam        36        38         {-}2             2}
\DocumentationTok{\#\# 16      FRA             France        21        19          2             2}
\DocumentationTok{\#\# 27      KHM           Cambodia        35        37         {-}2             2}
\DocumentationTok{\#\# 33      MLT              Malta        10        12         {-}2             2}
\DocumentationTok{\#\# 34      MMR            Myanmar        41        43         {-}2             2}
\DocumentationTok{\#\# 35      MNG           Mongolia        44        46         {-}2             2}
\DocumentationTok{\#\# 50      THA           Thailand        43        41          2             2}
\DocumentationTok{\#\# 51      VNM            Vietnam        38        36          2             2}
\DocumentationTok{\#\# 2       AUT            Austria         8         7          1             1}
\DocumentationTok{\#\# 5       BGR           Bulgaria        29        30         {-}1             1}
\DocumentationTok{\#\# 9       CYP             Cyprus        30        29          1             1}
\DocumentationTok{\#\# 12      DNK            Denmark         2         3         {-}1             1}
\DocumentationTok{\#\# 13      ESP              Spain        25        24          1             1}
\DocumentationTok{\#\# 14      EST            Estonia        20        21         {-}1             1}
\DocumentationTok{\#\# 18      GRC             Greece        33        32          1             1}
\DocumentationTok{\#\# 20      HUN            Hungary        19        20         {-}1             1}
\DocumentationTok{\#\# 21      IDN          Indonesia        45        44          1             1}
\DocumentationTok{\#\# 23      IRL            Ireland        12        11          1             1}
\DocumentationTok{\#\# 31      LUX         Luxembourg         7         8         {-}1             1}
\DocumentationTok{\#\# 32      LVA             Latvia        22        23         {-}1             1}
\DocumentationTok{\#\# 37      NLD        Netherlands         3         2          1             1}
\DocumentationTok{\#\# 39      NZL        New Zealand        32        33         {-}1             1}
\DocumentationTok{\#\# 44      ROU            Romania        24        25         {-}1             1}
\DocumentationTok{\#\# 47      SVK           Slovakia        23        22          1             1}
\DocumentationTok{\#\# 48      SVN           Slovenia        11        10          1             1}
\DocumentationTok{\#\# 3       BEL            Belgium         5         5          0             0}
\DocumentationTok{\#\# 4       BGD         Bangladesh        48        48          0             0}
\DocumentationTok{\#\# 7       CHE        Switzerland         1         1          0             0}
\DocumentationTok{\#\# 8       CHN              China        49        49          0             0}
\DocumentationTok{\#\# 10      CZE     Czech Republic        17        17          0             0}
\DocumentationTok{\#\# 11      DEU            Germany         9         9          0             0}
\DocumentationTok{\#\# 15      FIN            Finland        14        14          0             0}
\DocumentationTok{\#\# 17      GBR     United Kingdom        15        15          0             0}
\DocumentationTok{\#\# 19      HRV            Croatia        18        18          0             0}
\DocumentationTok{\#\# 24      ITA              Italy        28        28          0             0}
\DocumentationTok{\#\# 25      JPN              Japan        34        34          0             0}
\DocumentationTok{\#\# 26      KAZ         Kazakhstan        47        47          0             0}
\DocumentationTok{\#\# 28      KOR              Korea        31        31          0             0}
\DocumentationTok{\#\# 30      LTU          Lithuania        16        16          0             0}
\DocumentationTok{\#\# 36      MYS           Malaysia        40        40          0             0}
\DocumentationTok{\#\# 38      NOR             Norway         4         4          0             0}
\DocumentationTok{\#\# 40      PAK           Pakistan        50        50          0             0}
\DocumentationTok{\#\# 42      POL             Poland        26        26          0             0}
\DocumentationTok{\#\# 43      PRT           Portugal        27        27          0             0}
\DocumentationTok{\#\# 45      RUS Russian Federation        51        51          0             0}
\DocumentationTok{\#\# 46      SGP          Singapore        13        13          0             0}
\DocumentationTok{\#\# 49      SWE             Sweden         6         6          0             0}
\end{Highlighting}
\end{Shaded}

This shows that the maximum rank change is 6 places, at the index level.

\hypertarget{export}{%
\section{Export}\label{export}}

Most likely not everyone you present the results to will want to see it in R. COINr has a simple but quick way to export the entire contents of the COIN to Excel (effectively, it exports all data frames that are present). We will first generate a results table of the main results, attach it to the COIN, then export to Excel. Note that the results are anyway present in \texttt{ASEM\$Data\$Aggregated}, but the \texttt{getResults()} function provides tables that are better to present (the highest levels of aggregation are the first columns, rather than the last, and it is sorted by index score).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Write full results table to COIN}
\NormalTok{COINr}\SpecialCharTok{::}\FunctionTok{getResults}\NormalTok{(ASEM, }\AttributeTok{tab\_type =} \StringTok{"FullWithDenoms"}\NormalTok{, }\AttributeTok{out2 =} \StringTok{"COIN"}\NormalTok{)}

\CommentTok{\# Export entire COIN to Excel}
\NormalTok{COINr}\SpecialCharTok{::}\FunctionTok{coin2Excel}\NormalTok{(ASEM, }\StringTok{"ASEM\_results.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

This is a fast example of COINr functions for building a composite indicator. Normally this would be an iterative process, but it showcases how simple commands can be used to do fairly complex operations in many cases. There is a lot more to all of the functions used here, and you should check the respective chapters of this book to better tune them to your needs.

\hypertarget{appendix-analysing-a-composite-indicator-example}{%
\chapter{Appendix: Analysing a Composite Indicator Example}\label{appendix-analysing-a-composite-indicator-example}}

Here some possible steps for analysing a composite indicator are given. The format for doing this might depend on what you are trying to accomplish. If you want to analyse your own composite indicator as you are building it, this should be integrated with the previous \protect\hyperlink{appendix-building-a-composite-indicator-example}{Appendix: Building a Composite Indicator Example}. If you are analysing an existing composite indicator built by someone else, you may wish to do this in a separate R Markdown document, for example.

\hypertarget{loading-data}{%
\section{Loading data}\label{loading-data}}

If the analysis is part of constructing a composite indicator in COINr, you should have a COIN assembled following the steps in the previous chapter. However, if you are analysing an existing composite indicator, there may be a slight difference. It is possible, for example, that you may be provided with indicator data, which may already be normalised and aggregated.

COINr's \texttt{assemble()} function allows you to assemble a COIN with pre-aggregated data. This is done by setting \texttt{preagg\ =\ TRUE}. If this is enabled, COINr will create a COIN with a data set called ``PreAggregated''. To do this, you need a pre-aggregated data set which includes (possibly normalised) columns for all indicators \emph{and} for all aggregates.

To demonstrate, we can use a pre-aggregated data set created by COINr.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(COINr)}

\CommentTok{\# build ASEM index}
\NormalTok{ASEM }\OtherTok{\textless{}{-}} \FunctionTok{build\_ASEM}\NormalTok{()}

\CommentTok{\# extract aggregated data set (as a data frame)}
\NormalTok{Aggregated\_Data }\OtherTok{\textless{}{-}}\NormalTok{ ASEM}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Aggregated}

\CommentTok{\# assemble new COIN only including pre{-}aggregated data}
\NormalTok{ASEM\_preagg }\OtherTok{\textless{}{-}} \FunctionTok{assemble}\NormalTok{(}\AttributeTok{IndData =}\NormalTok{ Aggregated\_Data,}
                        \AttributeTok{IndMeta =}\NormalTok{ ASEMIndMeta,}
                        \AttributeTok{AggMeta =}\NormalTok{ ASEMAggMeta,}
                        \AttributeTok{preagg =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

COINr will check that the column names in the indicator data correspond to the codes supplied in the \texttt{IndMeta} and \texttt{AggMeta}. This means that these two latter data frames still need to be supplied. However, from this point the COIN functions as any other, although consider that it cannot be regenerated (the methodology to arrive at the pre-aggregated data is unknown), and the only data set present is the ``PreAggregated'' data.

\hypertarget{check-calculations}{%
\section{Check calculations}\label{check-calculations}}

If you are using pre-aggregated data, you may wish to check the calculations to make sure that they are correct. If you additionally have raw data, and you know the methodology used to build the index, you can recreate this by rebuilding the index in COINr. If you only have normalised and aggregated data, you can still at least check the aggregation stage as follows. Assuming that the indicator columns in your pre-aggregated data are normalised, we can first manually create a normalised data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{ASEM\_preagg}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Normalised }\OtherTok{\textless{}{-}}\NormalTok{ ASEM\_preagg}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{PreAggregated }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\NormalTok{ASEM}\SpecialCharTok{$}\NormalTok{Input}\SpecialCharTok{$}\NormalTok{AggMeta}\SpecialCharTok{$}\NormalTok{Code)}
\end{Highlighting}
\end{Shaded}

Here we have just copied the pre-aggregated data, but removed any aggregation columns.

Next, we can aggregate these columns using COINr.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ASEM\_preagg }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(ASEM\_preagg, }\AttributeTok{dset =} \StringTok{"Normalised"}\NormalTok{, }\AttributeTok{agtype =} \StringTok{"arith\_mean"}\NormalTok{)}

\CommentTok{\# check data set names}
\FunctionTok{names}\NormalTok{(ASEM\_preagg}\SpecialCharTok{$}\NormalTok{Data)}
\DocumentationTok{\#\# [1] "PreAggregated" "Normalised"    "Aggregated"}
\end{Highlighting}
\end{Shaded}

Finally, we can check to see whether these data frames are the same. There are many possible ways of doing this, but a simple way is to use dplyr's \texttt{all\_equal()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{all\_equal}\NormalTok{(ASEM\_preagg}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{PreAggregated,}
\NormalTok{          ASEM\_preagg}\SpecialCharTok{$}\NormalTok{Data}\SpecialCharTok{$}\NormalTok{Aggregated)}
\DocumentationTok{\#\# [1] TRUE}
\end{Highlighting}
\end{Shaded}

As expected, here the results are the same. If the results are \emph{not} the same, \texttt{all\_equal()} will give some information about the differences. If you reconstruct the index from raw data, and you find differences, a few points are worth considering:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The difference could be due to an error in the pre-aggregated data, or even a bug in COINr. If you suspect the latter please open an issue on the repo.
\item
  If you have used data treatment or imputation, differences can easily arise. One reason is that some things are possible to calculate in different ways. COINr uses certain choices, but other choices are also valid. Examples of this include:

  \begin{itemize}
  \tightlist
  \item
    Skew and kurtosis (underlying data treatment) - see e.g.~\texttt{?e1071::skewness}
  \item
    Correlation and treatment of missing values - see \texttt{?cor}
  \item
    Ranks and how to handle ties - see \texttt{?rank}
  \end{itemize}
\item
  Errors can also arise from how you entered the data. Worth re-checking all that as well.
\end{enumerate}

Double-checking calculations is tedious but in the process you often learn a lot.

\hypertarget{indicator-statistics}{%
\section{Indicator statistics}\label{indicator-statistics}}

\emph{To be completed}

\hypertarget{acknowledgements}{%
\chapter{Acknowledgements}\label{acknowledgements}}

The construction of this package was funded by the European Commission's Joint Research Centre, in particular by the \href{https://composite-indicators.jrc.ec.europa.eu/}{Competence Centre for Composite Indicators and Scoreboards}. It was also developed with and inspired by colleagues from the same group.

Some significant parts of the package were built in tandem with, and tested for the World Intellectual Property Organisation's \href{https://www.globalinnovationindex.org/Home}{Global Innovation Index} - this, and interactions with GII staff, greatly helped to improve the internal working of the package and develop its features. Similarly, \href{https://www.unido.org/}{UNIDO}'s Quality Infrastructure for Sustainable Development Index was used to further test and refine the package. The \href{https://wefnexusindex.org/}{Water-Energy-Food Nexus Index} was also used as a test case for sensitivity analysis.

COINr, like all R packages, has been built off of the back of many other packages. Some that COINr uses in particular are:

\begin{itemize}
\tightlist
\item
  Plotly, for beautiful javascript plots
\item
  ggplot2, for beautiful static plots
\item
  reactable, for nice interactive tables
\item
  shiny, for apps
\item
  Various elements of the tidyverse, in particularly dplyr and purrr
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
